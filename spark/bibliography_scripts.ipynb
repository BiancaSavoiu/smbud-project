{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U4ugBnQJt8dk",
    "outputId": "85f42e41-b3d2-4c1b-de49-56addeabdf0a"
   },
   "outputs": [],
   "source": [
    "# Dowloading pyspark\n",
    "#!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PREPARE THE ENVIRONMENT, UPLOAD DATA, PREPROCESS DATA AND CREATE THE TABLES: Author, Paper, Affiliation, Book, Journal and Conference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# With sparkSession we create a connection to our database\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import count, col, xxhash64, collect_list, explode\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType, TimestampType\n",
    "\n",
    "# Create an entry point to the PySpark Application\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"Bibliography\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "#INPUT_FILE = \"/content/drive/MyDrive/bibliography.json\"\n",
    "INPUT_FILE = \"bibliography.json\"\n",
    "OPTIONS = {'multiline': 'true', 'allowNumericLeadingZero': 'true', 'timestampFormat': \"yyyy-MM-dd'T'HH:mm:ss[.ZZZ'Z']\"}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#AUTHOR TABLE\n",
    "schemaAut = StructType(\n",
    "    [StructField('authors', ArrayType(StructType([\n",
    "        StructField('_id', StringType(), nullable=False),\n",
    "        StructField('name', StringType(), True),\n",
    "        StructField('email', StringType(), True),\n",
    "        StructField('bio', StringType(), True),\n",
    "    ])), True)\n",
    "     ])\n",
    "\n",
    "df_aut = spark.read.format('json').options(**OPTIONS).schema(schemaAut).json(INPUT_FILE)\n",
    "df_aut = df_aut.select(explode(df_aut.authors))\n",
    "df_aut = df_aut.withColumnRenamed(\"col\", \"authors\")\n",
    "df_aut = df_aut.filter(col(\"authors._id\") != \"null\").select(\"authors._id\", \"authors.name\", \"authors.email\",\n",
    "                                                            \"authors.bio\")\n",
    "df_aut = df_aut.withColumnRenamed(\"_id\", \"author_id\")\n",
    "df_aut = df_aut.dropDuplicates([\"author_id\"])\n",
    "df_aut.printSchema()\n",
    "df_aut.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# PAPER TABLE WITHOUT PUBLICATION_ID\n",
    "schemaPaper = StructType(\n",
    "    [StructField('_id', StringType(), True),\n",
    "     StructField('title', StringType(), True),\n",
    "     StructField('keywords', ArrayType(StringType()), True),\n",
    "     StructField('fos', ArrayType(StringType()), True),\n",
    "     StructField('references', ArrayType(StringType()), True),\n",
    "     StructField('page_start', IntegerType(), True),\n",
    "     StructField('page_end', IntegerType(), True),\n",
    "     StructField('lang', StringType(), True),\n",
    "     StructField('doi', StringType(), True),\n",
    "     StructField('url', ArrayType(StringType()), True),\n",
    "     StructField('abstract', StringType(), True),\n",
    "     StructField('publication_type', StringType(), True),\n",
    "     StructField('date', TimestampType(), True)\n",
    "     ])\n",
    "\n",
    "df_paper = spark.read.format('json').options(**OPTIONS).schema(schemaPaper).json(INPUT_FILE)\n",
    "df_paper = df_paper.withColumnRenamed(\"_id\", \"paper_id\")\n",
    "df_paper.printSchema()\n",
    "df_paper.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# AFFILIATION TABLE\n",
    "schemaAffiliation = StructType(\n",
    "    [StructField('_id', StringType(), True),\n",
    "     StructField('authors', ArrayType(StructType([\n",
    "         StructField('_id', StringType(), True),\n",
    "         StructField('org', StringType(), True)\n",
    "     ])), True),\n",
    "     ])\n",
    "\n",
    "df_aff = spark.read.format('json').options(**OPTIONS).schema(schemaAffiliation).json(INPUT_FILE)\n",
    "df_aff = df_aff.withColumnRenamed(\"_id\", \"paper_id\")\n",
    "df_aff = df_aff.select(\"paper_id\", explode(df_aff.authors))\n",
    "df_aff = df_aff.withColumnRenamed(\"col\", \"authors\")\n",
    "df_aff = df_aff.filter(col(\"authors._id\") != \"null\").filter(col(\"paper_id\") != \"null\").select(\"paper_id\", \"authors._id\",\n",
    "                                                                                              \"authors.org\")\n",
    "df_aff = df_aff.withColumnRenamed(\"_id\", \"author_id\")\n",
    "df_aff = df_aff.dropDuplicates([\"author_id\", \"paper_id\"])\n",
    "df_aff = df_aff.withColumnRenamed(\"org\", \"organization\")\n",
    "df_aff.printSchema()\n",
    "df_aff.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# JOURNAL TABLE\n",
    "# Preprocessing of the journals for cleaning and merging the journals\n",
    "\n",
    "journal_schema_preprocessing = StructType(\n",
    "    [StructField('_id', StringType(), True),\n",
    "     StructField('issn', StringType(), True),\n",
    "     StructField('publisher', StringType(), True),\n",
    "     StructField('venue', StringType(), True),\n",
    "     StructField('volume', IntegerType(), True),\n",
    "     StructField('issue', IntegerType(), True),\n",
    "     StructField('publication_type', StringType(), True)])\n",
    "\n",
    "# Reading the json file\n",
    "df_journals_to_filter = spark.read.format('json').options(**OPTIONS).schema(journal_schema_preprocessing).json(\n",
    "    INPUT_FILE)\n",
    "\n",
    "# Filtering and adjusting the dataframe\n",
    "df_journals_to_filter = df_journals_to_filter.filter(col('publication_type') == 'Journal').filter(\n",
    "    col('issn') != 'null').filter(col('venue') != 'null').filter(col('issue') >= 0).filter(col('volume') >= 0)\n",
    "df_journals_to_filter = df_journals_to_filter.groupBy('venue', 'volume', 'issue', 'issn').agg(\n",
    "    collect_list('publisher').alias('publishersArray'), collect_list('_id').alias('_id'),\n",
    "    count(col('publisher')))  # count can be removed (I was interested in evaluating if the group by was meaningful)\n",
    "df_journals_to_insert = df_journals_to_filter.withColumn('publisher',\n",
    "                                                         df_journals_to_filter['publishersArray'][0]).select('venue',\n",
    "                                                                                                             'volume',\n",
    "                                                                                                             'issue',\n",
    "                                                                                                             'publisher',\n",
    "                                                                                                             'issn',\n",
    "                                                                                                             '_id')\n",
    "\n",
    "# Adding the new column which contains the publication_identifier\n",
    "df_journals = df_journals_to_insert.withColumn(\"publication_id\", xxhash64('venue', 'volume', 'issue', 'issn'))\n",
    "\n",
    "# Adding the foreign key to the papers\n",
    "exploded_journals = df_journals.select(explode('_id'), 'publication_id')\n",
    "#exploded_journals.show(truncate = False)\n",
    "\n",
    "df_papers_in_journals = exploded_journals.join(df_paper, exploded_journals.col == df_paper.paper_id, \"inner\")\n",
    "df_papers_in_journals = df_papers_in_journals.drop('col')\n",
    "\n",
    "df_journals = df_journals.drop(df_journals._id)\n",
    "\n",
    "# Visualizing the data\n",
    "# print('Papers')\n",
    "# df_papers_in_journals.show(truncate = False)\n",
    "print('Schema of the journals')\n",
    "df_journals.printSchema()\n",
    "print('Journals')\n",
    "df_journals.show(truncate=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# BOOK TABLE\n",
    "# Preprocessing of the books for cleaning and merging the books\n",
    "book_schema_preprocessing = StructType(\n",
    "    [StructField('_id', StringType(), True),\n",
    "     StructField('isbn', StringType(), True),\n",
    "     StructField('publisher', StringType(), True),\n",
    "     StructField('venue', StringType(), True),\n",
    "     StructField('publication_type', StringType(), True)])\n",
    "\n",
    "# Reading the json file\n",
    "df_books_to_filter = spark.read.format('json').options(**OPTIONS).schema(book_schema_preprocessing).json(INPUT_FILE)\n",
    "\n",
    "# Filtering and adjusting the dataframe\n",
    "df_books_to_filter = df_books_to_filter.filter(col('publication_type') == 'Book').filter(col('isbn') != 'null').filter(\n",
    "    col('venue') != 'null')\n",
    "df_books_to_filter = df_books_to_filter.groupBy('isbn', 'venue').agg(collect_list('publisher').alias('publishersArray'),\n",
    "                                                                     collect_list('_id').alias('_id'), count(\n",
    "        col('publisher')))  # count can be removed (I was interested in evaluating if the group by was meaningful)\n",
    "dfbooks_to_insert = df_books_to_filter.withColumn('publisher', df_books_to_filter['publishersArray'][0]).select('venue',\n",
    "                                                                                                                'isbn',\n",
    "                                                                                                                'publisher',\n",
    "                                                                                                                '_id')\n",
    "\n",
    "# Adding the new column which is the id\n",
    "df_books = dfbooks_to_insert.withColumn('publication_id', xxhash64('isbn', 'venue'))\n",
    "\n",
    "# Adding the foreign key to the papers\n",
    "exploded_books = df_books.select(explode('_id'), 'publication_id')\n",
    "# exploded_books.show(truncate = False)\n",
    "\n",
    "df_papers_in_books = exploded_books.join(df_paper, exploded_books.col == df_paper.paper_id)\n",
    "df_papers_in_books = df_papers_in_books.drop('col')\n",
    "\n",
    "df_books = df_books.drop(df_books._id)\n",
    "\n",
    "# Visualizing the data\n",
    "# print('Papers')\n",
    "# df_papers_in_books.show(truncate = False)\n",
    "print('Schema of the books')\n",
    "df_books.printSchema()\n",
    "print('Books')\n",
    "df_books.show(truncate=40)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# CONFERENCE TABLE\n",
    "# Preprocessing of the books for cleaning and merging the books\n",
    "\n",
    "schemaConf = StructType(\n",
    "    [StructField('_id', StringType(), True),\n",
    "     StructField('location', StringType(), True),\n",
    "     StructField('venue', StringType(), True),\n",
    "     StructField('publication_type', StringType(), True)])\n",
    "# Reading the json file\n",
    "df_conferences_to_filter = spark.read.format('json').options(**OPTIONS).schema(schemaConf).json(INPUT_FILE)\n",
    "\n",
    "# Filtering and adjusting the dataframe\n",
    "df_conferences_to_filter = df_conferences_to_filter \\\n",
    "    .filter(col('publication_type') == 'Conference') \\\n",
    "    .filter(col('venue') != 'null')\n",
    "# count can be removed (I was interested in evaluating if the group by was meaningful)\n",
    "df_conferences_to_filter = df_conferences_to_filter \\\n",
    "    .groupBy('venue') \\\n",
    "    .agg(collect_list('location').alias('locations_array'), collect_list('_id').alias('_id'), count(col('location')))\n",
    "df_conferences_to_insert = df_conferences_to_filter \\\n",
    "    .withColumn('location', df_conferences_to_filter['locations_array'][0]) \\\n",
    "    .select('venue', 'location', '_id')\n",
    "\n",
    "# Adding the new column which is the id\n",
    "df_conferences = df_conferences_to_insert.withColumn('publication_id', xxhash64('venue'))\n",
    "\n",
    "# Adding the foreign key to the papers\n",
    "exploded_conferences = df_conferences.select(explode('_id'), 'publication_id')\n",
    "#exploded_conferences.show(truncate = False)\n",
    "\n",
    "df_papers_in_conferences = exploded_conferences.join(df_paper, exploded_conferences.col == df_paper.paper_id)\n",
    "df_papers_in_conferences = df_papers_in_conferences.drop('col')\n",
    "\n",
    "df_conferences = df_conferences.drop(df_conferences._id)\n",
    "\n",
    "# Visualizing the data\n",
    "#print('Papers')\n",
    "#df_papers_in_conferences.show(truncate = False)\n",
    "print('Schema of the conferences')\n",
    "df_conferences.printSchema()\n",
    "print('Conferences')\n",
    "df_conferences.show(truncate=50)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Merging the 3 dataframe which one contains the papers published in a specific media\n",
    "df_papers = df_papers_in_books \\\n",
    "    .union(df_papers_in_journals) \\\n",
    "    .union(df_papers_in_conferences)\n",
    "\n",
    "# Visualizing the data\n",
    "print('Papers schema')\n",
    "df_papers.printSchema()\n",
    "print('Papers data')\n",
    "df_papers.show(truncate=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# For checking the result\n",
    "print('Papers published in books')\n",
    "df_papers \\\n",
    "    .filter(col('publication_type') == 'Book') \\\n",
    "    .select('paper_id', 'title', 'publication_type', 'publication_id') \\\n",
    "    .show()\n",
    "print('Papers published in journals')\n",
    "df_papers \\\n",
    "    .filter(col('publication_type') == 'Journal') \\\n",
    "    .select('paper_id', 'title', 'publication_type', 'publication_id') \\\n",
    "    .show()\n",
    "print('Papers published in conferences')\n",
    "df_papers \\\n",
    "    .filter(col('publication_type') == 'Conference') \\\n",
    "    .select('paper_id', 'title', 'publication_type', 'publication_id') \\\n",
    "    .show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "COMMANDS"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Command 2:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Command 4: delete a group of rows\n",
    "\n",
    "# Use the function year to extract the year from the timestamp\n",
    "from pyspark.sql.functions import year\n",
    "\n",
    "# Drop rows with conditions – where clause\n",
    "# From 37626 to 37175 -> delete all the rows that represent papers published before 1950, because obsolete\n",
    "df_papers = df_papers.where(year('date') > '1950')\n",
    "df_papers \\\n",
    "    .select('title', 'publication_type', 'date') \\\n",
    "    .orderBy('date') \\\n",
    "    .show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Command 5: create a new column with the length of the paper (number of total pages)\n",
    "\n",
    "df_papers_total_pages = df_papers \\\n",
    "    .filter((col('page_start') >= 0) & (col('page_end') >= 0) & (col('page_start') <= col('page_end'))) \\\n",
    "    .withColumn('total_pages', col('page_end') - col('page_start'))\n",
    "\n",
    "df_papers_total_pages \\\n",
    "    .select(col('title'), col('page_start'), col('page_end'), col('total_pages')) \\\n",
    "    .show(5, truncate=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "QUERIES"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Query 1: WHERE, JOIN\n",
    "\n",
    "venue, volume, issue = ('BMC Bioinformatics', '14', '1')\n",
    "\n",
    "df_papers_q1 = df_journals \\\n",
    "    .filter((col('venue') == venue) &\n",
    "            (col('volume') == volume) &\n",
    "            (col('issue') == issue)) \\\n",
    "    .join(df_papers,\n",
    "          (df_journals['publication_id'] == df_papers['publication_id']) &\n",
    "          (df_papers['publication_type'] == 'Journal'))\n",
    "\n",
    "df_papers_q1 \\\n",
    "    .select(['paper_id', 'title']) \\\n",
    "    .show(truncate=60)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp, unix_timestamp\n",
    "\n",
    "# Query 2: WHERE, LIMIT, LIKE\n",
    "# Find the papers written in the last twenty years in english whose keywords have the word \\verb|artificial| inside the keywords. We require that these papers have the DOI set to a not null value.\n",
    "# The results are ordered ascendingly by the date and only 20 elements are printed.\n",
    "\n",
    "df = df_papers.withColumn('current time', current_timestamp())\n",
    "df \\\n",
    "    .filter((((unix_timestamp('current time') - unix_timestamp('date')) / 3600 / 24 / 365) > 50) &\n",
    "            (col('doi').isNotNull())) \\\n",
    "    .select('title', 'date', explode('keywords').alias('keyword')) \\\n",
    "    .filter(col('keyword').like('%artificial%')) \\\n",
    "    .distinct() \\\n",
    "    .sort(col('date').asc()) \\\n",
    "    .limit(20) \\\n",
    "    .show(truncate=50)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Query 3: WHERE, IN, Nested Query\n",
    "# Show the papers collected in a book that have `database` or `neural network` as keyword.\n",
    "\n",
    "nested_query = df_papers \\\n",
    "    .select('title',\n",
    "            'publication_type',\n",
    "            'paper_id',\n",
    "            'publication_id',\n",
    "            'date',\n",
    "            explode('keywords').alias('keyword')) \\\n",
    "    .filter(col('keyword').isin('multiagent system')) \\\n",
    "    .select('title',\n",
    "            'publication_type',\n",
    "            'paper_id',\n",
    "            'publication_id',\n",
    "            'date')\n",
    "nested_query \\\n",
    "    .join(df_conferences, 'publication_id') \\\n",
    "    .filter(col('publication_type') == 'Conference') \\\n",
    "    .orderBy('date', ascending=False) \\\n",
    "    .select(col('title').alias('paper title'),\n",
    "            col('venue').alias('conference venue')) \\\n",
    "    .show(5, truncate=50)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Query 4: GROUP BY, JOIN, AS\n",
    "\n",
    "from pyspark.sql.functions import collect_set, size\n",
    "\n",
    "df_aff \\\n",
    "    .join(df_papers, df_papers.paper_id == df_aff.paper_id, 'inner') \\\n",
    "    .drop(df_papers.paper_id) \\\n",
    "    .select('paper_id', 'organization', 'publication_type') \\\n",
    "    .filter((col('organization').isNotNull()) & (col('organization') != \"\") & (col('publication_type') == \"Conference\")) \\\n",
    "    .groupBy('organization') \\\n",
    "    .agg(collect_set('paper_id').alias('papers')) \\\n",
    "    .filter(size(col('papers')) > 10) \\\n",
    "    .show(truncate=50)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Query 5: WHERE, GROUP BY\n",
    "# Retrieve some statistics about papers\n",
    "\n",
    "from pyspark.sql.functions import sum, min, max, avg, format_number, variance\n",
    "\n",
    "df_papers_total_pages \\\n",
    "    .filter(year(col('date')) >= 2015) \\\n",
    "    .groupBy(year(col('date')).alias('year')) \\\n",
    "    .agg(count('paper_id').alias('total_papers'),\n",
    "         sum('total_pages').alias('total_pages'),\n",
    "         min('total_pages').alias('min_pages'),\n",
    "         max('total_pages').alias('max_pages'),\n",
    "         format_number(avg('total_pages'), 2).alias('avg_pages'),\n",
    "         format_number(variance('total_pages'), 2).alias('var_pages')) \\\n",
    "    .sort(col('year').desc()) \\\n",
    "    .show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Query 6: GROUP BY, HAVING, AS\n",
    "\n",
    "df_papers \\\n",
    "    .select('paper_id', 'title', explode(col('references')).alias('reference')) \\\n",
    "    .groupBy('reference') \\\n",
    "    .agg(count('paper_id').alias('references_count')) \\\n",
    "    .filter(col('references_count') > 30) \\\n",
    "    .join(df_papers, col('reference') == df_papers.paper_id) \\\n",
    "    .sort(col('references_count').desc()) \\\n",
    "    .select(['title', 'references_count']) \\\n",
    "    .show(truncate=50)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Query 7: WHERE, GROUP BY, HAVING, AS\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Query 8: WHERE, Nested Query (i.e., 2-step Queries), GROUP BY\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Query 9: WHERE, GROUP BY, HAVING, 1 JOIN\n",
    "\n",
    "from pyspark.sql.functions import collect_set, concat, size\n",
    "\n",
    "df_journals \\\n",
    "    .withColumnRenamed('venue', 'venueJournals') \\\n",
    "    .filter((col('volume')) > 10) \\\n",
    "    .join(df_books, df_books.publisher == df_journals.publisher, \"inner\") \\\n",
    "    .drop(df_journals.publisher) \\\n",
    "    .withColumnRenamed('venue', 'venueBooks') \\\n",
    "    .select('venueBooks', 'venueJournals', 'publisher') \\\n",
    "    .dropDuplicates(['venueBooks', 'venueJournals', 'publisher']) \\\n",
    "    .groupBy('publisher') \\\n",
    "    .agg(collect_set('venueBooks').alias('books'), collect_set('venueJournals').alias('journals')) \\\n",
    "    .withColumn(\"total_publications_per_publisher\", concat(col(\"books\"), col(\"journals\"))) \\\n",
    "    .filter(size(col(\"total_publications_per_publisher\")) > '500') \\\n",
    "    .select('publisher', \"total_publications_per_publisher\") \\\n",
    "    .show(truncate=50)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Query 10: WHERE, GROUP BY, HAVING, 2 JOINs\n",
    "# Retrieve authors who worked for at least 3 different organizations and have published at least 3 papers with at least 5 fos and 5 references each\n",
    "\n",
    "from pyspark.sql.functions import approx_count_distinct\n",
    "\n",
    "df_papers \\\n",
    "    .filter((size(col('fos')) >= 5) & (size(col('references')) >= 5)) \\\n",
    "    .join(df_aff, df_papers.paper_id == df_aff.paper_id, \"inner\") \\\n",
    "    .drop(df_aff.paper_id) \\\n",
    "    .join(df_aut, df_aff.author_id == df_aut.author_id, \"inner\") \\\n",
    "    .drop(df_aff.author_id) \\\n",
    "    .groupBy(\"author_id\") \\\n",
    "    .agg(count(\"paper_id\").alias(\"papers_count\"),\n",
    "         approx_count_distinct(\"organization\").alias(\"organizations_count\"),\n",
    "         collect_set(\"name\").alias(\"name\")) \\\n",
    "    .filter((size(\"name\") == 1) & (col(\"papers_count\") >= 3) & (col(\"organizations_count\") >= 3)) \\\n",
    "    .orderBy(col(\"papers_count\").desc(), col(\"organizations_count\").desc()) \\\n",
    "    .select(explode(col(\"name\")).alias(\"name\"), col(\"papers_count\"), col(\"organizations_count\")) \\\n",
    "    .show(5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
