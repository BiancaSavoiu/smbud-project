{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U4ugBnQJt8dk",
    "outputId": "85f42e41-b3d2-4c1b-de49-56addeabdf0a"
   },
   "outputs": [],
   "source": [
    "# Dowloading pyspark\n",
    "#!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "PREPARE THE ENVIRONMENT, UPLOAD DATA, PREPROCESS DATA AND CREATE THE TABLES: Author, Paper, Affiliation, Book, Journal and Conference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# With sparkSession we create a connection to our database\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType, TimestampType\n",
    "from pyspark.sql.functions import count, col, xxhash64, collect_list, explode\n",
    "\n",
    "# Create an entry point to the PySpark Application\n",
    "spark = SparkSession.builder \\\n",
    "      .master(\"local\") \\\n",
    "      .appName(\"Bibliography\") \\\n",
    "      .getOrCreate()\n",
    "\n",
    "#INPUT_FILE = \"/content/drive/MyDrive/bibliography.json\"\n",
    "INPUT_FILE = \"bibliography.json\"\n",
    "OPTIONS = {'multiline': 'true', 'allowNumericLeadingZero': 'true','timestampFormat': \"yyyy-MM-dd'T'HH:mm:ss[.ZZZ'Z']\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#AUTHOR TABLE\n",
    "schemaAut = StructType(\n",
    "            [StructField('authors', ArrayType(StructType([\n",
    "                StructField('_id', StringType(), nullable = False),\n",
    "                StructField('name', StringType(), True),\n",
    "                StructField('email', StringType(), True),\n",
    "                StructField('bio', StringType(), True),\n",
    "                ])), True)\n",
    "            ])\n",
    "\n",
    "dfAut = spark.read.format('json').options(**OPTIONS).schema(schemaAut).json(INPUT_FILE)\n",
    "dfAut = dfAut.select(explode(dfAut.authors))\n",
    "dfAut = dfAut.withColumnRenamed(\"col\", \"authors\")\n",
    "dfAut = dfAut.filter(col(\"authors._id\") != \"null\").select(\"authors._id\",\"authors.name\",\"authors.email\", \"authors.bio\")\n",
    "dfAut = dfAut.withColumnRenamed(\"_id\", \"authorID\")\n",
    "dfAut = dfAut.dropDuplicates([\"authorID\"])\n",
    "dfAut.printSchema()\n",
    "dfAut.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# PAPER TABLE WITHOUT PUBLICATION_ID\n",
    "schemaPaper = StructType(\n",
    "            [StructField('_id', StringType(), True),\n",
    "             StructField('title', StringType(),True),\n",
    "             StructField('keywords', ArrayType(StringType()), True),\n",
    "             StructField('fos', ArrayType(StringType()), True),\n",
    "             StructField('references', ArrayType(StringType()), True),\n",
    "             StructField('page_start', IntegerType(), True),\n",
    "             StructField('page_end', IntegerType(), True),\n",
    "             StructField('lang', StringType(),True),\n",
    "             StructField('doi', StringType(),True),\n",
    "             StructField('url', ArrayType(StringType()),True),\n",
    "             StructField('abstract', StringType(),True),\n",
    "             StructField('publication_type', StringType(),True),\n",
    "             StructField('date', TimestampType(), True)\n",
    "            ])\n",
    "\n",
    "dfPaper = spark.read.format('json').options(**OPTIONS).schema(schemaPaper).json(INPUT_FILE)\n",
    "dfPaper = dfPaper.withColumnRenamed(\"_id\", \"paperID\")\n",
    "dfPaper.printSchema()\n",
    "dfPaper.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# AFFILIATION TABLE\n",
    "schemaAffiliation = StructType(\n",
    "            [StructField('_id', StringType(), True),\n",
    "             StructField('authors', ArrayType(StructType([\n",
    "                    StructField('_id', StringType(), True),\n",
    "                    StructField('org', StringType(), True)\n",
    "             ])), True),\n",
    "            ])\n",
    "\n",
    "dfAff = spark.read.format('json').options(**OPTIONS).schema(schemaAffiliation).json(INPUT_FILE)\n",
    "dfAff = dfAff.withColumnRenamed(\"_id\", \"paperID\")\n",
    "dfAff = dfAff.select(\"paperID\", explode(dfAff.authors))\n",
    "dfAff = dfAff.withColumnRenamed(\"col\", \"authors\")\n",
    "dfAff = dfAff.filter(col(\"authors._id\") != \"null\").filter(col(\"paperID\") != \"null\").select(\"paperID\", \"authors._id\",\"authors.org\")\n",
    "dfAff = dfAff.withColumnRenamed(\"_id\", \"authorID\")\n",
    "dfAff = dfAff.dropDuplicates([\"authorID\", \"paperID\"])\n",
    "dfAff = dfAff.withColumnRenamed(\"org\", \"organization\")\n",
    "dfAff.printSchema()\n",
    "dfAff.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# JOURNAL TABLE\n",
    "# Preprocessing of the journals for cleaning and merging the journals\n",
    "\n",
    "journal_schema_preprocessing = StructType(\n",
    "    [StructField('_id', StringType(), True),\n",
    "     StructField('issn', StringType(), True),\n",
    "     StructField('publisher', StringType(), True),\n",
    "     StructField('venue', StringType(), True),\n",
    "     StructField('volume', IntegerType(), True),\n",
    "     StructField('issue', IntegerType(), True),\n",
    "     StructField('publication_type', StringType(),True)])\n",
    "\n",
    "# Reading the json file\n",
    "df_journals_to_filter = spark.read.format('json').options(**OPTIONS).schema(journal_schema_preprocessing).json(INPUT_FILE)\n",
    "\n",
    "# Filtering and adjusting the dataframe\n",
    "df_journals_to_filter = df_journals_to_filter.filter(col('publication_type') == 'Journal').filter(col('issn') != 'null').filter(col('venue') != 'null').filter(col('issue') >= 0).filter(col('volume') >= 0)\n",
    "df_journals_to_filter = df_journals_to_filter.groupBy('venue', 'volume', 'issue', 'issn').agg(collect_list('publisher').alias('publishersArray'), collect_list('_id').alias('_id'), count(col('publisher'))) # count can be removed (I was interested in evaluating if the group by was meaningful)\n",
    "df_journals_to_insert = df_journals_to_filter.withColumn('publisher', df_journals_to_filter['publishersArray'][0]).select('venue', 'volume', 'issue', 'publisher', 'issn', '_id')\n",
    "\n",
    "# Adding the new column which contains the publication identifier\n",
    "df_journals = df_journals_to_insert.withColumn(\"publication id\", xxhash64('venue', 'volume', 'issue', 'issn'))\n",
    "\n",
    "# Adding the foreign key to the papers\n",
    "exploded_journals = df_journals.select(explode('_id'), 'publication id')\n",
    "#exploded_journals.show(truncate = False)\n",
    "\n",
    "df_papers_in_journals = exploded_journals.join(dfPaper, exploded_journals.col == dfPaper.paperID, \"inner\")\n",
    "df_papers_in_journals = df_papers_in_journals.drop('col')\n",
    "\n",
    "df_journals = df_journals.drop(df_journals._id)\n",
    "\n",
    "# Visualizing the data\n",
    "# print('Papers')\n",
    "# df_papers_in_journals.show(truncate = False)\n",
    "print('Schema of the journals')\n",
    "df_journals.printSchema()\n",
    "print('Journals')\n",
    "df_journals.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# BOOK TABLE\n",
    "# Preprocessing of the books for cleaning and merging the books\n",
    "book_schema_preprocessing = StructType(\n",
    "    [StructField('_id', StringType(), True),\n",
    "     StructField('isbn', StringType(), True),\n",
    "     StructField('publisher', StringType(), True),\n",
    "     StructField('venue', StringType(), True),\n",
    "     StructField('publication_type', StringType(),True)])\n",
    "\n",
    "# Reading the json file\n",
    "dfbooks_to_filter = spark.read.format('json').options(**OPTIONS).schema(book_schema_preprocessing).json(INPUT_FILE)\n",
    "\n",
    "# Filtering and adjusting the dataframe\n",
    "dfbooks_to_filter = dfbooks_to_filter.filter(col('publication_type') == 'Book').filter(col('isbn') != 'null').filter(col('venue') != 'null')\n",
    "dfbooks_to_filter = dfbooks_to_filter.groupBy('isbn', 'venue').agg(collect_list('publisher').alias('publishersArray'), collect_list('_id').alias('_id'), count(col('publisher'))) # count can be removed (I was interested in evaluating if the group by was meaningful)\n",
    "dfbooks_to_insert = dfbooks_to_filter.withColumn('publisher', dfbooks_to_filter['publishersArray'][0]).select('venue', 'isbn', 'publisher', '_id')\n",
    "\n",
    "# Adding the new column which is the id\n",
    "df_books = dfbooks_to_insert.withColumn('publication id', xxhash64('isbn', 'venue'))\n",
    "\n",
    "# Adding the foreign key to the papers\n",
    "exploded_books = df_books.select(explode('_id'), 'publication id')\n",
    "# exploded_books.show(truncate = False)\n",
    "\n",
    "df_papers_in_books = exploded_books.join(dfPaper, exploded_books.col == dfPaper.paperID)\n",
    "df_papers_in_books = df_papers_in_books.drop('col')\n",
    "\n",
    "df_books = df_books.drop(df_books._id)\n",
    "\n",
    "# Visualizing the data\n",
    "# print('Papers')\n",
    "# df_papers_in_books.show(truncate = False)\n",
    "print('Schema of the books')\n",
    "df_books.printSchema()\n",
    "print('Books')\n",
    "df_books.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# CONFERENCE TABLE\n",
    "# Preprocessing of the books for cleaning and merging the books\n",
    "\n",
    "schemaConf = StructType(\n",
    "    [StructField('_id', StringType(), True),\n",
    "     StructField('location', StringType(), True),\n",
    "     StructField('venue', StringType(), True),\n",
    "     StructField('publication_type', StringType(),True)])\n",
    "# Reading the json file\n",
    "df_conferences_to_filter = spark.read.format('json').options(**OPTIONS).schema(schemaConf).json(INPUT_FILE)\n",
    "\n",
    "# Filtering and adjusting the dataframe\n",
    "df_conferences_to_filter = df_conferences_to_filter.filter(col('publication_type') == 'Conference').filter(col('venue') != 'null')\n",
    "df_conferences_to_filter = df_conferences_to_filter.groupBy('venue').agg(collect_list('location').alias('locations_array'), collect_list('_id').alias('_id'), count(col('location'))) # count can be removed (I was interested in evaluating if the group by was meaningful)\n",
    "df_conferences_to_insert = df_conferences_to_filter.withColumn('location', df_conferences_to_filter['locations_array'][0]).select('venue', 'location', '_id')\n",
    "\n",
    "# Adding the new column which is the id\n",
    "df_conferences = df_conferences_to_insert.withColumn('publication id', xxhash64('venue'))\n",
    "\n",
    "# Adding the foreign key to the papers\n",
    "exploded_conferences = df_conferences.select(explode('_id'), 'publication id')\n",
    "#exploded_conferences.show(truncate = False)\n",
    "\n",
    "df_papers_in_conferences = exploded_conferences.join(dfPaper, exploded_conferences.col == dfPaper.paperID)\n",
    "df_papers_in_conferences = df_papers_in_conferences.drop('col')\n",
    "\n",
    "df_conferences = df_conferences.drop(df_conferences._id)\n",
    "\n",
    "# Visualizing the data\n",
    "#print('Papers')\n",
    "#df_papers_in_conferences.show(truncate = False)\n",
    "print('Schema of the conferences')\n",
    "df_conferences.printSchema()\n",
    "print('Conferences')\n",
    "df_conferences.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Merging the 3 dataframe which one contains the papers published in a specific media\n",
    "df_papers = df_papers_in_books.union(df_papers_in_journals).union(df_papers_in_conferences)\n",
    "\n",
    "# Visualizing the data\n",
    "print('Papers schema')\n",
    "df_papers.printSchema()\n",
    "print('Papers data')\n",
    "df_papers.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# For checking the result\n",
    "print('Papers published in books')\n",
    "df_papers.filter(col('publication_type') == 'Book').select('paperID', 'title', 'publication_type', 'publication id').show()\n",
    "print('Papers published in journals')\n",
    "df_papers.filter(col('publication_type') == 'Journal').select('paperID', 'title', 'publication_type', 'publication id').show()\n",
    "print('Papers published in conferences')\n",
    "df_papers.filter(col('publication_type') == 'Conference').select('paperID', 'title', 'publication_type', 'publication id').show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "COMMANDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Command 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Command 4: delete a group of rows\n",
    "\n",
    "# Use the function year to extract the year from the timestamp\n",
    "from pyspark.sql.functions import year\n",
    "\n",
    "# Drop rows with conditions – where clause\n",
    "# From 37626 to 37175 -> delete all the rows that represent papers published before 1950, because obsolete\n",
    "df_papers = df_papers.where(year('date') > '1950')\n",
    "df_papers.select('title', 'publication_type', 'date').orderBy('date').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Command 5: create a new column with the length of the paper (number of total pages)\n",
    "\n",
    "df_papers_total_pages = df_papers \\\n",
    "    .filter((col('page_start') >= 0) & (col('page_end') >= 0) & (col('page_start') <= col('page_end'))) \\\n",
    "    .withColumn('total_pages', col('page_end') - col('page_start'))\n",
    "\n",
    "df_papers_total_pages \\\n",
    "    .select(col('title'), col('page_start'), col('page_end'), col('total_pages')) \\\n",
    "    .show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "QUERIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Query 1: WHERE, JOIN\n",
    "\n",
    "venue, volume, issue = ('BMC Bioinformatics', '14', '1') \n",
    "\n",
    "df_papers_q1 = df_journals\\\n",
    "               .filter((col('venue') == venue) &\n",
    "                       (col('volume') == volume) &\n",
    "                       (col('issue') == issue))\\\n",
    "                .join(df_papers,\n",
    "                      (df_journals['publication id'] == df_papers['publication id']) &\n",
    "                        (df_papers['publication_type'] == 'Journal')\n",
    "                     )\n",
    "\n",
    "df_papers_q1.select(['paperID', 'title']).show(truncate=60)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp, unix_timestamp\n",
    "# Query 2: WHERE, LIMIT, LIKE\n",
    "# Find the papers written in the last twenty years in english whose keywords have the word \\verb|artificial| inside the keywords. We require that these papers have the DOI set to a not null value.\n",
    "# The results are ordered ascendengly by the date and only 20 elements are printed.\n",
    "\n",
    "df = df_papers.withColumn('current time', current_timestamp())\n",
    "df = df\\\n",
    "    .filter(\n",
    "        (((unix_timestamp('current time') - unix_timestamp('date')) / 3600 / 24 / 365) > 50) & \n",
    "        (col('doi').isNotNull()))\\\n",
    "    .select('title', 'date', explode('keywords').alias('keyword'))\\\n",
    "    .filter(col('keyword').like('%artificial%'))\\\n",
    "    .distinct()\\\n",
    "    .sort(col('date').asc())\\\n",
    "    .limit(20)\\\n",
    "    .show(truncate = False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Query 4: GROUP BY, JOIN, AS\n",
    "from pyspark.sql.functions import collect_set, size\n",
    "\n",
    "df = dfAff\\\n",
    "    .join(df_papers, df_papers.paperID == dfAff.paperID, 'inner')\\\n",
    "    .drop(df_papers.paperID).select('paperID', 'organization', 'publication_type')\\\n",
    "    .filter((col('organization').isNotNull()) & (col('organization') != \"\") & (col('publication_type') == \"Conference\"))\\\n",
    "    .groupBy('organization')\\\n",
    "    .agg(collect_set('paperID').alias('papers'))\\\n",
    "    .filter(size(col('papers')) > 10)\\\n",
    "    .show(truncate = 50)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Query 5: WHERE, GROUP BY\n",
    "# Retrieve some statistics about papers\n",
    "from pyspark.sql.functions import sum, min, max, avg, format_number, variance\n",
    "\n",
    "df_papers_total_pages.filter(year(col('date')) >= 2015) \\\n",
    "    .groupBy(year(col('date')).alias('year')) \\\n",
    "    .agg(count('paperID').alias('total_papers'),\n",
    "         sum('total_pages').alias('total_pages'),\n",
    "         min('total_pages').alias('min_pages'),\n",
    "         max('total_pages').alias('max_pages'),\n",
    "         format_number(avg('total_pages'), 2).alias('avg_pages'),\n",
    "         format_number(variance('total_pages'), 2).alias('var_pages')) \\\n",
    "    .sort(col('year').desc()) \\\n",
    "    .show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Query 6: GROUP BY, HAVING, AS\n",
    "\n",
    "df_papers_q6 = df_papers\\\n",
    "               .select('paperID',\n",
    "                       'title',\n",
    "                       explode(col('references')).alias('reference'))\\\n",
    "               .groupBy('reference')\\\n",
    "               .agg(count('paperID').alias('references_count'))\\\n",
    "               .filter(col('references_count') > 30)\\\n",
    "               .join(df_papers,\n",
    "                     col('reference') == df_papers.paperID)\\\n",
    "               .sort(col('references_count').desc())\\\n",
    "\n",
    "df_papers_q6.select(['title', 'references_count']).show(truncate=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Query 7: WHERE, GROUP BY, HAVING, AS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Query 9: WHERE, GROUP BY, HAVING, 1 JOIN\n",
    "\n",
    "from pyspark.sql.functions import collect_set, concat, size\n",
    "\n",
    "df_journals_venue_rename = df_journals.withColumnRenamed('venue', 'venueJournals')\n",
    "df_books_venue_rename = df_books.withColumnRenamed('venue', 'venueBooks')\n",
    "\n",
    "df = df_journals_venue_rename\\\n",
    "    .filter((col('volume')) > 10)\\\n",
    "    .join(df_books_venue_rename,\n",
    "          df_books_venue_rename.publisher == df_journals_venue_rename.publisher,\n",
    "          \"inner\")\\\n",
    "    .drop(df_journals.publisher)\\\n",
    "    .select('venueBooks', 'venueJournals', 'publisher')\\\n",
    "    .dropDuplicates(['venueBooks', 'venueJournals', 'publisher'])\\\n",
    "    .groupBy('publisher')\\\n",
    "    .agg(collect_set('venueBooks').alias('books'),\n",
    "         collect_set('venueJournals').alias('journals'))\\\n",
    "    .withColumn(\"total_publications_per_publisher\",\n",
    "                concat(col(\"books\"), col(\"journals\")))\\\n",
    "    .filter(size(col(\"total_publications_per_publisher\")) > '500')\\\n",
    "    .select('publisher', \"total_publications_per_publisher\")\\\n",
    "    .show(truncate = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Query 10: WHERE, GROUP BY, HAVING, 2 JOINs\n",
    "#Prendere i journal con almeno 5 for e 5  references i cui autori hanno scritto almeno 3 e che hanno lavorato in affiliation di tipo politecnico, mettere anche la order by e limit\n",
    "\n",
    "df_papers \\\n",
    "    .filter((size(col('fos')) >= 5) & (size(col('references')) >= 5)) \\\n",
    "    .join(dfAff, df_papers.paperID == dfAff.paperID, \"inner\") \\\n",
    "    .show()\n",
    "#.withColumn('size', size(col('references'))) \\\n",
    "#.sort(col('size').desc()).select(col('title'), col('size')).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mF-dMuin2Ucy",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Query 9: WHERE, GROUP BY, HAVING, 1 JOIN\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aYGmhJiy2Ucy",
    "outputId": "300cae9a-8b5e-409f-d008-99562cd98da5",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Query 10: WHERE, GROUP BY, HAVING, 2 JOINs\n",
    "#Prendere i journal con almeno 5 for e 5  references i cui autori hanno scritto almeno 3 e che hanno lavorato in affiliation di tipo politecnico, mettere anche la order by e limit\n",
    "\n",
    "df_papers \\\n",
    "    .filter((size(col('fos')) >= 5) & (size(col('references')) >= 5)) \\\n",
    "    .join(dfAff, df_papers.paperID == dfAff.paperID, \"inner\") \\\n",
    "    .show()\n",
    "#.withColumn('size', size(col('references'))) \\\n",
    "#.sort(col('size').desc()).select(col('title'), col('size')).show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
