{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U4ugBnQJt8dk",
    "outputId": "85f42e41-b3d2-4c1b-de49-56addeabdf0a"
   },
   "outputs": [],
   "source": [
    "# Dowloading pyspark\n",
    "#p!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PREPARE THE ENVIRONMENT, UPLOAD DATA, PREPROCESS DATA AND CREATE THE TABLES: Author, Paper, Affiliation, Book, Journal and Conference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# With sparkSession we create a connection to our database\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType, TimestampType\n",
    "from pyspark.sql.functions import count, col, xxhash64, collect_list, explode\n",
    "\n",
    "# Create an entry point to the PySpark Application\n",
    "spark = SparkSession.builder \\\n",
    "      .master(\"local\") \\\n",
    "      .appName(\"Bibliography\") \\\n",
    "      .getOrCreate()\n",
    "\n",
    "#INPUT_FILE = \"/content/drive/MyDrive/bibliography.json\"\n",
    "INPUT_FILE = \"/Users/enricosimionato/Desktop/bibliography.json\"\n",
    "OPTIONS = {'multiline': 'true', 'allowNumericLeadingZero': 'true','timestampFormat': \"yyyy-MM-dd'T'HH:mm:ss[.ZZZ'Z']\"}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- author_id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- bio: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+--------------------+--------------------+\n",
      "|           author_id|              name|               email|                 bio|\n",
      "+--------------------+------------------+--------------------+--------------------+\n",
      "|53f3186fdabfae9a8...|   A. M. A. Hariri|a..m..a..hariride...|My name is A. M. ...|\n",
      "|53f3186fdabfae9a8...|    Matthew Prowse|matthew.prowsefb@...|My name is Matthe...|\n",
      "|53f31870dabfae9a8...|       Sui-ping Qi|sui-ping.qi19@gma...|My name is Sui-pi...|\n",
      "|53f31871dabfae9a8...|     Renato Fabbri|renato.fabbrib7@g...|My name is Renato...|\n",
      "|53f31873dabfae9a8...|   Joachim Schimpf|joachim.schimpf8a...|My name is Joachi...|\n",
      "|53f31874dabfae9a8...|    E. Di Bernardo|e..di.bernardo10@...|My name is E. Di ...|\n",
      "|53f31875dabfae9a8...|    Steven F. Roth|steven.f..roth46@...|My name is Steven...|\n",
      "|53f31878dabfae9a8...|      Nima Zahadat|nima.zahadat3d@gm...|My name is Nima Z...|\n",
      "|53f3187ddabfae9a8...|         Ke Fa Cen|ke.fa.cen23@gmail...|My name is Ke Fa ...|\n",
      "|53f31881dabfae9a8...|    Ricky Houghton|ricky.houghton97@...|My name is Ricky ...|\n",
      "|53f31881dabfae9a8...|Eduardo H. Ramirez|eduardo.h..ramire...|My name is Eduard...|\n",
      "|53f31883dabfae9a8...| Cassidy J. Curtis|cassidy.j..curtis...|My name is Cassid...|\n",
      "|53f31885dabfae9a8...|  Brian D. Koblenz|brian.d..koblenz4...|My name is Brian ...|\n",
      "|53f31887dabfae9a8...|        Guohua Wan|guohua.wanca@gmai...|My name is Guohua...|\n",
      "|53f3188adabfae9a8...|     Irineu Theiss|irineu.theissfe@g...|My name is Irineu...|\n",
      "|53f31892dabfae9a8...|     Derek Yip-Hoi|derek.yip-hoidd@g...|My name is Derek ...|\n",
      "|53f31892dabfae9a8...|     Soo-Hyung Kim|soo-hyung.kimf5@g...|My name is Soo-Hy...|\n",
      "|53f31893dabfae9a8...|      Harold Lorin|harold.lorin7c@gm...|My name is Harold...|\n",
      "|53f3189ddabfae9a8...|    J. Nagy-György|j..nagy-györgyf4@...|My name is J. Nag...|\n",
      "|53f3189ddabfae9a8...|    Roy Varshavsky|roy.varshavsky1d@...|My name is Roy Va...|\n",
      "+--------------------+------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#AUTHOR TABLE\n",
    "schemaAut = StructType(\n",
    "            [StructField('authors', ArrayType(StructType([\n",
    "                StructField('_id', StringType(), nullable = False),\n",
    "                StructField('name', StringType(), True),\n",
    "                StructField('email', StringType(), True),\n",
    "                StructField('bio', StringType(), True),\n",
    "                ])), True)\n",
    "            ])\n",
    "\n",
    "df_aut = spark.read.format('json').options(**OPTIONS).schema(schemaAut).json(INPUT_FILE)\n",
    "df_aut = df_aut.select(explode(df_aut.authors))\n",
    "df_aut = df_aut.withColumnRenamed(\"col\", \"authors\")\n",
    "df_aut = df_aut.filter(col(\"authors._id\") != \"null\").select(\"authors._id\",\"authors.name\",\"authors.email\", \"authors.bio\")\n",
    "df_aut = df_aut.withColumnRenamed(\"_id\", \"author_id\")\n",
    "df_aut = df_aut.dropDuplicates([\"author_id\"])\n",
    "df_aut.printSchema()\n",
    "df_aut.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- paper_id: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- keywords: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- fos: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- references: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- page_start: integer (nullable = true)\n",
      " |-- page_end: integer (nullable = true)\n",
      " |-- lang: string (nullable = true)\n",
      " |-- doi: string (nullable = true)\n",
      " |-- url: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- abstract: string (nullable = true)\n",
      " |-- publication_type: string (nullable = true)\n",
      " |-- date: timestamp (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 13:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+----------+--------+----+--------------------+--------------------+--------------------+----------------+-------------------+\n",
      "|            paper_id|               title|            keywords|                 fos|          references|page_start|page_end|lang|                 doi|                 url|            abstract|publication_type|               date|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+----------+--------+----+--------------------+--------------------+--------------------+----------------+-------------------+\n",
      "|53e99784b7602d970...|Using XML to Inte...|[internet, hyperm...|[xml base, world ...|[53e9adbdb7602d97...|       167|     172|  en|10.1109/CMPSAC.20...|[http://dx.doi.or...|The eXtensible Ma...|            Book|1974-09-13 06:34:29|\n",
      "|53e99784b7602d970...|               FCLOS|[molap, subsumpti...|[information syst...|[53e99ee0b7602d97...|       192|     220|  en|10.1016/j.datak.2...|[http://dx.doi.or...|Mobile online ana...|         Journal|1950-04-14 05:33:22|\n",
      "|53e99785b7602d970...|              Bhoomi|[icts, e governan...|[revenue, transpa...|[53e9b2ffb7602d97...|        20|      31|  en|10.1016/j.tele.20...|[http://dx.doi.or...|The emergence of ...|         Journal|1984-04-06 03:49:29|\n",
      "|53e9978ab7602d970...|                Laps|[health care, hom...|[health care, pop...|[573695936e3b1202...|       962|     976|  en|10.1016/j.ejor.20...|[http://dx.doi.or...|The health care s...|         Journal|1951-10-28 04:52:04|\n",
      "|53e9978db7602d970...|             Mindful|[meta learning, h...|[data science, ro...|[53e9b0c2b7602d97...|      3253|    3274|  en|10.1016/j.ins.200...|[http://dx.doi.or...|Common inductive ...|         Journal|1955-04-13 19:06:45|\n",
      "|53e9978db7602d970...|             MESHMdl|[tuple space, mob...|[middleware, mobi...|[53e99a49b7602d97...|       467|     487|  en|10.1016/j.pmcj.20...|[http://dx.doi.or...|Mobile ad hoc net...|         Journal|2009-09-23 09:35:36|\n",
      "|53e9978db7602d970...|               iCity|[irregular cellul...|[software tool, a...|[53e9ac54b7602d97...|       761|     773|  en|10.1016/j.envsoft...|[http://dx.doi.or...|The objective of ...|         Journal|2002-05-14 19:30:38|\n",
      "|53e9978db7602d970...|              Wisdom|[decision support...|[cognitive map, r...|[53e99b7eb7602d97...|       156|     171|  en|10.1016/j.ejor.20...|[http://dx.doi.or...|Many decision sup...|         Journal|1987-12-30 04:51:23|\n",
      "|53e99792b7602d970...|        SwissAnalyst|         [key words]|[computer science...|[5c795a6e4895d9cb...|       393|     406|  en|10.1007/1-4020-81...|                null|This paper introd...|      Conference|1991-03-30 19:52:33|\n",
      "|53e99792b7602d970...|Short-Term Traffi...|[considerable acc...|[spline mathemati...|[53e9b95bb7602d97...|       669|     675|  en|10.1109/FSKD.2008...|[http://dx.doi.or...|A promising traff...|      Conference|2020-03-15 04:13:40|\n",
      "|53e99792b7602d970...|An approach to fe...|[feature location...|[data mining, cau...|[53e9b6eeb7602d97...|        57|      68|  en|10.1016/j.jss.200...|[http://dx.doi.or...|This paper descri...|         Journal|2012-05-05 11:48:40|\n",
      "|53e99792b7602d970...|Nested Graph-Stru...|[nested graph str...|[adjacency matrix...|[53e9b049b7602d97...|         1|      12|  en|  10.1007/BFb0056317|[http://dx.doi.or...|This paper descri...|            Book|1965-07-21 08:40:34|\n",
      "|53e99792b7602d970...|Demo: Yalut -- us...|[cellular data tr...|[world wide web, ...|[53e9b04eb7602d97...|       360|     361|  en|10.1145/2594368.2...|[http://dx.doi.or...|Yalut is a novel ...|      Conference|2009-02-24 08:00:34|\n",
      "|53e99792b7602d970...|A Uniform Paralle...|[knowledge discov...|[data mining, dat...|[53e99cbbb7602d97...|       306|     312|  en|10.1007/978-3-540...|[http://dx.doi.or...|Grid is a new sol...|         Journal|2015-02-28 22:51:57|\n",
      "|53e99792b7602d970...|WAVELET-BASED IMA...|[mathematical mod...|[computer vision,...|[53e9b550b7602d97...|      1737|    1740|  en|10.1109/ICIP.2010...|[http://dx.doi.or...|Because digital i...|      Conference|2005-12-16 23:43:26|\n",
      "|53e99792b7602d970...|A fuzzy multi-obj...|[location, fire s...|[objective progra...|[53e9bd50b7602d97...|       903|     915|  en|10.1016/j.ejor.20...|[http://dx.doi.or...|Location of fire ...|         Journal|1971-12-17 16:55:38|\n",
      "|53e99792b7602d970...|Adaptive presenta...|[evolving informa...|[metadata, progra...|[53e9a2c8b7602d97...|       193|     196|  en|10.1109/ICALT.200...|[http://dx.doi.or...|The article descr...|            Book|2011-05-08 17:41:02|\n",
      "|53e99792b7602d970...|Testing the stabi...|[asymptotic justi...|[functional princ...|[53e9ace1b7602d97...|       352|     367|  en|10.1016/j.jmva.20...|[http://dx.doi.or...|The functional au...|         Journal|1984-05-13 13:56:58|\n",
      "|53e99792b7602d970...|Statistical Prope...|[power cables, po...|[topology, trippi...|[53e9a7ddb7602d97...|      2517|    2526|  en|10.1109/HICSS.201...|[http://dx.doi.or...|We present the sy...|      Conference|2018-08-18 19:58:00|\n",
      "|53e99792b7602d970...|Generating novel ...|[adaptive behavio...|[interactive evol...|[53e9bc80b7602d97...|         8|      14|  en|10.1145/1056754.1...|[http://dx.doi.or...|Simulated evoluti...|         Journal|2021-05-05 05:11:38|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+----------+--------+----+--------------------+--------------------+--------------------+----------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# PAPER TABLE WITHOUT PUBLICATION_ID\n",
    "schemaPaper = StructType(\n",
    "            [StructField('_id', StringType(), True),\n",
    "             StructField('title', StringType(),True),\n",
    "             StructField('keywords', ArrayType(StringType()), True),\n",
    "             StructField('fos', ArrayType(StringType()), True),\n",
    "             StructField('references', ArrayType(StringType()), True),\n",
    "             StructField('page_start', IntegerType(), True),\n",
    "             StructField('page_end', IntegerType(), True),\n",
    "             StructField('lang', StringType(),True),\n",
    "             StructField('doi', StringType(),True),\n",
    "             StructField('url', ArrayType(StringType()),True),\n",
    "             StructField('abstract', StringType(),True),\n",
    "             StructField('publication_type', StringType(),True),\n",
    "             StructField('date', TimestampType(), True)\n",
    "            ])\n",
    "\n",
    "df_paper = spark.read.format('json').options(**OPTIONS).schema(schemaPaper).json(INPUT_FILE)\n",
    "df_paper = df_paper.withColumnRenamed(\"_id\", \"paper_id\")\n",
    "df_paper.printSchema()\n",
    "df_paper.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- paper_id: string (nullable = true)\n",
      " |-- author_id: string (nullable = true)\n",
      " |-- organization: string (nullable = true)\n",
      "\n",
      "+--------------------+--------------------+--------------------+\n",
      "|            paper_id|           author_id|        organization|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|53e998c7b7602d970...|53f3186fdabfae9a8...|Department of Sta...|\n",
      "|53e99827b7602d970...|53f3186fdabfae9a8...|Laboratory for Fo...|\n",
      "|53e99924b7602d970...|53f31870dabfae9a8...|Henan Academy of ...|\n",
      "|53e998dbb7602d970...|53f31871dabfae9a8...|Instituto de Físi...|\n",
      "|53e998f6b7602d970...|53f31873dabfae9a8...|                null|\n",
      "|53e998bfb7602d970...|53f31874dabfae9a8...|                null|\n",
      "|53e9984bb7602d970...|53f31875dabfae9a8...|                null|\n",
      "|53e998e8b7602d970...|53f31878dabfae9a8...|George Mason Univ...|\n",
      "|53e99905b7602d970...|53f3187ddabfae9a8...|State Key Laborat...|\n",
      "|53e998e9b7602d970...|53f31881dabfae9a8...|                null|\n",
      "|53e9984fb7602d970...|53f31881dabfae9a8...|Tecnologico de Mo...|\n",
      "|53e9980eb7602d970...|53f31883dabfae9a8...|University of Was...|\n",
      "|53e997e9b7602d970...|53f31885dabfae9a8...|Digital Equipment...|\n",
      "|53e998b0b7602d970...|53f31887dabfae9a8...|Antai College of ...|\n",
      "|53e998cdb7602d970...|53f31887dabfae9a8...|Department of Ind...|\n",
      "|53e998f0b7602d970...|53f3188adabfae9a8...|E-Gov, Juridical ...|\n",
      "|53e998fcb7602d970...|53f31892dabfae9a8...|2250 G G Brown, A...|\n",
      "|53e99998b7602d970...|53f31892dabfae9a8...|Chonnam National ...|\n",
      "|53e99800b7602d970...|53f31893dabfae9a8...|IBM Systems Resea...|\n",
      "|53e99866b7602d970...|53f3189ddabfae9a8...|Department of Mat...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# AFFILIATION TABLE\n",
    "schemaAffiliation = StructType(\n",
    "            [StructField('_id', StringType(), True),\n",
    "             StructField('authors', ArrayType(StructType([\n",
    "                    StructField('_id', StringType(), True),\n",
    "                    StructField('org', StringType(), True)\n",
    "             ])), True),\n",
    "            ])\n",
    "\n",
    "df_aff = spark.read.format('json').options(**OPTIONS).schema(schemaAffiliation).json(INPUT_FILE)\n",
    "df_aff = df_aff.withColumnRenamed(\"_id\", \"paper_id\")\n",
    "df_aff = df_aff.select(\"paper_id\", explode(df_aff.authors))\n",
    "df_aff = df_aff.withColumnRenamed(\"col\", \"authors\")\n",
    "df_aff = df_aff.filter(col(\"authors._id\") != \"null\").filter(col(\"paper_id\") != \"null\").select(\"paper_id\", \"authors._id\",\"authors.org\")\n",
    "df_aff = df_aff.withColumnRenamed(\"_id\", \"author_id\")\n",
    "df_aff = df_aff.dropDuplicates([\"author_id\", \"paper_id\"])\n",
    "df_aff = df_aff.withColumnRenamed(\"org\", \"organization\")\n",
    "df_aff.printSchema()\n",
    "df_aff.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema of the journals\n",
      "root\n",
      " |-- venue: string (nullable = true)\n",
      " |-- volume: integer (nullable = true)\n",
      " |-- issue: integer (nullable = true)\n",
      " |-- publisher: string (nullable = true)\n",
      " |-- issn: string (nullable = true)\n",
      " |-- publication_id: long (nullable = false)\n",
      "\n",
      "Journals\n",
      "+-----+------+-----+---------+----+--------------+\n",
      "|venue|volume|issue|publisher|issn|publication_id|\n",
      "+-----+------+-----+---------+----+--------------+\n",
      "+-----+------+-----+---------+----+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# JOURNAL TABLE\n",
    "# Preprocessing of the journals for cleaning and merging the journals\n",
    "\n",
    "journal_schema_preprocessing = StructType(\n",
    "    [StructField('_id', StringType(), True),\n",
    "     StructField('issn', StringType(), True),\n",
    "     StructField('publisher', StringType(), True),\n",
    "     StructField('venue', StringType(), True),\n",
    "     StructField('volume', IntegerType(), True),\n",
    "     StructField('issue', IntegerType(), True),\n",
    "     StructField('publication_type', StringType(),True)])\n",
    "\n",
    "# Reading the json file\n",
    "df_journals_to_filter = spark.read.format('json').options(**OPTIONS).schema(journal_schema_preprocessing).json(INPUT_FILE)\n",
    "\n",
    "# Filtering and adjusting the dataframe\n",
    "df_journals_to_filter = df_journals_to_filter.filter(col('publication_type') == 'Journal').filter(col('issn') != 'null').filter(col('venue') != 'null').filter(col('issue') >= 0).filter(col('volume') >= 0)\n",
    "df_journals_to_filter = df_journals_to_filter.groupBy('venue', 'volume', 'issue', 'issn').agg(collect_list('publisher').alias('publishersArray'), collect_list('_id').alias('_id'), count(col('publisher'))) # count can be removed (I was interested in evaluating if the group by was meaningful)\n",
    "df_journals_to_insert = df_journals_to_filter.withColumn('publisher', df_journals_to_filter['publishersArray'][0]).select('venue', 'volume', 'issue', 'publisher', 'issn', '_id')\n",
    "\n",
    "# Adding the new column which contains the publication_identifier\n",
    "df_journals = df_journals_to_insert.withColumn(\"publication_id\", xxhash64('venue', 'volume', 'issue', 'issn'))\n",
    "\n",
    "# Adding the foreign key to the papers\n",
    "exploded_journals = df_journals.select(explode('_id'), 'publication_id')\n",
    "#exploded_journals.show(truncate = False)\n",
    "\n",
    "df_papers_in_journals = exploded_journals.join(df_paper, exploded_journals.col == df_paper.paper_id, \"inner\")\n",
    "df_papers_in_journals = df_papers_in_journals.drop('col')\n",
    "\n",
    "df_journals = df_journals.drop(df_journals._id)\n",
    "\n",
    "# Visualizing the data\n",
    "# print('Papers')\n",
    "# df_papers_in_journals.show(truncate = False)\n",
    "print('Schema of the journals')\n",
    "df_journals.printSchema()\n",
    "print('Journals')\n",
    "df_journals.show(truncate=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema of the books\n",
      "root\n",
      " |-- venue: string (nullable = true)\n",
      " |-- isbn: string (nullable = true)\n",
      " |-- publisher: string (nullable = true)\n",
      " |-- publication_id: long (nullable = false)\n",
      "\n",
      "Books\n",
      "+------------------------------------------------------------------------------------+-------------+--------------------------------------------------+--------------------+\n",
      "|venue                                                                               |isbn         |publisher                                         |publication_id      |\n",
      "+------------------------------------------------------------------------------------+-------------+--------------------------------------------------+--------------------+\n",
      "|ACM SIGSOFT Software Engineering Notes                                              |-159593-125-2|AGH University of Science and Technology          |-4146681027596907540|\n",
      "|Theor. Comput. Sci.                                                                 |0-0304-3975  |Elsevier                                          |-8641604748836216424|\n",
      "|ACL (Companion)                                                                     |0-111-456789 |Intellect Ltd.                                    |-1765624198251536382|\n",
      "|VLDB                                                                                |0-12-088469-0|The American Society of Mechanical Engineers(ASME)|-6546099223240010863|\n",
      "|Papers from the second workshop Vol. 13 on Human and Machine Vision II              |0-12-597345-4|SpringerOpen                                      |-5170613698490267273|\n",
      "|VLDB                                                                                |0-12-722442-4|Open Library of Humanities                        |5803877754064281982 |\n",
      "|Artif. Intell. Rev.                                                                 |0-13-176751-8|World Scientific Publishing Co. Pte Ltd           |-5191022792199354246|\n",
      "|SFCS '85 Proceedings of the 26th Annual Symposium on Foundations of Computer Science|0-13-652017-0|Elsevier                                          |4898287511903231544 |\n",
      "|CHI                                                                                 |0-201-30987-4|AGH University of Science and Technology          |4745093320936461129 |\n",
      "|Computer Human Interaction                                                          |0-201-30987-4|Brown University                                  |-3213114594400519435|\n",
      "|SIGGRAPH                                                                            |0-201-48560-5|Springer Science + Business Media                 |7678090193378385384 |\n",
      "|CHI                                                                                 |0-201-50932-6|Elsevier                                          |22285894483278744   |\n",
      "|Sigplan Notices                                                                     |0-201-53372-3|Elsevier                                          |127134876966261428  |\n",
      "|ISSAC                                                                               |0-201-54892-5|Taylor and Francis Ltd.                           |-1355898789984227595|\n",
      "|History of programming languages---II                                               |0-201-89502-1|Springer Verlag                                   |-2667848576388063536|\n",
      "|Neural Networks                                                                     |0-262-03188-4|Elsevier                                          |4141971837342876240 |\n",
      "|Computational Statistics                                                            |0-262-10076-2|Springer Science + Business Media                 |-619826641325096993 |\n",
      "|Symposium on Programming Language Implementation and Logic Programming              |0-262-19297-7|Elsevier                                          |6701520537364205852 |\n",
      "|AAAI                                                                                |0-262-51057-X|Tsinghua University Press                         |1804946402782986900 |\n",
      "|Artif. Intell.                                                                      |0-262-51091-X|Editorial and Publishing Board of JIG             |529998888475133215  |\n",
      "+------------------------------------------------------------------------------------+-------------+--------------------------------------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# BOOK TABLE\n",
    "# Preprocessing of the books for cleaning and merging the books\n",
    "book_schema_preprocessing = StructType(\n",
    "    [StructField('_id', StringType(), True),\n",
    "     StructField('isbn', StringType(), True),\n",
    "     StructField('publisher', StringType(), True),\n",
    "     StructField('venue', StringType(), True),\n",
    "     StructField('publication_type', StringType(),True)])\n",
    "\n",
    "# Reading the json file\n",
    "df_books_to_filter = spark.read.format('json').options(**OPTIONS).schema(book_schema_preprocessing).json(INPUT_FILE)\n",
    "\n",
    "# Filtering and adjusting the dataframe\n",
    "df_books_to_filter = df_books_to_filter.filter(col('publication_type') == 'Book').filter(col('isbn') != 'null').filter(col('venue') != 'null')\n",
    "df_books_to_filter = df_books_to_filter.groupBy('isbn', 'venue').agg(collect_list('publisher').alias('publishersArray'), collect_list('_id').alias('_id'), count(col('publisher'))) # count can be removed (I was interested in evaluating if the group by was meaningful)\n",
    "dfbooks_to_insert = df_books_to_filter.withColumn('publisher', df_books_to_filter['publishersArray'][0]).select('venue', 'isbn', 'publisher', '_id')\n",
    "\n",
    "# Adding the new column which is the id\n",
    "df_books = dfbooks_to_insert.withColumn('publication_id', xxhash64('isbn', 'venue'))\n",
    "\n",
    "# Adding the foreign key to the papers\n",
    "exploded_books = df_books.select(explode('_id'), 'publication_id')\n",
    "# exploded_books.show(truncate = False)\n",
    "\n",
    "df_papers_in_books = exploded_books.join(df_paper, exploded_books.col == df_paper.paper_id)\n",
    "df_papers_in_books = df_papers_in_books.drop('col')\n",
    "\n",
    "df_books = df_books.drop(df_books._id)\n",
    "\n",
    "# Visualizing the data\n",
    "# print('Papers')\n",
    "# df_papers_in_books.show(truncate = False)\n",
    "print('Schema of the books')\n",
    "df_books.printSchema()\n",
    "print('Books')\n",
    "df_books.show(truncate=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema of the conferences\n",
      "root\n",
      " |-- venue: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- publication_id: long (nullable = false)\n",
      "\n",
      "Conferences\n",
      "+----------------------------------------------------------------------------------------+--------------------------+--------------------+\n",
      "|venue                                                                                   |location                  |publication_id      |\n",
      "+----------------------------------------------------------------------------------------+--------------------------+--------------------+\n",
      "|\"EDUCON                                                                                 |Moscow, Russia            |950373860555954453  |\n",
      "|2012 50TH ANNUAL ALLERTON CONFERENCE ON COMMUNICATION, CONTROL, AND COMPUTING (ALLERTON)|Dublin, Ireland           |-4245717996156385657|\n",
      "|2985415099                                                                              |Mexico City, Mexico       |-762166203857654039 |\n",
      "|2985532720                                                                              |Milan, Italy              |-7124098142925130015|\n",
      "|2987493177                                                                              |Osaka, Japan              |7551243357793819285 |\n",
      "|3D-GIS                                                                                  |London, UK                |1334491883820369589 |\n",
      "|3DIM                                                                                    |Johannesburg, South Africa|-1039299792543803944|\n",
      "|3DIMPVT                                                                                 |Helsinki, Finland         |-4577231382119670667|\n",
      "|3DOR                                                                                    |London, UK                |-8797854120325021263|\n",
      "|3DPVT                                                                                   |Amsterdam, Netherlands    |1375276120944605061 |\n",
      "|3DUI                                                                                    |Seoul, South Korea        |1791021788110955539 |\n",
      "|3DV                                                                                     |Jakarta, Indonesia        |3883608681790763346 |\n",
      "|3PGCIC                                                                                  |Beijing, China            |-3347673940037205862|\n",
      "|50 Years of Integer Programming                                                         |London, UK                |-5154516725105661246|\n",
      "|AAMAS                                                                                   |Milan, Italy              |5503995461761334696 |\n",
      "|AAMAS (2)                                                                               |Paris, France             |-7242857281816016648|\n",
      "|ACAI                                                                                    |Toronto, Canada           |8524679771896464283 |\n",
      "|ACC'09 Proceedings of the 2009 conference on American Control Conference                |Beijing, China            |-3525577454028830485|\n",
      "|ACCV                                                                                    |London, UK                |8287527867910422941 |\n",
      "|ACCV (1)                                                                                |Tel Aviv, Israel          |-2757184551560203638|\n",
      "+----------------------------------------------------------------------------------------+--------------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# CONFERENCE TABLE\n",
    "# Preprocessing of the books for cleaning and merging the books\n",
    "\n",
    "schemaConf = StructType(\n",
    "    [StructField('_id', StringType(), True),\n",
    "     StructField('location', StringType(), True),\n",
    "     StructField('venue', StringType(), True),\n",
    "     StructField('publication_type', StringType(),True)])\n",
    "# Reading the json file\n",
    "df_conferences_to_filter = spark.read.format('json').options(**OPTIONS).schema(schemaConf).json(INPUT_FILE)\n",
    "\n",
    "# Filtering and adjusting the dataframe\n",
    "df_conferences_to_filter = df_conferences_to_filter.filter(col('publication_type') == 'Conference').filter(col('venue') != 'null')\n",
    "df_conferences_to_filter = df_conferences_to_filter.groupBy('venue').agg(collect_list('location').alias('locations_array'), collect_list('_id').alias('_id'), count(col('location'))) # count can be removed (I was interested in evaluating if the group by was meaningful)\n",
    "df_conferences_to_insert = df_conferences_to_filter.withColumn('location', df_conferences_to_filter['locations_array'][0]).select('venue', 'location', '_id')\n",
    "\n",
    "# Adding the new column which is the id\n",
    "df_conferences = df_conferences_to_insert.withColumn('publication_id', xxhash64('venue'))\n",
    "\n",
    "# Adding the foreign key to the papers\n",
    "exploded_conferences = df_conferences.select(explode('_id'), 'publication_id')\n",
    "#exploded_conferences.show(truncate = False)\n",
    "\n",
    "df_papers_in_conferences = exploded_conferences.join(df_paper, exploded_conferences.col == df_paper.paper_id)\n",
    "df_papers_in_conferences = df_papers_in_conferences.drop('col')\n",
    "\n",
    "df_conferences = df_conferences.drop(df_conferences._id)\n",
    "\n",
    "# Visualizing the data\n",
    "#print('Papers')\n",
    "#df_papers_in_conferences.show(truncate = False)\n",
    "print('Schema of the conferences')\n",
    "df_conferences.printSchema()\n",
    "print('Conferences')\n",
    "df_conferences.show(truncate=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Papers schema\n",
      "root\n",
      " |-- publication_id: long (nullable = false)\n",
      " |-- paper_id: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- keywords: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- fos: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- references: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- page_start: integer (nullable = true)\n",
      " |-- page_end: integer (nullable = true)\n",
      " |-- lang: string (nullable = true)\n",
      " |-- doi: string (nullable = true)\n",
      " |-- url: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- abstract: string (nullable = true)\n",
      " |-- publication_type: string (nullable = true)\n",
      " |-- date: timestamp (nullable = true)\n",
      "\n",
      "Papers data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 27:>                 (0 + 1) / 1][Stage 29:>                 (0 + 0) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+--------+----+--------------------+--------------------+--------------------+----------------+-------------------+\n",
      "|      publication_id|            paper_id|               title|            keywords|                 fos|          references|page_start|page_end|lang|                 doi|                 url|            abstract|publication_type|               date|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+--------+----+--------------------+--------------------+--------------------+----------------+-------------------+\n",
      "|-4336127124407324726|53e997d1b7602d970...|A fault diagnosis...|[stuck at defects...|[computer testing...|[53e9bca6b7602d97...|       494|     494|  en|10.1109/EDTC.1997...|[http://dx.doi.or...|In this paper we ...|            Book|1950-09-08 12:23:35|\n",
      "| 4674035537543784623|53e997e4b7602d970...|Problem Decomposi...|[problem decompos...|[inductive logic ...|[53e9a690b7602d97...|        17|      31|  en|10.1007/3-540-592...|[http://dx.doi.or...|One dimension of ...|            Book|1986-12-29 07:27:28|\n",
      "| 1144530160964015551|53e997e8b7602d970...|X-tract: Structur...|[information retr...|[information stru...|[53e9b0d8b7602d97...|         2|       2|  en|10.1109/SPIRE.199...|[http://computer....|Most available in...|            Book|1954-04-24 05:44:09|\n",
      "|-6465152039831803131|53e997e8b7602d970...|Cognitive agent p...|[dynamic logic, a...|[functional logic...|[53e9b38fb7602d97...|      1385|    1385|  en|10.1145/1082473.1...|[http://dx.doi.or...|This is a summary...|            Book|1992-05-16 17:49:48|\n",
      "| 5112790730981969721|53e997e9b7602d970...|Constraint based ...|[uniform framewor...|[computer science...|[53e9b326b7602d97...|       195|     204|  en|10.1145/318789.31...|[http://dx.doi.or...|The constraint tr...|            Book|1978-02-04 19:19:02|\n",
      "| 1369111316993402587|53e997ecb7602d970...|Automatic input r...|[google picasa, d...|[novel technique,...|[53e9a29db7602d97...|        80|      90|  en|10.1109/ICSE.2012...|[http://dx.doi.or...|We present a nove...|            Book|1971-08-25 11:47:59|\n",
      "| 1430384888819899512|53e997ecb7602d970...|Acceptability-ori...|[monitoring, acce...|[programming lang...|[53e9badfb7602d97...|        57|      75|  en|10.1145/966051.96...|[http://dx.doi.or...|We discuss a new ...|            Book|1964-08-09 22:48:00|\n",
      "|  710748220667054414|53e997ecb7602d970...|Anomalous Neighbo...|[column wise diff...|[mathematical opt...|[53e9a0a6b7602d97...|       474|     480|  en|10.1109/ICDMW.201...|[http://dx.doi.or...|We propose a meth...|            Book|1996-03-16 14:37:00|\n",
      "| 8148044123726554362|53e997f1b7602d970...|A Digital Watermark.|[authentication, ...|[computer vision,...|[53e99822b7602d97...|        86|      90|  en|10.1109/ICIP.1994...|[http://dx.doi.or...|This paper discus...|            Book|2001-12-04 20:38:24|\n",
      "|-8250923667053511275|53e997f4b7602d970...|Independent Tree ...|[independent tree...|[combinatorics, m...|[53e9b2f5b7602d97...|       203|     214|  en| 10.1007/10692760_17|[http://dx.doi.or...|\\n For any fixed ...|            Book|2014-02-26 04:06:46|\n",
      "| 8160813231728112221|53e997f5b7602d970...|Inverting onto fu...|[nondeterministic...|[computer science...|[53e9b4afb7602d97...|       213|     222|  en|10.1016/S0890-540...|[http://dx.doi.or...|We look at the hy...|            Book|2015-12-31 07:56:33|\n",
      "| 6712300784079919943|53e9980eb7602d970...|   EVES: An Overview|[never, formal me...|[programming lang...|[53e9a2bab7602d97...|       389|     405|  en|10.1007/3-540-548...|[http://dx.doi.or...|In this paper we ...|            Book|1972-08-07 17:35:34|\n",
      "|-1585425199704921662|53e99813b7602d970...|Distributed proba...|[error correction...|[binary pattern, ...|[53e99a67b7602d97...|      3398|    3401|  en|10.1109/ICASSP.20...|[http://dx.doi.or...|This paper introd...|            Book|2016-08-28 14:12:19|\n",
      "| 6134863903676584956|53e99818b7602d970...|Regulations by Va...|[power method, no...|[context sensitiv...|[53e9a946b7602d97...|       239|     248|  en|  10.1007/BFb0029967|[http://dx.doi.or...| . Valences are a...|            Book|1961-02-26 08:04:12|\n",
      "| 2530147181524079819|53e99818b7602d970...|Reflections on sy...|[object oriented,...|[communication de...|[53e99832b7602d97...|        28|      33|  en|10.1145/1556262.1...|[http://dx.doi.or...|Symmetry is routi...|            Book|2009-10-03 17:07:24|\n",
      "| 2735482298684145043|53e99827b7602d970...|Timestamping afte...|[committed transa...|[transaction proc...|[53e9b587b7602d97...|       160|     167|  en|10.1109/PDIS.1994...|[http://dx.doi.or...|Many applications...|            Book|1951-11-27 17:50:37|\n",
      "| 5744511372260825900|53e99827b7602d970...|The Animachine re...|[useful effect, v...|[3d computer grap...|[53e9ab2bb7602d97...|        90|      90|  en|10.1109/CA.1995.3...|[http://doi.ieeec...|Two-dimensional a...|            Book|2007-10-16 04:48:40|\n",
      "| 2483812910487536561|53e99827b7602d970...|     The CD1 system.|[advanced constra...|[sql, conceptual ...|[53e99967b7602d97...|         1|       9|  en|10.1007/978-3-642...|[http://dx.doi.or...|\\n We describe a ...|            Book|2013-03-31 03:16:37|\n",
      "| 8942054755879642437|53e9982cb7602d970...|Public-Key Regist...|[registration req...|[fixed access, cr...|[53e99a74b7602d97...|       451|     458|  en|10.1007/3-540-477...|[http://dx.doi.or...|A procedure is de...|            Book|1993-11-20 16:32:56|\n",
      "| 8730891231843712925|53e9983db7602d970...|Learning Automata...|[blue fringe edsm...|[quantum finite a...|[53e9afa6b7602d97...|        52|      65|  en|10.1007/978-3-642...|[http://dx.doi.or...|\\n We prove in th...|            Book|1960-12-16 13:42:14|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+--------+----+--------------------+--------------------+--------------------+----------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Merging the 3 dataframe which one contains the papers published in a specific media\n",
    "df_papers = df_papers_in_books.union(df_papers_in_journals).union(df_papers_in_conferences)\n",
    "\n",
    "# Visualizing the data\n",
    "print('Papers schema')\n",
    "df_papers.printSchema()\n",
    "print('Papers data')\n",
    "df_papers.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Papers published in books\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----------------+--------------------+\n",
      "|            paper_id|               title|publication_type|      publication_id|\n",
      "+--------------------+--------------------+----------------+--------------------+\n",
      "|53e997d1b7602d970...|A fault diagnosis...|            Book|-4336127124407324726|\n",
      "|53e997e4b7602d970...|Problem Decomposi...|            Book| 4674035537543784623|\n",
      "|53e997e8b7602d970...|X-tract: Structur...|            Book| 1144530160964015551|\n",
      "|53e997e8b7602d970...|Cognitive agent p...|            Book|-6465152039831803131|\n",
      "|53e997e9b7602d970...|Constraint based ...|            Book| 5112790730981969721|\n",
      "|53e997ecb7602d970...|Automatic input r...|            Book| 1369111316993402587|\n",
      "|53e997ecb7602d970...|Acceptability-ori...|            Book| 1430384888819899512|\n",
      "|53e997ecb7602d970...|Anomalous Neighbo...|            Book|  710748220667054414|\n",
      "|53e997f1b7602d970...|A Digital Watermark.|            Book| 8148044123726554362|\n",
      "|53e997f4b7602d970...|Independent Tree ...|            Book|-8250923667053511275|\n",
      "|53e997f5b7602d970...|Inverting onto fu...|            Book| 8160813231728112221|\n",
      "|53e9980eb7602d970...|   EVES: An Overview|            Book| 6712300784079919943|\n",
      "|53e99813b7602d970...|Distributed proba...|            Book|-1585425199704921662|\n",
      "|53e99818b7602d970...|Regulations by Va...|            Book| 6134863903676584956|\n",
      "|53e99818b7602d970...|Reflections on sy...|            Book| 2530147181524079819|\n",
      "|53e99827b7602d970...|Timestamping afte...|            Book| 2735482298684145043|\n",
      "|53e99827b7602d970...|The Animachine re...|            Book| 5744511372260825900|\n",
      "|53e99827b7602d970...|     The CD1 system.|            Book| 2483812910487536561|\n",
      "|53e9982cb7602d970...|Public-Key Regist...|            Book| 8942054755879642437|\n",
      "|53e9983db7602d970...|Learning Automata...|            Book| 8730891231843712925|\n",
      "+--------------------+--------------------+----------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Papers published in journals\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+----------------+--------------+\n",
      "|paper_id|title|publication_type|publication_id|\n",
      "+--------+-----+----------------+--------------+\n",
      "+--------+-----+----------------+--------------+\n",
      "\n",
      "Papers published in conferences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 102:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----------------+--------------------+\n",
      "|            paper_id|               title|publication_type|      publication_id|\n",
      "+--------------------+--------------------+----------------+--------------------+\n",
      "|53e99854b7602d970...|  The EOLES project.|      Conference|  950373860555954453|\n",
      "|53e9989bb7602d970...|Life is engineeri...|      Conference|  950373860555954453|\n",
      "|53e998bfb7602d970...|Gaining and maint...|      Conference|  950373860555954453|\n",
      "|53e998c0b7602d970...|From manuals towa...|      Conference|  950373860555954453|\n",
      "|53e998e9b7602d970...|Learning with com...|      Conference|  950373860555954453|\n",
      "|53e99976b7602d970...|Cloud E-learning ...|      Conference|  950373860555954453|\n",
      "|53e9997eb7602d970...|Motivating progra...|      Conference|  950373860555954453|\n",
      "|53e99991b7602d970...|Monitoring studen...|      Conference|  950373860555954453|\n",
      "|53e99998b7602d970...|OLAREX project: O...|      Conference|  950373860555954453|\n",
      "|53e99859b7602d970...|Studying dynamic ...|      Conference|-4245717996156385657|\n",
      "|53e99860b7602d970...|Equivalence of br...|      Conference|-4245717996156385657|\n",
      "|53e998bfb7602d970...|Fish Schools: PDE...|      Conference| -762166203857654039|\n",
      "|53e9997eb7602d970...|Automatic Generat...|      Conference|-7124098142925130015|\n",
      "|53e998fdb7602d970...|Fault-Tolerant Sc...|      Conference| 7551243357793819285|\n",
      "|53e9990db7602d970...|Automatic Generat...|      Conference| 1334491883820369589|\n",
      "|53e99822b7602d970...|Stroboscopic Ster...|      Conference|-1039299792543803944|\n",
      "|53e998bfb7602d970...|Image-Based Techn...|      Conference|-1039299792543803944|\n",
      "|53e9991cb7602d970...|Scanning and Proc...|      Conference|-1039299792543803944|\n",
      "|53e99940b7602d970...|Range Image Regis...|      Conference|-1039299792543803944|\n",
      "|53e99937b7602d970...|Cage-Based Motion...|      Conference|-4577231382119670667|\n",
      "+--------------------+--------------------+----------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# For checking the result\n",
    "print('Papers published in books')\n",
    "df_papers.filter(col('publication_type') == 'Book').select('paper_id', 'title', 'publication_type', 'publication_id').show()\n",
    "print('Papers published in journals')\n",
    "df_papers.filter(col('publication_type') == 'Journal').select('paper_id', 'title', 'publication_type', 'publication_id').show()\n",
    "print('Papers published in conferences')\n",
    "df_papers.filter(col('publication_type') == 'Conference').select('paper_id', 'title', 'publication_type', 'publication_id').show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "COMMANDS"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Column '_id' does not exist. Did you mean one of the following? [doi, fos, url, date, lang, title, paper_id, page_end, keywords, abstract, page_start, references, publication_id, publication_type];\n'Project ['_id, title#672, keywords#673, fos#674, references#675, page_start#676, page_end#677, lang#678, doi#9900, url#9915, abstract#681, publication_type#682, date#683, publication_id#1015L]\n+- Project [publication_id#1015L, paper_id#697, title#672, keywords#673, fos#674, references#675, page_start#676, page_end#677, lang#678, abstract#681, publication_type#682, date#683, doi#9900, new_url#9854 AS url#9915]\n   +- Project [publication_id#1015L, paper_id#697, title#672, keywords#673, fos#674, references#675, page_start#676, page_end#677, lang#678, abstract#681, publication_type#682, date#683, new_doi#9838 AS doi#9900, new_url#9854]\n      +- Project [publication_id#1015L, paper_id#697, title#672, keywords#673, fos#674, references#675, page_start#676, page_end#677, lang#678, abstract#681, publication_type#682, date#683, new_doi#9838, new_url#9854]\n         +- Project [publication_id#1015L, paper_id#697, title#672, keywords#673, fos#674, references#675, page_start#676, page_end#677, lang#678, url#680, abstract#681, publication_type#682, date#683, new_doi#9838, new_url#9854]\n            +- Project [publication_id#1015L, paper_id#697, title#672, keywords#673, fos#674, references#675, page_start#676, page_end#677, lang#678, doi#679, url#680, abstract#681, publication_type#682, date#683, new_doi#9838, array(https://link.springer.com/chapter/10.1007/11944577_37) AS new_url#9854]\n               +- Project [publication_id#1015L, paper_id#697, title#672, keywords#673, fos#674, references#675, page_start#676, page_end#677, lang#678, doi#679, url#680, abstract#681, publication_type#682, date#683, 10.1007/11944577_37 AS new_doi#9838]\n                  +- Filter (paper_id#697 = 53e997e4b7602d9701fdb48a)\n                     +- Union false, false\n                        :- Project [publication_id#1015L, paper_id#697, title#672, keywords#673, fos#674, references#675, page_start#676, page_end#677, lang#678, doi#679, url#680, abstract#681, publication_type#682, date#683]\n                        :  +- Join Inner, (col#1021 = paper_id#697)\n                        :     :- Project [col#1021, publication_id#1015L]\n                        :     :  +- Generate explode(_id#996), false, [col#1021]\n                        :     :     +- Project [venue#980, isbn#978, publisher#1004, _id#996, xxhash64(isbn#978, venue#980, 42) AS publication_id#1015L]\n                        :     :        +- Project [venue#980, isbn#978, publisher#1004, _id#996]\n                        :     :           +- Project [isbn#978, venue#980, publishersArray#994, _id#996, count(publisher)#998L, publishersArray#994[0] AS publisher#1004]\n                        :     :              +- Aggregate [isbn#978, venue#980], [isbn#978, venue#980, collect_list(publisher#979, 0, 0) AS publishersArray#994, collect_list(_id#977, 0, 0) AS _id#996, count(publisher#979) AS count(publisher)#998L]\n                        :     :                 +- Filter NOT (venue#980 = null)\n                        :     :                    +- Filter NOT (isbn#978 = null)\n                        :     :                       +- Filter (publication_type#981 = Book)\n                        :     :                          +- Relation [_id#977,isbn#978,publisher#979,venue#980,publication_type#981] json\n                        :     +- Project [_id#671 AS paper_id#697, title#672, keywords#673, fos#674, references#675, page_start#676, page_end#677, lang#678, doi#679, url#680, abstract#681, publication_type#682, date#683]\n                        :        +- Relation [_id#671,title#672,keywords#673,fos#674,references#675,page_start#676,page_end#677,lang#678,doi#679,url#680,abstract#681,publication_type#682,date#683] json\n                        :- Project [publication_id#881L AS publication_id#1233L, paper_id#697 AS paper_id#1234, title#1221 AS title#1235, keywords#1222 AS keywords#1236, fos#1223 AS fos#1237, references#1224 AS references#1238, page_start#1225 AS page_start#1239, page_end#1226 AS page_end#1240, lang#1227 AS lang#1241, doi#1228 AS doi#1242, url#1229 AS url#1243, abstract#1230 AS abstract#1244, publication_type#1231 AS publication_type#1245, date#1232 AS date#1246]\n                        :  +- Join Inner, (col#889 = paper_id#697)\n                        :     :- Project [col#889, publication_id#881L]\n                        :     :  +- Generate explode(_id#856), false, [col#889]\n                        :     :     +- Project [venue#834, volume#835, issue#836, publisher#866, issn#832, _id#856, xxhash64(venue#834, volume#835, issue#836, issn#832, 42) AS publication_id#881L]\n                        :     :        +- Project [venue#834, volume#835, issue#836, publisher#866, issn#832, _id#856]\n                        :     :           +- Project [venue#834, volume#835, issue#836, issn#832, publishersArray#854, _id#856, count(publisher)#858L, publishersArray#854[0] AS publisher#866]\n                        :     :              +- Aggregate [venue#834, volume#835, issue#836, issn#832], [venue#834, volume#835, issue#836, issn#832, collect_list(publisher#833, 0, 0) AS publishersArray#854, collect_list(_id#831, 0, 0) AS _id#856, count(publisher#833) AS count(publisher)#858L]\n                        :     :                 +- Filter (volume#835 >= 0)\n                        :     :                    +- Filter (issue#836 >= 0)\n                        :     :                       +- Filter NOT (venue#834 = null)\n                        :     :                          +- Filter NOT (issn#832 = null)\n                        :     :                             +- Filter (publication_type#837 = Journal)\n                        :     :                                +- Relation [_id#831,issn#832,publisher#833,venue#834,volume#835,issue#836,publication_type#837] json\n                        :     +- Project [_id#1220 AS paper_id#697, title#1221, keywords#1222, fos#1223, references#1224, page_start#1225, page_end#1226, lang#1227, doi#1228, url#1229, abstract#1230, publication_type#1231, date#1232]\n                        :        +- Relation [_id#1220,title#1221,keywords#1222,fos#1223,references#1224,page_start#1225,page_end#1226,lang#1227,doi#1228,url#1229,abstract#1230,publication_type#1231,date#1232] json\n                        +- Project [publication_id#1137L AS publication_id#1275L, paper_id#697 AS paper_id#1276, title#1263 AS title#1277, keywords#1264 AS keywords#1278, fos#1265 AS fos#1279, references#1266 AS references#1280, page_start#1267 AS page_start#1281, page_end#1268 AS page_end#1282, lang#1269 AS lang#1283, doi#1270 AS doi#1284, url#1271 AS url#1285, abstract#1272 AS abstract#1286, publication_type#1273 AS publication_type#1287, date#1274 AS date#1288]\n                           +- Project [publication_id#1137L, paper_id#697, title#1263, keywords#1264, fos#1265, references#1266, page_start#1267, page_end#1268, lang#1269, doi#1270, url#1271, abstract#1272, publication_type#1273, date#1274]\n                              +- Join Inner, (col#1142 = paper_id#697)\n                                 :- Project [col#1142, publication_id#1137L]\n                                 :  +- Generate explode(_id#1121), false, [col#1142]\n                                 :     +- Project [venue#1107, location#1128, _id#1121, xxhash64(venue#1107, 42) AS publication_id#1137L]\n                                 :        +- Project [venue#1107, location#1128, _id#1121]\n                                 :           +- Project [venue#1107, locations_array#1119, _id#1121, count(location)#1123L, locations_array#1119[0] AS location#1128]\n                                 :              +- Aggregate [venue#1107], [venue#1107, collect_list(location#1106, 0, 0) AS locations_array#1119, collect_list(_id#1105, 0, 0) AS _id#1121, count(location#1106) AS count(location)#1123L]\n                                 :                 +- Filter NOT (venue#1107 = null)\n                                 :                    +- Filter (publication_type#1108 = Conference)\n                                 :                       +- Relation [_id#1105,location#1106,venue#1107,publication_type#1108] json\n                                 +- Project [_id#1262 AS paper_id#697, title#1263, keywords#1264, fos#1265, references#1266, page_start#1267, page_end#1268, lang#1269, doi#1270, url#1271, abstract#1272, publication_type#1273, date#1274]\n                                    +- Relation [_id#1262,title#1263,keywords#1264,fos#1265,references#1266,page_start#1267,page_end#1268,lang#1269,doi#1270,url#1271,abstract#1272,publication_type#1273,date#1274] json\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/3m/67ywp3l11x1bmc5kxj_p38h80000gn/T/ipykernel_15940/3790925120.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     15\u001B[0m     \u001B[0;34m.\u001B[0m\u001B[0mwithColumn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'new_doi'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'10.1007/11944577_37'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     16\u001B[0m     \u001B[0;34m.\u001B[0m\u001B[0mwithColumn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'new_url'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0marray\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mlit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'https://link.springer.com/chapter/10.1007/11944577_37'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 17\u001B[0;31m \u001B[0mupdated_df_filter\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mupdated_df_filter\u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     18\u001B[0m     \u001B[0;34m.\u001B[0m\u001B[0mdrop\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcol\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'doi'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdrop\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcol\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'url'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     19\u001B[0m     \u001B[0;34m.\u001B[0m\u001B[0mwithColumnRenamed\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'new_doi'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'doi'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\u001B[0m in \u001B[0;36mselect\u001B[0;34m(self, *cols)\u001B[0m\n\u001B[1;32m   2021\u001B[0m         \u001B[0;34m[\u001B[0m\u001B[0mRow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'Alice'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mage\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m12\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mRow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'Bob'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mage\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m15\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2022\u001B[0m         \"\"\"\n\u001B[0;32m-> 2023\u001B[0;31m         \u001B[0mjdf\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mselect\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jcols\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0mcols\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   2024\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mDataFrame\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mjdf\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msparkSession\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2025\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1319\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1320\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1321\u001B[0;31m         return_value = get_return_value(\n\u001B[0m\u001B[1;32m   1322\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[1;32m   1323\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    194\u001B[0m                 \u001B[0;31m# Hide where the exception came from that shows a non-Pythonic\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    195\u001B[0m                 \u001B[0;31m# JVM exception message.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 196\u001B[0;31m                 \u001B[0;32mraise\u001B[0m \u001B[0mconverted\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    197\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    198\u001B[0m                 \u001B[0;32mraise\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mAnalysisException\u001B[0m: Column '_id' does not exist. Did you mean one of the following? [doi, fos, url, date, lang, title, paper_id, page_end, keywords, abstract, page_start, references, publication_id, publication_type];\n'Project ['_id, title#672, keywords#673, fos#674, references#675, page_start#676, page_end#677, lang#678, doi#9900, url#9915, abstract#681, publication_type#682, date#683, publication_id#1015L]\n+- Project [publication_id#1015L, paper_id#697, title#672, keywords#673, fos#674, references#675, page_start#676, page_end#677, lang#678, abstract#681, publication_type#682, date#683, doi#9900, new_url#9854 AS url#9915]\n   +- Project [publication_id#1015L, paper_id#697, title#672, keywords#673, fos#674, references#675, page_start#676, page_end#677, lang#678, abstract#681, publication_type#682, date#683, new_doi#9838 AS doi#9900, new_url#9854]\n      +- Project [publication_id#1015L, paper_id#697, title#672, keywords#673, fos#674, references#675, page_start#676, page_end#677, lang#678, abstract#681, publication_type#682, date#683, new_doi#9838, new_url#9854]\n         +- Project [publication_id#1015L, paper_id#697, title#672, keywords#673, fos#674, references#675, page_start#676, page_end#677, lang#678, url#680, abstract#681, publication_type#682, date#683, new_doi#9838, new_url#9854]\n            +- Project [publication_id#1015L, paper_id#697, title#672, keywords#673, fos#674, references#675, page_start#676, page_end#677, lang#678, doi#679, url#680, abstract#681, publication_type#682, date#683, new_doi#9838, array(https://link.springer.com/chapter/10.1007/11944577_37) AS new_url#9854]\n               +- Project [publication_id#1015L, paper_id#697, title#672, keywords#673, fos#674, references#675, page_start#676, page_end#677, lang#678, doi#679, url#680, abstract#681, publication_type#682, date#683, 10.1007/11944577_37 AS new_doi#9838]\n                  +- Filter (paper_id#697 = 53e997e4b7602d9701fdb48a)\n                     +- Union false, false\n                        :- Project [publication_id#1015L, paper_id#697, title#672, keywords#673, fos#674, references#675, page_start#676, page_end#677, lang#678, doi#679, url#680, abstract#681, publication_type#682, date#683]\n                        :  +- Join Inner, (col#1021 = paper_id#697)\n                        :     :- Project [col#1021, publication_id#1015L]\n                        :     :  +- Generate explode(_id#996), false, [col#1021]\n                        :     :     +- Project [venue#980, isbn#978, publisher#1004, _id#996, xxhash64(isbn#978, venue#980, 42) AS publication_id#1015L]\n                        :     :        +- Project [venue#980, isbn#978, publisher#1004, _id#996]\n                        :     :           +- Project [isbn#978, venue#980, publishersArray#994, _id#996, count(publisher)#998L, publishersArray#994[0] AS publisher#1004]\n                        :     :              +- Aggregate [isbn#978, venue#980], [isbn#978, venue#980, collect_list(publisher#979, 0, 0) AS publishersArray#994, collect_list(_id#977, 0, 0) AS _id#996, count(publisher#979) AS count(publisher)#998L]\n                        :     :                 +- Filter NOT (venue#980 = null)\n                        :     :                    +- Filter NOT (isbn#978 = null)\n                        :     :                       +- Filter (publication_type#981 = Book)\n                        :     :                          +- Relation [_id#977,isbn#978,publisher#979,venue#980,publication_type#981] json\n                        :     +- Project [_id#671 AS paper_id#697, title#672, keywords#673, fos#674, references#675, page_start#676, page_end#677, lang#678, doi#679, url#680, abstract#681, publication_type#682, date#683]\n                        :        +- Relation [_id#671,title#672,keywords#673,fos#674,references#675,page_start#676,page_end#677,lang#678,doi#679,url#680,abstract#681,publication_type#682,date#683] json\n                        :- Project [publication_id#881L AS publication_id#1233L, paper_id#697 AS paper_id#1234, title#1221 AS title#1235, keywords#1222 AS keywords#1236, fos#1223 AS fos#1237, references#1224 AS references#1238, page_start#1225 AS page_start#1239, page_end#1226 AS page_end#1240, lang#1227 AS lang#1241, doi#1228 AS doi#1242, url#1229 AS url#1243, abstract#1230 AS abstract#1244, publication_type#1231 AS publication_type#1245, date#1232 AS date#1246]\n                        :  +- Join Inner, (col#889 = paper_id#697)\n                        :     :- Project [col#889, publication_id#881L]\n                        :     :  +- Generate explode(_id#856), false, [col#889]\n                        :     :     +- Project [venue#834, volume#835, issue#836, publisher#866, issn#832, _id#856, xxhash64(venue#834, volume#835, issue#836, issn#832, 42) AS publication_id#881L]\n                        :     :        +- Project [venue#834, volume#835, issue#836, publisher#866, issn#832, _id#856]\n                        :     :           +- Project [venue#834, volume#835, issue#836, issn#832, publishersArray#854, _id#856, count(publisher)#858L, publishersArray#854[0] AS publisher#866]\n                        :     :              +- Aggregate [venue#834, volume#835, issue#836, issn#832], [venue#834, volume#835, issue#836, issn#832, collect_list(publisher#833, 0, 0) AS publishersArray#854, collect_list(_id#831, 0, 0) AS _id#856, count(publisher#833) AS count(publisher)#858L]\n                        :     :                 +- Filter (volume#835 >= 0)\n                        :     :                    +- Filter (issue#836 >= 0)\n                        :     :                       +- Filter NOT (venue#834 = null)\n                        :     :                          +- Filter NOT (issn#832 = null)\n                        :     :                             +- Filter (publication_type#837 = Journal)\n                        :     :                                +- Relation [_id#831,issn#832,publisher#833,venue#834,volume#835,issue#836,publication_type#837] json\n                        :     +- Project [_id#1220 AS paper_id#697, title#1221, keywords#1222, fos#1223, references#1224, page_start#1225, page_end#1226, lang#1227, doi#1228, url#1229, abstract#1230, publication_type#1231, date#1232]\n                        :        +- Relation [_id#1220,title#1221,keywords#1222,fos#1223,references#1224,page_start#1225,page_end#1226,lang#1227,doi#1228,url#1229,abstract#1230,publication_type#1231,date#1232] json\n                        +- Project [publication_id#1137L AS publication_id#1275L, paper_id#697 AS paper_id#1276, title#1263 AS title#1277, keywords#1264 AS keywords#1278, fos#1265 AS fos#1279, references#1266 AS references#1280, page_start#1267 AS page_start#1281, page_end#1268 AS page_end#1282, lang#1269 AS lang#1283, doi#1270 AS doi#1284, url#1271 AS url#1285, abstract#1272 AS abstract#1286, publication_type#1273 AS publication_type#1287, date#1274 AS date#1288]\n                           +- Project [publication_id#1137L, paper_id#697, title#1263, keywords#1264, fos#1265, references#1266, page_start#1267, page_end#1268, lang#1269, doi#1270, url#1271, abstract#1272, publication_type#1273, date#1274]\n                              +- Join Inner, (col#1142 = paper_id#697)\n                                 :- Project [col#1142, publication_id#1137L]\n                                 :  +- Generate explode(_id#1121), false, [col#1142]\n                                 :     +- Project [venue#1107, location#1128, _id#1121, xxhash64(venue#1107, 42) AS publication_id#1137L]\n                                 :        +- Project [venue#1107, location#1128, _id#1121]\n                                 :           +- Project [venue#1107, locations_array#1119, _id#1121, count(location)#1123L, locations_array#1119[0] AS location#1128]\n                                 :              +- Aggregate [venue#1107], [venue#1107, collect_list(location#1106, 0, 0) AS locations_array#1119, collect_list(_id#1105, 0, 0) AS _id#1121, count(location#1106) AS count(location)#1123L]\n                                 :                 +- Filter NOT (venue#1107 = null)\n                                 :                    +- Filter (publication_type#1108 = Conference)\n                                 :                       +- Relation [_id#1105,location#1106,venue#1107,publication_type#1108] json\n                                 +- Project [_id#1262 AS paper_id#697, title#1263, keywords#1264, fos#1265, references#1266, page_start#1267, page_end#1268, lang#1269, doi#1270, url#1271, abstract#1272, publication_type#1273, date#1274]\n                                    +- Relation [_id#1262,title#1263,keywords#1264,fos#1265,references#1266,page_start#1267,page_end#1268,lang#1269,doi#1270,url#1271,abstract#1272,publication_type#1273,date#1274] json\n"
     ]
    }
   ],
   "source": [
    "# Command 2: Update one single row of a dataframe or multiple rows\n",
    "\n",
    "# There may be many handmade ways to update a dataframe since there is no standard methods\n",
    "# I want to modify the DOI and the url of document because the given information were wrong.\n",
    "# For doing this operation we firstly filter the dataframe keeping only the row to be modified. We add a column with the value we want to insert under the name \\verb|new_field_name|. Then we drop the old columns containing the previos values and we rename the new columns with the name of the old ones. Finally, we make the union between the entire dataframe, without the row we want to modify, and the new entry.\n",
    "from pyspark.sql.functions import lit, array\n",
    "\n",
    "#df_papers\\\n",
    "#    .filter(col('title').like('%chatbot%'))\\\n",
    "#    .select('title', 'paper_id')\\\n",
    "#    .show(truncate = False)\n",
    "updated_df_filter = df_papers\\\n",
    "    .filter(col('paper_id') == '53e997e4b7602d9701fdb48a')\n",
    "updated_df_filter = updated_df_filter\\\n",
    "    .withColumn('new_doi', lit('10.1007/11944577_37'))\\\n",
    "    .withColumn('new_url', array([lit('https://link.springer.com/chapter/10.1007/11944577_37')]))\n",
    "updated_df_filter = updated_df_filter\\\n",
    "    .drop(col('doi')).drop(col('url'))\\\n",
    "    .withColumnRenamed('new_doi', 'doi')\\\n",
    "    .withColumnRenamed('new_url', 'url').select()\n",
    "updated_df_filter.schema['doi'].nullable = True\n",
    "updated_df_filter.schema['url'].nullable = True\n",
    "\n",
    "updated_df_filter.printSchema()\n",
    "df_papers.printSchema()\n",
    "\n",
    "updated_df_filter = df_papers\\\n",
    "    .filter(col('paper_id') != '53e997e4b7602d9701fdb48a')\\\n",
    "    .union(updated_df_filter)\n",
    "updated_df_filter.show()\n",
    "\n",
    "# non funziona del tutto ancora\n",
    "#updated_df_filter = df_papers.filter()\n",
    "# The same methos can be applied when we consider multiple rows. In this case the filter will keep many rows and not just one. If we are interested in updating in a different way the attributes we can modify...."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------+-------------------+\n",
      "|               title|publication_type|               date|\n",
      "+--------------------+----------------+-------------------+\n",
      "|Generalized one-u...|      Conference|1951-01-03 03:43:07|\n",
      "|Predicting PDZ do...|         Journal|1951-01-03 03:43:07|\n",
      "|A New EDI-based D...|         Journal|1951-01-03 03:43:07|\n",
      "|Search-based Exec...|      Conference|1951-01-03 03:43:07|\n",
      "|Multi-structural ...|         Journal|1951-01-03 03:43:07|\n",
      "|Corpus-based ling...|            Book|1951-01-03 03:43:07|\n",
      "|A comparative stu...|         Journal|1951-01-03 03:43:07|\n",
      "|Fast Solution of ...|         Journal|1951-01-03 03:43:07|\n",
      "|Local Hausdorff D...|         Journal|1951-01-03 03:43:07|\n",
      "|Grammatical Evolu...|            Book|1951-01-03 03:43:07|\n",
      "|Global optimizati...|         Journal|1951-01-03 03:43:07|\n",
      "|Processing UML Mo...|            Book|1951-01-03 03:43:07|\n",
      "|Balancing buffer ...|         Journal|1951-01-03 03:43:07|\n",
      "|The Animation of ...|            Book|1951-01-03 03:43:07|\n",
      "|Composing graphic...|      Conference|1951-01-03 03:43:07|\n",
      "|Locally bounded c...|         Journal|1951-01-31 14:29:19|\n",
      "|Robust observer d...|         Journal|1951-01-31 14:29:19|\n",
      "|Object-oriented f...|            Book|1951-01-31 14:29:19|\n",
      "|A CRT-based RSA c...|            Book|1951-01-31 14:29:19|\n",
      "|Agent-based auctions|            Book|1951-01-31 14:29:19|\n",
      "+--------------------+----------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Command 4: delete a group of rows\n",
    "\n",
    "# Use the function year to extract the year from the timestamp\n",
    "from pyspark.sql.functions import year\n",
    "\n",
    "# Drop rows with conditions – where clause\n",
    "# From 37626 to 37175 -> delete all the rows that represent papers published before 1950, because obsolete\n",
    "df_papers = df_papers.where(year('date') > '1950')\n",
    "df_papers.select('title', 'publication_type', 'date').orderBy('date').show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 87:>   (0 + 1) / 1][Stage 89:>   (0 + 0) / 1][Stage 91:>   (0 + 0) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------+----------+--------+-----------+\n",
      "|title                                                            |page_start|page_end|total_pages|\n",
      "+-----------------------------------------------------------------+----------+--------+-----------+\n",
      "|Problem Decomposition and the Learning of Skills                 |17        |31      |14         |\n",
      "|X-tract: Structure Extraction from Botanical Textual Descriptions|2         |2       |0          |\n",
      "|Cognitive agent programming                                      |1385      |1385    |0          |\n",
      "|Constraint based vectorization                                   |195       |204     |9          |\n",
      "|Automatic input rectification                                    |80        |90      |10         |\n",
      "+-----------------------------------------------------------------+----------+--------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Command 5: create a new column with the length of the paper (number of total pages)\n",
    "\n",
    "df_papers_total_pages = df_papers \\\n",
    "    .filter((col('page_start') >= 0) & (col('page_end') >= 0) & (col('page_start') <= col('page_end'))) \\\n",
    "    .withColumn('total_pages', col('page_end') - col('page_start'))\n",
    "\n",
    "df_papers_total_pages \\\n",
    "    .select(col('title'), col('page_start'), col('page_end'), col('total_pages')) \\\n",
    "    .show(5, truncate=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "QUERIES"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 100:>  (0 + 1) / 1][Stage 102:>  (0 + 0) / 1][Stage 104:>  (0 + 0) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+------------------------------------------------------------+\n",
      "|                paper_id|                                                       title|\n",
      "+------------------------+------------------------------------------------------------+\n",
      "|53e997f4b7602d9701ff55f7|BioCause: Annotating and analysing causality in the biome...|\n",
      "|53e99860b7602d970209d6b9|Predicting PDZ domain mediated protein interactions from ...|\n",
      "|53e99867b7602d97020a2868|Jimena: efficient computing and system state identificati...|\n",
      "|53e9989bb7602d97020d129a|Reconciliation-based detection of co-evolving gene families.|\n",
      "|53e998c7b7602d97021001f0|SeqSIMLA: a sequence and phenotype simulation tool for co...|\n",
      "|53e998cdb7602d97021066c0|Cheminformatic models based on machine learning for pyruv...|\n",
      "|53e99938b7602d9702177171|SynTView - an interactive multi-view genome browser for n...|\n",
      "|53e99953b7602d970219688f|CoMAGC: a corpus with multi-faceted annotations of gene-c...|\n",
      "|53e9998bb7602d97021d0801|Large-scale extraction of accurate drug-disease treatment...|\n",
      "+------------------------+------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Query 1: WHERE, JOIN\n",
    "\n",
    "venue, volume, issue = ('BMC Bioinformatics', '14', '1') \n",
    "\n",
    "df_papers_q1 = df_journals\\\n",
    "               .filter((col('venue') == venue) &\n",
    "                       (col('volume') == volume) &\n",
    "                       (col('issue') == issue))\\\n",
    "                .join(df_papers,\n",
    "                      (df_journals['publication_id'] == df_papers['publication_id']) &\n",
    "                        (df_papers['publication_type'] == 'Journal')\n",
    "                     )\n",
    "\n",
    "df_papers_q1.select(['paper_id', 'title']).show(truncate=60)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------------------+-------------------+-----------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|title                                                                                          |date               |keyword                                                                                                                                        |\n",
      "+-----------------------------------------------------------------------------------------------+-------------------+-----------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|Evolved swarming without positioning information: an application in aerial communication relay |1951-02-02 18:36:03|artificial evolution                                                                                                                           |\n",
      "|Evolved swarming without positioning information: an application in aerial communication relay |1951-02-02 18:36:03|swarm intelligence swarming without positioning micro air vehicles mavs communication relay artificial evolution situated communication smavnet|\n",
      "|NeuroFPGA -- Implementing Artificial Neural Networks on Programmable Logic Devices             |1951-03-08 02:45:20|artificial neural network                                                                                                                      |\n",
      "|NeuroFPGA -- Implementing Artificial Neural Networks on Programmable Logic Devices             |1951-03-08 02:45:20|implementing artificial neural networks                                                                                                        |\n",
      "|Wang Notation Tool: Layout independent representation of tables                                |1951-03-08 02:45:20|ontologies artificial intelligence                                                                                                             |\n",
      "|Variability in Behavior of Command Agents with Human-Like Decision Making Strategies           |1951-04-12 23:40:04|artificial intelligence                                                                                                                        |\n",
      "|Semantic Filtering in Social Media for Trend Modeling                                          |1951-04-13 01:32:11|learning artificial intelligence                                                                                                               |\n",
      "|A stochastic viewpoint on the generation of spatiotemporal datasets                            |1951-05-07 14:04:46|artificial datasets                                                                                                                            |\n",
      "|Enforceable social laws                                                                        |1951-07-06 05:46:48|artificial social system                                                                                                                       |\n",
      "|Enforceable social laws                                                                        |1951-07-06 05:46:48|artificial social systems                                                                                                                      |\n",
      "|Engineering Drug Design Using a Multi-Input Multi-Output Neuro-Fuzzy System                    |1951-07-25 19:20:07|learning artificial intelligence                                                                                                               |\n",
      "|Engineering Drug Design Using a Multi-Input Multi-Output Neuro-Fuzzy System                    |1951-07-25 19:20:07|artificial neural network                                                                                                                      |\n",
      "|Development of a knowledge-based system for monitoring and diagnosis of the CO2 capture process|1951-07-25 19:20:07|artificial intelligence techniques                                                                                                             |\n",
      "|Image Attribute Adaptation                                                                     |1951-07-25 19:20:07|learning artificial intelligence                                                                                                               |\n",
      "|Image Attribute Adaptation                                                                     |1951-07-25 19:20:07|ontologies artificial intelligence                                                                                                             |\n",
      "|Template based evolution                                                                       |1951-08-30 13:01:12|artificial creature                                                                                                                            |\n",
      "|Heuristic Method for Discriminative Structure Learning of Markov Logic Networks                |1951-09-06 15:17:17|learning artificial intelligence                                                                                                               |\n",
      "|A Dynamic Service Level Negotiation Mechanism for QoS Provisioning in NGEO Satellite Networks  |1951-09-11 17:11:44|artificial satellites                                                                                                                          |\n",
      "|Learning bimanual coordination patterns for rhythmic movements                                 |1951-09-11 17:11:44|artificial system                                                                                                                              |\n",
      "|A microworld approach to the formalization of musical knowledge                                |1951-09-11 17:11:44|artificial intelligence and music                                                                                                              |\n",
      "+-----------------------------------------------------------------------------------------------+-------------------+-----------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query 2: WHERE, LIMIT, LIKE\n",
    "# Find the papers written in the last twenty years in english whose keywords have the word \\verb|artificial| inside the keywords. We require that these papers have the DOI set to a not null value.\n",
    "# The results are ordered ascendengly by the date and only 20 elements are printed.\n",
    "\n",
    "from pyspark.sql.functions import current_timestamp, unix_timestamp\n",
    "\n",
    "df = df_papers.withColumn('current time', current_timestamp())\n",
    "df = df\\\n",
    "    .filter(\n",
    "        (((unix_timestamp('current time') - unix_timestamp('date')) / 3600 / 24 / 365) > 50) & \n",
    "        (col('doi').isNotNull()) &\n",
    "        (col('lang') == 'eng'))\\\n",
    "    .select('title', 'date', explode('keywords').alias('keyword'))\\\n",
    "    .filter(col('keyword').like('%artificial%'))\\\n",
    "    .distinct()\\\n",
    "    .sort(col('date').asc())\\\n",
    "    .limit(20)\\\n",
    "    .show(truncate = False)\n",
    "#df = df_papers.filter().select().groupby().filter().select()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------+--------------------------------------------------+\n",
      "|                                      organization|                                            papers|\n",
      "+--------------------------------------------------+--------------------------------------------------+\n",
      "|        Carnegie Mellon University, Pittsburgh, PA|[53e998dbb7602d9702118f29, 53e9987db7602d97020b...|\n",
      "|   Carnegie Mellon University, Pittsburgh, PA, USA|[53e998c7b7602d97020fdf46, 53e99905b7602d970214...|\n",
      "| Georgia Institute of Technology, Atlanta, GA, USA|[53e99827b7602d9702048d5f, 53e998f6b7602d970213...|\n",
      "|           Microsoft Research Asia, Beijing, China|[53e99976b7602d97021b91db, 53e9980eb7602d970202...|\n",
      "|              Microsoft Research, Redmond, WA, USA|[53e99885b7602d97020bfe89, 53e99930b7602d970216...|\n",
      "|National University of Singapore, Singapore, Si...|[53e99813b7602d970202da43, 53e99876b7602d97020b...|\n",
      "|               Tsinghua University, Beijing, China|[53e998c7b7602d97020fdf46, 53e99998b7602d97021d...|\n",
      "|        University of Washington, Seattle, WA, USA|[53e998aab7602d97020e600e, 53e99967b7602d97021a...|\n",
      "+--------------------------------------------------+--------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query 4: GROUP BY, JOIN, AS\n",
    "from pyspark.sql.functions import collect_set, size\n",
    "\n",
    "df = df_aff\\\n",
    "    .join(df_papers, df_papers.paper_id == df_aff.paper_id, 'inner')\\\n",
    "    .drop(df_papers.paper_id).select('paper_id', 'organization', 'publication_type')\\\n",
    "    .filter((col('organization').isNotNull()) & (col('organization') != \"\") & (col('publication_type') == \"Conference\"))\\\n",
    "    .groupBy('organization')\\\n",
    "    .agg(collect_set('paper_id').alias('papers'))\\\n",
    "    .filter(size(col('papers')) > 10)\\\n",
    "    .show(truncate = 50)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------+-----------+---------+---------+---------+---------+\n",
      "|year|total_papers|total_pages|min_pages|max_pages|avg_pages|var_pages|\n",
      "+----+------------+-----------+---------+---------+---------+---------+\n",
      "|2022|         289|       2918|        0|       41|    10.10|    47.79|\n",
      "|2021|         455|       5881|        0|     1585|    12.93| 5,509.97|\n",
      "|2020|         556|       5345|        0|      145|     9.61|    94.19|\n",
      "|2019|         459|       4334|        0|      135|     9.44|    80.38|\n",
      "|2018|         504|       4756|        0|      135|     9.44|    74.82|\n",
      "|2017|         423|       3818|        0|       49|     9.03|    50.92|\n",
      "|2016|         728|       6687|        0|       55|     9.19|    45.74|\n",
      "|2015|         530|       5364|        0|      158|    10.12|    92.43|\n",
      "+----+------------+-----------+---------+---------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query 5: WHERE, GROUP BY\n",
    "# Retrieve some statistics about papers\n",
    "from pyspark.sql.functions import sum, min, max, avg, format_number, variance\n",
    "\n",
    "df_papers_total_pages.filter(year(col('date')) >= 2015) \\\n",
    "    .groupBy(year(col('date')).alias('year')) \\\n",
    "    .agg(count('paper_id').alias('total_papers'),\n",
    "         sum('total_pages').alias('total_pages'),\n",
    "         min('total_pages').alias('min_pages'),\n",
    "         max('total_pages').alias('max_pages'),\n",
    "         format_number(avg('total_pages'), 2).alias('avg_pages'),\n",
    "         format_number(variance('total_pages'), 2).alias('var_pages')) \\\n",
    "    .sort(col('year').desc()) \\\n",
    "    .show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------+----------------+\n",
      "|title                                                            |references_count|\n",
      "+-----------------------------------------------------------------+----------------+\n",
      "|Distinctive Image Features from Scale-Invariant Keypoints        |195             |\n",
      "|A simple transmit diversity technique for wireless communications|62              |\n",
      "|Light field rendering                                            |56              |\n",
      "|Network information flow                                         |55              |\n",
      "|Mining Sequential Patterns                                       |44              |\n",
      "|Symbolic Model Checking                                          |44              |\n",
      "|Geodesic Active Contour.                                         |40              |\n",
      "|Differential Power Analysis                                      |38              |\n",
      "+-----------------------------------------------------------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query 6: GROUP BY, HAVING, AS\n",
    "\n",
    "df_papers_q6 = df_papers\\\n",
    "               .select('paper_id',\n",
    "                       'title',\n",
    "                       explode(col('references')).alias('reference'))\\\n",
    "               .groupBy('reference')\\\n",
    "               .agg(count('paper_id').alias('references_count'))\\\n",
    "               .filter(col('references_count') > 30)\\\n",
    "               .join(df_papers,\n",
    "                     col('reference') == df_papers.paper_id)\\\n",
    "               .sort(col('references_count').desc())\\\n",
    "\n",
    "df_papers_q6.select(['title', 'references_count']).show(truncate=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 347:============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+----------------------+------------+\n",
      "|fos                    |keyword               |couple count|\n",
      "+-----------------------+----------------------+------------+\n",
      "|computer science       |data mining           |209         |\n",
      "|computer science       |internet              |202         |\n",
      "|computer science       |computer science      |197         |\n",
      "|computer science       |real time             |158         |\n",
      "|computer science       |protocols             |143         |\n",
      "|computer science       |satisfiability        |132         |\n",
      "|computer science       |real time systems     |129         |\n",
      "|feature extraction     |feature extraction    |126         |\n",
      "|artificial intelligence|feature extraction    |125         |\n",
      "|computer science       |feature extraction    |122         |\n",
      "|computer science       |quality of service    |120         |\n",
      "|computer science       |information retrieval |115         |\n",
      "|computer science       |computational modeling|112         |\n",
      "|the internet           |internet              |107         |\n",
      "|computer science       |application software  |104         |\n",
      "|computer science       |software engineering  |103         |\n",
      "+-----------------------+----------------------+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Query 7: WHERE, GROUP BY, HAVING, AS\n",
    "# The query returns the association between fields of study and keywords which are more present within the database and how many times they appear together\n",
    "from pyspark.sql.functions import year, col, lit\n",
    "df = df_papers\n",
    "df = df.withColumn('count', lit(1))\n",
    "df = df\\\n",
    "    .filter(\n",
    "        (col('doi').isNotNull()) &\n",
    "        (year(col('date')) >= 2000) &\n",
    "        (size(col('fos')) > 0) &\n",
    "        (size(col('keywords')) > 0))\\\n",
    "    .select('fos', 'count', explode('keywords').alias('keyword'))\\\n",
    "    .select('keyword', explode('fos').alias('fos'), 'count')\\\n",
    "    .groupby('fos', 'keyword')\\\n",
    "    .sum('count')\\\n",
    "    .withColumnRenamed('sum(count)', 'couple count')\\\n",
    "    .filter(col('couple count') > 100)\\\n",
    "    .sort(col('couple count').desc())\\\n",
    "    .show(truncate = False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------------------+\n",
      "|publisher|total_publications_per_publisher|\n",
      "+---------+--------------------------------+\n",
      "+---------+--------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query 9: WHERE, GROUP BY, HAVING, 1 JOIN\n",
    "\n",
    "from pyspark.sql.functions import collect_set, concat, size\n",
    "\n",
    "df = df_journals\\\n",
    "    .withColumnRenamed('venue', 'venueJournals')\\\n",
    "    .filter((col('volume')) > 10)\\\n",
    "    .join(df_books,df_books.publisher == df_journals.publisher,\"inner\")\\\n",
    "    .drop(df_journals.publisher)\\\n",
    "    .withColumnRenamed('venue', 'venueBooks')\\\n",
    "    .select('venueBooks', 'venueJournals', 'publisher')\\\n",
    "    .dropDuplicates(['venueBooks', 'venueJournals', 'publisher'])\\\n",
    "    .groupBy('publisher')\\\n",
    "    .agg(collect_set('venueBooks').alias('books'),\n",
    "         collect_set('venueJournals').alias('journals'))\\\n",
    "    .withColumn(\"total_publications_per_publisher\",\n",
    "                concat(col(\"books\"), col(\"journals\")))\\\n",
    "    .filter(size(col(\"total_publications_per_publisher\")) > '500')\\\n",
    "    .select('publisher', \"total_publications_per_publisher\")\\\n",
    "    .show(truncate = 50)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/08 22:46:57 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------+-------------------+\n",
      "|               name|papers_count|organizations_count|\n",
      "+-------------------+------------+-------------------+\n",
      "|   Rachid Guerraoui|          22|                 11|\n",
      "|Thomas A. Henzinger|          21|                 14|\n",
      "|        Dacheng Tao|          20|                 13|\n",
      "|     Moshe Y. Vardi|          20|                  7|\n",
      "|    Thomas S. Huang|          19|                 10|\n",
      "+-------------------+------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query 10: WHERE, GROUP BY, HAVING, 2 JOINs\n",
    "# Retrieve authors who worked for at least 3 different organizations and have published at least 3 papers with at least 5 fos and 5 references each\n",
    "\n",
    "from pyspark.sql.functions import approx_count_distinct\n",
    "\n",
    "df_papers \\\n",
    "    .filter((size(col('fos')) >= 5) & (size(col('references')) >= 5)) \\\n",
    "    .join(df_aff, df_papers.paper_id == df_aff.paper_id, \"inner\") \\\n",
    "    .drop(df_aff.paper_id) \\\n",
    "    .join(df_aut, df_aff.author_id == df_aut.author_id, \"inner\") \\\n",
    "    .drop(df_aff.author_id) \\\n",
    "    .groupBy(\"author_id\") \\\n",
    "    .agg(count(\"paper_id\").alias(\"papers_count\"),\n",
    "         approx_count_distinct(\"organization\").alias(\"organizations_count\"),\n",
    "         collect_set(\"name\").alias(\"name\")) \\\n",
    "    .filter((size(\"name\") == 1) & (col(\"papers_count\") >= 3)  & (col(\"organizations_count\") >= 3)) \\\n",
    "    .orderBy(col(\"papers_count\").desc(), col(\"organizations_count\").desc()) \\\n",
    "    .select(explode(col(\"name\")).alias(\"name\"), col(\"papers_count\"), col(\"organizations_count\")) \\\n",
    "    .show(5)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
