{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U4ugBnQJt8dk",
    "outputId": "85f42e41-b3d2-4c1b-de49-56addeabdf0a"
   },
   "outputs": [],
   "source": [
    "# Dowloading pyspark\n",
    "#p!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PREPARE THE ENVIRONMENT, UPLOAD DATA, PREPROCESS DATA AND CREATE THE TABLES: Author, Paper, Affiliation, Book, Journal and Conference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# With sparkSession we create a connection to our database\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType, TimestampType\n",
    "from pyspark.sql.functions import count, col, xxhash64, collect_list, explode\n",
    "\n",
    "# Create an entry point to the PySpark Application\n",
    "spark = SparkSession.builder \\\n",
    "      .master(\"local\") \\\n",
    "      .appName(\"Bibliography\") \\\n",
    "      .getOrCreate()\n",
    "\n",
    "INPUT_FILE = \"bibliography.json\"\n",
    "OPTIONS = {'multiline': 'true', 'allowNumericLeadingZero': 'true','timestampFormat': \"yyyy-MM-dd'T'HH:mm:ss[.ZZZ'Z']\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#AUTHOR TABLE\n",
    "schemaAut = StructType(\n",
    "            [StructField('authors', ArrayType(StructType([\n",
    "                StructField('_id', StringType(), nullable = False),\n",
    "                StructField('name', StringType(), True),\n",
    "                StructField('email', StringType(), True),\n",
    "                StructField('bio', StringType(), True),\n",
    "                ])), True)\n",
    "            ])\n",
    "\n",
    "df_aut = spark.read.format('json').options(**OPTIONS).schema(schemaAut).json(INPUT_FILE)\n",
    "df_aut = df_aut.select(explode(df_aut.authors))\n",
    "df_aut = df_aut.withColumnRenamed(\"col\", \"authors\")\n",
    "df_aut = df_aut.filter(col(\"authors._id\") != \"null\").select(\"authors._id\",\"authors.name\",\"authors.email\", \"authors.bio\")\n",
    "df_aut = df_aut.withColumnRenamed(\"_id\", \"author_id\")\n",
    "df_aut = df_aut.dropDuplicates([\"author_id\"])\n",
    "df_aut.printSchema()\n",
    "df_aut.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# PAPER TABLE WITHOUT PUBLICATION_ID\n",
    "schemaPaper = StructType(\n",
    "            [StructField('_id', StringType(), True),\n",
    "             StructField('title', StringType(),True),\n",
    "             StructField('keywords', ArrayType(StringType()), True),\n",
    "             StructField('fos', ArrayType(StringType()), True),\n",
    "             StructField('references', ArrayType(StringType()), True),\n",
    "             StructField('page_start', IntegerType(), True),\n",
    "             StructField('page_end', IntegerType(), True),\n",
    "             StructField('lang', StringType(),True),\n",
    "             StructField('abstract', StringType(),True),\n",
    "             StructField('publication_type', StringType(),True),\n",
    "             StructField('date', TimestampType(), True),\n",
    "             StructField('doi', StringType(),True),\n",
    "             StructField('url', ArrayType(StringType()),True),\n",
    "            ])\n",
    "\n",
    "df_paper = spark.read.format('json').options(**OPTIONS).schema(schemaPaper).json(INPUT_FILE)\n",
    "df_paper = df_paper.withColumnRenamed(\"_id\", \"paper_id\")\n",
    "df_paper.printSchema()\n",
    "df_paper.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# AFFILIATION TABLE\n",
    "schemaAffiliation = StructType(\n",
    "            [StructField('_id', StringType(), True),\n",
    "             StructField('authors', ArrayType(StructType([\n",
    "                    StructField('_id', StringType(), True),\n",
    "                    StructField('org', StringType(), True)\n",
    "             ])), True),\n",
    "            ])\n",
    "\n",
    "df_aff = spark.read.format('json').options(**OPTIONS).schema(schemaAffiliation).json(INPUT_FILE)\n",
    "df_aff = df_aff.withColumnRenamed(\"_id\", \"paper_id\")\n",
    "df_aff = df_aff.select(\"paper_id\", explode(df_aff.authors))\n",
    "df_aff = df_aff.withColumnRenamed(\"col\", \"authors\")\n",
    "df_aff = df_aff.filter(col(\"authors._id\") != \"null\").filter(col(\"paper_id\") != \"null\").select(\"paper_id\", \"authors._id\",\"authors.org\")\n",
    "df_aff = df_aff.withColumnRenamed(\"_id\", \"author_id\")\n",
    "df_aff = df_aff.dropDuplicates([\"author_id\", \"paper_id\"])\n",
    "df_aff = df_aff.withColumnRenamed(\"org\", \"organization\")\n",
    "df_aff.printSchema()\n",
    "df_aff.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# JOURNAL TABLE\n",
    "# Preprocessing of the journals for cleaning and merging the journals\n",
    "\n",
    "journal_schema_preprocessing = StructType(\n",
    "    [StructField('_id', StringType(), True),\n",
    "     StructField('issn', StringType(), True),\n",
    "     StructField('publisher', StringType(), True),\n",
    "     StructField('venue', StringType(), True),\n",
    "     StructField('volume', IntegerType(), True),\n",
    "     StructField('issue', IntegerType(), True),\n",
    "     StructField('publication_type', StringType(),True)])\n",
    "\n",
    "# Reading the json file\n",
    "df_journals_to_filter = spark.read.format('json').options(**OPTIONS).schema(journal_schema_preprocessing).json(INPUT_FILE)\n",
    "\n",
    "# Filtering and adjusting the dataframe\n",
    "df_journals_to_filter = df_journals_to_filter.filter(col('publication_type') == 'Journal').filter(col('issn') != 'null').filter(col('venue') != 'null').filter(col('issue') >= 0).filter(col('volume') >= 0)\n",
    "df_journals_to_filter = df_journals_to_filter.groupBy('venue', 'volume', 'issue', 'issn').agg(collect_list('publisher').alias('publishersArray'), collect_list('_id').alias('_id'), count(col('publisher'))) # count can be removed (I was interested in evaluating if the group by was meaningful)\n",
    "df_journals_to_insert = df_journals_to_filter.withColumn('publisher', df_journals_to_filter['publishersArray'][0]).select('venue', 'volume', 'issue', 'publisher', 'issn', '_id')\n",
    "\n",
    "# Adding the new column which contains the publication_identifier\n",
    "df_journals = df_journals_to_insert.withColumn(\"publication_id\", xxhash64('venue', 'volume', 'issue', 'issn'))\n",
    "\n",
    "# Adding the foreign key to the papers\n",
    "exploded_journals = df_journals.select(explode('_id'), 'publication_id')\n",
    "#exploded_journals.show(truncate = False)\n",
    "\n",
    "df_papers_in_journals = exploded_journals.join(df_paper, exploded_journals.col == df_paper.paper_id, \"inner\")\n",
    "df_papers_in_journals = df_papers_in_journals.drop('col')\n",
    "\n",
    "df_journals = df_journals.drop(df_journals._id)\n",
    "\n",
    "# Visualizing the data\n",
    "# print('Papers')\n",
    "# df_papers_in_journals.show(truncate = False)\n",
    "print('Schema of the journals')\n",
    "df_journals.printSchema()\n",
    "print('Journals')\n",
    "df_journals.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# BOOK TABLE\n",
    "# Preprocessing of the books for cleaning and merging the books\n",
    "book_schema_preprocessing = StructType(\n",
    "    [StructField('_id', StringType(), True),\n",
    "     StructField('isbn', StringType(), True),\n",
    "     StructField('publisher', StringType(), True),\n",
    "     StructField('venue', StringType(), True),\n",
    "     StructField('publication_type', StringType(),True)])\n",
    "\n",
    "# Reading the json file\n",
    "df_books_to_filter = spark.read.format('json').options(**OPTIONS).schema(book_schema_preprocessing).json(INPUT_FILE)\n",
    "\n",
    "# Filtering and adjusting the dataframe\n",
    "df_books_to_filter = df_books_to_filter.filter(col('publication_type') == 'Book').filter(col('isbn') != 'null').filter(col('venue') != 'null')\n",
    "df_books_to_filter = df_books_to_filter.groupBy('isbn', 'venue').agg(collect_list('publisher').alias('publishersArray'), collect_list('_id').alias('_id'), count(col('publisher'))) # count can be removed (I was interested in evaluating if the group by was meaningful)\n",
    "dfbooks_to_insert = df_books_to_filter.withColumn('publisher', df_books_to_filter['publishersArray'][0]).select('venue', 'isbn', 'publisher', '_id')\n",
    "\n",
    "# Adding the new column which is the id\n",
    "df_books = dfbooks_to_insert.withColumn('publication_id', xxhash64('isbn', 'venue'))\n",
    "\n",
    "# Adding the foreign key to the papers\n",
    "exploded_books = df_books.select(explode('_id'), 'publication_id')\n",
    "# exploded_books.show(truncate = False)\n",
    "\n",
    "df_papers_in_books = exploded_books.join(df_paper, exploded_books.col == df_paper.paper_id)\n",
    "df_papers_in_books = df_papers_in_books.drop('col')\n",
    "\n",
    "df_books = df_books.drop(df_books._id)\n",
    "\n",
    "# Visualizing the data\n",
    "# print('Papers')\n",
    "# df_papers_in_books.show(truncate = False)\n",
    "print('Schema of the books')\n",
    "df_books.printSchema()\n",
    "print('Books')\n",
    "df_books.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# CONFERENCE TABLE\n",
    "# Preprocessing of the books for cleaning and merging the books\n",
    "\n",
    "schemaConf = StructType(\n",
    "    [StructField('_id', StringType(), True),\n",
    "     StructField('location', StringType(), True),\n",
    "     StructField('venue', StringType(), True),\n",
    "     StructField('publication_type', StringType(),True)])\n",
    "# Reading the json file\n",
    "df_conferences_to_filter = spark.read.format('json').options(**OPTIONS).schema(schemaConf).json(INPUT_FILE)\n",
    "\n",
    "# Filtering and adjusting the dataframe\n",
    "df_conferences_to_filter = df_conferences_to_filter.filter(col('publication_type') == 'Conference').filter(col('venue') != 'null')\n",
    "df_conferences_to_filter = df_conferences_to_filter.groupBy('venue').agg(collect_list('location').alias('locations_array'), collect_list('_id').alias('_id'), count(col('location'))) # count can be removed (I was interested in evaluating if the group by was meaningful)\n",
    "df_conferences_to_insert = df_conferences_to_filter.withColumn('location', df_conferences_to_filter['locations_array'][0]).select('venue', 'location', '_id')\n",
    "\n",
    "# Adding the new column which is the id\n",
    "df_conferences = df_conferences_to_insert.withColumn('publication_id', xxhash64('venue'))\n",
    "\n",
    "# Adding the foreign key to the papers\n",
    "exploded_conferences = df_conferences.select(explode('_id'), 'publication_id')\n",
    "#exploded_conferences.show(truncate = False)\n",
    "\n",
    "df_papers_in_conferences = exploded_conferences.join(df_paper, exploded_conferences.col == df_paper.paper_id)\n",
    "df_papers_in_conferences = df_papers_in_conferences.drop('col')\n",
    "\n",
    "df_conferences = df_conferences.drop(df_conferences._id)\n",
    "\n",
    "# Visualizing the data\n",
    "#print('Papers')\n",
    "#df_papers_in_conferences.show(truncate = False)\n",
    "print('Schema of the conferences')\n",
    "df_conferences.printSchema()\n",
    "print('Conferences')\n",
    "df_conferences.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Merging the 3 dataframe which one contains the papers published in a specific media\n",
    "df_papers = df_papers_in_books.union(df_papers_in_journals).union(df_papers_in_conferences)\n",
    "\n",
    "# Visualizing the data\n",
    "print('Papers schema')\n",
    "df_papers.printSchema()\n",
    "print('Papers data')\n",
    "df_papers.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# For checking the result\n",
    "print('Papers published in books')\n",
    "df_papers.filter(col('publication_type') == 'Book').select('paper_id', 'title', 'publication_type', 'publication_id').show()\n",
    "print('Papers published in journals')\n",
    "df_papers.filter(col('publication_type') == 'Journal').select('paper_id', 'title', 'publication_type', 'publication_id').show()\n",
    "print('Papers published in conferences')\n",
    "df_papers.filter(col('publication_type') == 'Conference').select('paper_id', 'title', 'publication_type', 'publication_id').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COMMANDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Command 1: Add a new row to the Paper DataFrame\n",
    "\n",
    "new_paper_file = 'single_paper.json'\n",
    "\n",
    "new_paper = spark.read.options(**OPTIONS).json(new_paper_file, schemaPaper)\\\n",
    "                 .withColumnRenamed(\"_id\", \"paper_id\")\n",
    "\n",
    "journal_schema = StructType(\n",
    "    [StructField('issn', StringType(), True),\n",
    "     StructField('publisher', StringType(), True),\n",
    "     StructField('venue', StringType(), True),\n",
    "     StructField('volume', IntegerType(), True),\n",
    "     StructField('issue', IntegerType(), True)])\n",
    "\n",
    "\n",
    "journal = spark.read.options(**OPTIONS).json(new_paper_file, journal_schema)\\\n",
    "               .withColumn(\"publication_id\", xxhash64('venue', 'volume', 'issue', 'issn'))\n",
    "\n",
    "foreign_key = journal.head(1)[0].asDict()['publication_id']\n",
    "\n",
    "if df_journals.filter(col('publication_id') == foreign_key).count() == 0:\n",
    "    df_jounrals = df_journals.union(journal)\n",
    "\n",
    "paper_id = new_paper.head(1)[0].asDict()['paper_id']\n",
    "\n",
    "# Inserting foreign_key at position 0\n",
    "new_paper = new_paper.select(lit(foreign_key).alias('publication_id'), '*')\n",
    "\n",
    "if df_papers.filter(col('paper_id') == paper_id).collect() == []:\n",
    "    df_papers = df_papers.union(new_paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Update one single row of a dataframe or multiple rows\n",
    "\n",
    "#The command modifies the DOI and URL of a paper with identifier equal to '53e997e4b7602d9701fdb48a'.\n",
    "#For doing this operation we firstly filter the dataframe keeping only the row to be modified. We add a column for each value we want to insert under the name new_field_name. Then we drop the old columns containing the previous values and we rename the new columns with the name of the old ones.\n",
    "#Finally, we make the union between the entire dataframe, without the row we want to modify, and the new entry.\n",
    "from pyspark.sql.functions import lit, array\n",
    "\n",
    "#df_papers\\\n",
    "#    .filter(col('title').like('%chatbot%'))\\\n",
    "#    .select('title', 'paper_id')\\\n",
    "#    .show(truncate = False)\n",
    "updated_df_papers = df_papers\\\n",
    "    .filter(col('paper_id') == '53e997e4b7602d9701fdb48a')\n",
    "updated_df_papers = updated_df_papers\\\n",
    "    .withColumn('new_doi', lit('10.1007/11944577_37'))\\\n",
    "    .withColumn('new_url', array([lit('https://link.springer.com/chapter/10.1007/11944577_37')]))\n",
    "updated_df_papers = updated_df_papers\\\n",
    "    .drop(col('doi')).drop(col('url'))\\\n",
    "    .withColumnRenamed('new_doi', 'doi')\\\n",
    "    .withColumnRenamed('new_url', 'url')\n",
    "\n",
    "# To check the number of the paper is the same\n",
    "updated_df_papers = df_papers.filter(col('paper_id') != '53e997e4b7602d9701fdb48a').union(updated_df_papers)\n",
    "\n",
    "print('The size of the entire initial database is ' + str(df_papers.count()) + ', the size of the current database is ' + str(updated_df_papers.count()))\n",
    "updated_df_papers.filter(col('paper_id') == '53e997e4b7602d9701fdb48a').select('paper_id', 'title', 'doi', 'url').show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Command 4: delete a group of rows\n",
    "from pyspark.sql.functions import year\n",
    "\n",
    "df_papers = df_papers\\\n",
    "    .filter(year('date') > '1950')\\\n",
    "    .select('title', 'publication_type', 'date')\\\n",
    "    .orderBy('date')\\\n",
    "    .show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Command 5: create a new column with the length of the paper (number of total pages)\n",
    "\n",
    "df_papers_total_pages = df_papers \\\n",
    "    .filter((col('page_start') >= 0) & (col('page_end') >= 0) & (col('page_start') <= col('page_end'))) \\\n",
    "    .withColumn('total_pages', col('page_end') - col('page_start'))\n",
    "\n",
    "df_papers_total_pages \\\n",
    "    .select(col('title'), col('page_start'), col('page_end'), col('total_pages')) \\\n",
    "    .show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QUERIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#Query 1: WHERE, JOIN\n",
    "\n",
    "venue, volume, issue = ('BMC Bioinformatics', '14', '1') \n",
    "\n",
    "df_papers_q1 = df_journals\\\n",
    "               .filter((col('venue') == venue) &\n",
    "                       (col('volume') == volume) &\n",
    "                       (col('issue') == issue))\\\n",
    "                .join(df_papers,\n",
    "                      (df_journals['publication_id'] == df_papers['publication_id']) &\n",
    "                        (df_papers['publication_type'] == 'Journal')\n",
    "                     )\n",
    "\n",
    "df_papers_q1.select(['paper_id', 'title']).show(truncate=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Query 2: WHERE, LIMIT, LIKE\n",
    "# Find the papers written in the last twenty years in english whose keywords have the word \\verb|artificial| inside the keywords. We require that these papers have the DOI set to a not null value.\n",
    "# The results are ordered ascending by the date and only 20 elements are printed.\n",
    "\n",
    "from pyspark.sql.functions import current_timestamp, unix_timestamp\n",
    "\n",
    "df = df_papers.withColumn('current time', current_timestamp())\n",
    "df = df\\\n",
    "    .filter(\n",
    "        (((unix_timestamp('current time') - unix_timestamp('date')) / 3600 / 24 / 365) > 50) & \n",
    "        (col('doi').isNotNull()) &\n",
    "        (col('lang') == 'eng'))\\\n",
    "    .select('title', 'date', explode('keywords').alias('keyword'))\\\n",
    "    .filter(col('keyword').like('%artificial%'))\\\n",
    "    .distinct()\\\n",
    "    .sort(col('date').asc())\\\n",
    "    .limit(20)\\\n",
    "    .show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Query 4: GROUP BY, JOIN, AS\n",
    "from pyspark.sql.functions import collect_set, size\n",
    "\n",
    "df = df_aff\\\n",
    "    .join(df_papers, df_papers.paper_id == df_aff.paper_id, 'inner')\\\n",
    "    .drop(df_papers.paper_id).select('paper_id', 'organization', 'publication_type')\\\n",
    "    .filter((col('organization').isNotNull()) & (col('organization') != \"\") & (col('publication_type') == \"Conference\"))\\\n",
    "    .groupBy('organization')\\\n",
    "    .agg(collect_set('paper_id').alias('papers'))\\\n",
    "    .filter(size(col('papers')) > 10)\\\n",
    "    .show(truncate = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Query 5: WHERE, GROUP BY\n",
    "# Retrieve some statistics about papers\n",
    "from pyspark.sql.functions import sum, min, max, avg, format_number, variance\n",
    "\n",
    "df_papers_total_pages.filter(year(col('date')) >= 2015) \\\n",
    "    .groupBy(year(col('date')).alias('year')) \\\n",
    "    .agg(count('paper_id').alias('total_papers'),\n",
    "         sum('total_pages').alias('total_pages'),\n",
    "         min('total_pages').alias('min_pages'),\n",
    "         max('total_pages').alias('max_pages'),\n",
    "         format_number(avg('total_pages'), 2).alias('avg_pages'),\n",
    "         format_number(variance('total_pages'), 2).alias('var_pages')) \\\n",
    "    .sort(col('year').desc()) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Query 6: GROUP BY, HAVING, AS\n",
    "\n",
    "df_papers_q6 = df_papers\\\n",
    "               .select('paper_id',\n",
    "                       'title',\n",
    "                       explode(col('references')).alias('reference'))\\\n",
    "               .groupBy('reference')\\\n",
    "               .agg(count('paper_id').alias('references_count'))\\\n",
    "               .filter(col('references_count') > 30)\\\n",
    "               .join(df_papers,\n",
    "                     col('reference') == df_papers.paper_id)\\\n",
    "               .sort(col('references_count').desc())\\\n",
    "\n",
    "df_papers_q6.select(['title', 'references_count']).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Query 7: WHERE, GROUP BY, HAVING, AS\n",
    "# The query returns the association between fields of study and keywords which are more present within the database and how many times they appear together\n",
    "from pyspark.sql.functions import year, col, lit\n",
    "df = df_papers\n",
    "df = df.withColumn('count', lit(1))\n",
    "df = df\\\n",
    "    .filter(\n",
    "        (col('doi').isNotNull()) &\n",
    "        (year(col('date')) >= 2000) &\n",
    "        (size(col('fos')) > 0) &\n",
    "        (size(col('keywords')) > 0))\\\n",
    "    .select('fos', 'count', explode('keywords').alias('keyword'))\\\n",
    "    .select('keyword', explode('fos').alias('fos'), 'count')\\\n",
    "    .groupby('fos', 'keyword')\\\n",
    "    .sum('count')\\\n",
    "    .withColumnRenamed('sum(count)', 'couple count')\\\n",
    "    .filter(col('couple count') > 100)\\\n",
    "    .sort(col('couple count').desc())\\\n",
    "    .show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Query 9: WHERE, GROUP BY, HAVING, 1 JOIN\n",
    "\n",
    "from pyspark.sql.functions import collect_set, concat, size\n",
    "\n",
    "df = df_journals\\\n",
    "    .withColumnRenamed('venue', 'venueJournals')\\\n",
    "    .filter((col('volume')) > 10)\\\n",
    "    .join(df_books,df_books.publisher == df_journals.publisher,\"inner\")\\\n",
    "    .drop(df_journals.publisher)\\\n",
    "    .withColumnRenamed('venue', 'venueBooks')\\\n",
    "    .select('venueBooks', 'venueJournals', 'publisher')\\\n",
    "    .dropDuplicates(['venueBooks', 'venueJournals', 'publisher'])\\\n",
    "    .groupBy('publisher')\\\n",
    "    .agg(collect_set('venueBooks').alias('books'),\n",
    "         collect_set('venueJournals').alias('journals'))\\\n",
    "    .withColumn(\"total_publications_per_publisher\",\n",
    "                concat(col(\"books\"), col(\"journals\")))\\\n",
    "    .filter(size(col(\"total_publications_per_publisher\")) > '500')\\\n",
    "    .select('publisher', \"total_publications_per_publisher\")\\\n",
    "    .show(truncate = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Query 10: WHERE, GROUP BY, HAVING, 2 JOINs\n",
    "# Retrieve authors who worked for at least 3 different organizations and have published at least 3 papers with at least 5 fos and 5 references each\n",
    "\n",
    "from pyspark.sql.functions import approx_count_distinct\n",
    "\n",
    "df_papers \\\n",
    "    .filter((size(col('fos')) >= 5) & (size(col('references')) >= 5)) \\\n",
    "    .join(df_aff, df_papers.paper_id == df_aff.paper_id, \"inner\") \\\n",
    "    .drop(df_aff.paper_id) \\\n",
    "    .join(df_aut, df_aff.author_id == df_aut.author_id, \"inner\") \\\n",
    "    .drop(df_aff.author_id) \\\n",
    "    .groupBy(\"author_id\") \\\n",
    "    .agg(count(\"paper_id\").alias(\"papers_count\"),\n",
    "         approx_count_distinct(\"organization\").alias(\"organizations_count\"),\n",
    "         collect_set(\"name\").alias(\"name\")) \\\n",
    "    .filter((size(\"name\") == 1) & (col(\"papers_count\") >= 3)  & (col(\"organizations_count\") >= 3)) \\\n",
    "    .orderBy(col(\"papers_count\").desc(), col(\"organizations_count\").desc()) \\\n",
    "    .select(explode(col(\"name\")).alias(\"name\"), col(\"papers_count\"), col(\"organizations_count\")) \\\n",
    "    .show(5)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
