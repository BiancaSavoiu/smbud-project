{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U4ugBnQJt8dk",
    "outputId": "85f42e41-b3d2-4c1b-de49-56addeabdf0a"
   },
   "outputs": [],
   "source": [
    "# Downloading pyspark\n",
    "#!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PREPARE THE ENVIRONMENT, UPLOAD DATA, PREPROCESS DATA AND CREATE THE TABLES: Author, Paper, Affiliation, Book, Journal and Conference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/11 12:23:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# With sparkSession we create a connection to our database\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import count, col, xxhash64, collect_list, explode\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType, TimestampType\n",
    "\n",
    "# Create an entry point to the PySpark Application\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"Bibliography\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "#INPUT_FILE = \"/content/drive/MyDrive/bibliography.json\"\n",
    "INPUT_FILE = \"bibliography.json\"\n",
    "OPTIONS = {'multiline': 'true', 'allowNumericLeadingZero': 'true', 'timestampFormat': \"yyyy-MM-dd'T'HH:mm:ss[.ZZZ'Z']\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Path does not exist: file:/Users/enricosimionato/PycharmProjects/smbud-project/spark/bibliography.json",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/3m/67ywp3l11x1bmc5kxj_p38h80000gn/T/ipykernel_21473/2590387298.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      9\u001B[0m      ])\n\u001B[1;32m     10\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 11\u001B[0;31m \u001B[0mdf_aut\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'json'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0moptions\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m**\u001B[0m\u001B[0mOPTIONS\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mschema\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mschemaAut\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjson\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mINPUT_FILE\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     12\u001B[0m \u001B[0mdf_aut\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdf_aut\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mselect\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mexplode\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdf_aut\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mauthors\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     13\u001B[0m \u001B[0mdf_aut\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdf_aut\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwithColumnRenamed\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"col\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"authors\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/readwriter.py\u001B[0m in \u001B[0;36mjson\u001B[0;34m(self, path, schema, primitivesAsString, prefersDecimal, allowComments, allowUnquotedFieldNames, allowSingleQuotes, allowNumericLeadingZero, allowBackslashEscapingAnyCharacter, mode, columnNameOfCorruptRecord, dateFormat, timestampFormat, multiLine, allowUnquotedControlChars, lineSep, samplingRatio, dropFieldIfAllNull, encoding, locale, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, allowNonNumericNumbers)\u001B[0m\n\u001B[1;32m    282\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mtype\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mlist\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    283\u001B[0m             \u001B[0;32massert\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_spark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_sc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jvm\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 284\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_df\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jreader\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjson\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_spark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_sc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jvm\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mPythonUtils\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtoSeq\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    285\u001B[0m         \u001B[0;32melif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mRDD\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    286\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1319\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1320\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1321\u001B[0;31m         return_value = get_return_value(\n\u001B[0m\u001B[1;32m   1322\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[1;32m   1323\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    194\u001B[0m                 \u001B[0;31m# Hide where the exception came from that shows a non-Pythonic\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    195\u001B[0m                 \u001B[0;31m# JVM exception message.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 196\u001B[0;31m                 \u001B[0;32mraise\u001B[0m \u001B[0mconverted\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    197\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    198\u001B[0m                 \u001B[0;32mraise\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mAnalysisException\u001B[0m: Path does not exist: file:/Users/enricosimionato/PycharmProjects/smbud-project/spark/bibliography.json"
     ]
    }
   ],
   "source": [
    "#AUTHOR TABLE\n",
    "schemaAut = StructType(\n",
    "    [StructField('authors', ArrayType(StructType([\n",
    "        StructField('_id', StringType(), nullable=False),\n",
    "        StructField('name', StringType(), True),\n",
    "        StructField('email', StringType(), True),\n",
    "        StructField('bio', StringType(), True),\n",
    "    ])), True)\n",
    "     ])\n",
    "\n",
    "df_aut = spark.read.format('json').options(**OPTIONS).schema(schemaAut).json(INPUT_FILE)\n",
    "df_aut = df_aut.select(explode(df_aut.authors))\n",
    "df_aut = df_aut.withColumnRenamed(\"col\", \"authors\")\n",
    "df_aut = df_aut.filter(col(\"authors._id\") != \"null\").select(\"authors._id\", \"authors.name\", \"authors.email\",\n",
    "                                                            \"authors.bio\")\n",
    "df_aut = df_aut.withColumnRenamed(\"_id\", \"author_id\")\n",
    "df_aut = df_aut.dropDuplicates([\"author_id\"])\n",
    "df_aut.printSchema()\n",
    "df_aut.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# PAPER TABLE WITHOUT PUBLICATION_ID\n",
    "schemaPaper = StructType(\n",
    "    [StructField('_id', StringType(), True),\n",
    "     StructField('title', StringType(), True),\n",
    "     StructField('keywords', ArrayType(StringType()), True),\n",
    "     StructField('fos', ArrayType(StringType()), True),\n",
    "     StructField('references', ArrayType(StringType()), True),\n",
    "     StructField('page_start', IntegerType(), True),\n",
    "     StructField('page_end', IntegerType(), True),\n",
    "     StructField('lang', StringType(), True),\n",
    "     StructField('doi', StringType(), True),\n",
    "     StructField('url', ArrayType(StringType()), True),\n",
    "     StructField('abstract', StringType(), True),\n",
    "     StructField('publication_type', StringType(), True),\n",
    "     StructField('date', TimestampType(), True)\n",
    "     ])\n",
    "\n",
    "df_paper = spark.read.format('json').options(**OPTIONS).schema(schemaPaper).json(INPUT_FILE)\n",
    "df_paper = df_paper.withColumnRenamed(\"_id\", \"paper_id\")\n",
    "df_paper.printSchema()\n",
    "df_paper.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# AFFILIATION TABLE\n",
    "schemaAffiliation = StructType(\n",
    "    [StructField('_id', StringType(), True),\n",
    "     StructField('authors', ArrayType(StructType([\n",
    "         StructField('_id', StringType(), True),\n",
    "         StructField('org', StringType(), True)\n",
    "     ])), True),\n",
    "     ])\n",
    "\n",
    "df_aff = spark.read.format('json').options(**OPTIONS).schema(schemaAffiliation).json(INPUT_FILE)\n",
    "df_aff = df_aff.withColumnRenamed(\"_id\", \"paper_id\")\n",
    "df_aff = df_aff.select(\"paper_id\", explode(df_aff.authors))\n",
    "df_aff = df_aff.withColumnRenamed(\"col\", \"authors\")\n",
    "df_aff = df_aff.filter(col(\"authors._id\") != \"null\").filter(col(\"paper_id\") != \"null\").select(\"paper_id\", \"authors._id\",\n",
    "                                                                                              \"authors.org\")\n",
    "df_aff = df_aff.withColumnRenamed(\"_id\", \"author_id\")\n",
    "df_aff = df_aff.dropDuplicates([\"author_id\", \"paper_id\"])\n",
    "df_aff = df_aff.withColumnRenamed(\"org\", \"organization\")\n",
    "df_aff.printSchema()\n",
    "df_aff.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# JOURNAL TABLE\n",
    "# Preprocessing of the journals for cleaning and merging the journals\n",
    "\n",
    "journal_schema_preprocessing = StructType(\n",
    "    [StructField('_id', StringType(), True),\n",
    "     StructField('issn', StringType(), True),\n",
    "     StructField('publisher', StringType(), True),\n",
    "     StructField('venue', StringType(), True),\n",
    "     StructField('volume', IntegerType(), True),\n",
    "     StructField('issue', IntegerType(), True),\n",
    "     StructField('publication_type', StringType(), True)])\n",
    "\n",
    "# Reading the json file\n",
    "df_journals_to_filter = spark.read.format('json').options(**OPTIONS).schema(journal_schema_preprocessing).json(\n",
    "    INPUT_FILE)\n",
    "\n",
    "# Filtering and adjusting the dataframe\n",
    "df_journals_to_filter = df_journals_to_filter.filter(col('publication_type') == 'Journal').filter(\n",
    "    col('issn') != 'null').filter(col('venue') != 'null').filter(col('issue') >= 0).filter(col('volume') >= 0)\n",
    "df_journals_to_filter = df_journals_to_filter.groupBy('venue', 'volume', 'issue', 'issn').agg(\n",
    "    collect_list('publisher').alias('publishersArray'), collect_list('_id').alias('_id'),\n",
    "    count(col('publisher')))  # count can be removed (I was interested in evaluating if the group by was meaningful)\n",
    "df_journals_to_insert = df_journals_to_filter.withColumn('publisher',\n",
    "                                                         df_journals_to_filter['publishersArray'][0]).select('venue',\n",
    "                                                                                                             'volume',\n",
    "                                                                                                             'issue',\n",
    "                                                                                                             'publisher',\n",
    "                                                                                                             'issn',\n",
    "                                                                                                             '_id')\n",
    "\n",
    "# Adding the new column which contains the publication_identifier\n",
    "df_journals = df_journals_to_insert.withColumn(\"publication_id\", xxhash64('venue', 'volume', 'issue', 'issn'))\n",
    "\n",
    "# Adding the foreign key to the papers\n",
    "exploded_journals = df_journals.select(explode('_id'), 'publication_id')\n",
    "#exploded_journals.show(truncate = False)\n",
    "\n",
    "df_papers_in_journals = exploded_journals.join(df_paper, exploded_journals.col == df_paper.paper_id, \"inner\")\n",
    "df_papers_in_journals = df_papers_in_journals.drop('col')\n",
    "\n",
    "df_journals = df_journals.drop(df_journals._id)\n",
    "\n",
    "# Visualizing the data\n",
    "# print('Papers')\n",
    "# df_papers_in_journals.show(truncate = False)\n",
    "print('Schema of the journals')\n",
    "df_journals.printSchema()\n",
    "print('Journals')\n",
    "df_journals.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# BOOK TABLE\n",
    "# Preprocessing of the books for cleaning and merging the books\n",
    "book_schema_preprocessing = StructType(\n",
    "    [StructField('_id', StringType(), True),\n",
    "     StructField('isbn', StringType(), True),\n",
    "     StructField('publisher', StringType(), True),\n",
    "     StructField('venue', StringType(), True),\n",
    "     StructField('publication_type', StringType(), True)])\n",
    "\n",
    "# Reading the json file\n",
    "df_books_to_filter = spark.read.format('json').options(**OPTIONS).schema(book_schema_preprocessing).json(INPUT_FILE)\n",
    "\n",
    "# Filtering and adjusting the dataframe\n",
    "df_books_to_filter = df_books_to_filter.filter(col('publication_type') == 'Book').filter(col('isbn') != 'null').filter(\n",
    "    col('venue') != 'null')\n",
    "df_books_to_filter = df_books_to_filter.groupBy('isbn', 'venue').agg(collect_list('publisher').alias('publishersArray'),\n",
    "                                                                     collect_list('_id').alias('_id'), count(\n",
    "        col('publisher')))  # count can be removed (I was interested in evaluating if the group by was meaningful)\n",
    "dfbooks_to_insert = df_books_to_filter.withColumn('publisher', df_books_to_filter['publishersArray'][0]).select('venue',\n",
    "                                                                                                                'isbn',\n",
    "                                                                                                                'publisher',\n",
    "                                                                                                                '_id')\n",
    "\n",
    "# Adding the new column which is the id\n",
    "df_books = dfbooks_to_insert.withColumn('publication_id', xxhash64('isbn', 'venue'))\n",
    "\n",
    "# Adding the foreign key to the papers\n",
    "exploded_books = df_books.select(explode('_id'), 'publication_id')\n",
    "# exploded_books.show(truncate = False)\n",
    "\n",
    "df_papers_in_books = exploded_books.join(df_paper, exploded_books.col == df_paper.paper_id)\n",
    "df_papers_in_books = df_papers_in_books.drop('col')\n",
    "\n",
    "df_books = df_books.drop(df_books._id)\n",
    "\n",
    "# Visualizing the data\n",
    "# print('Papers')\n",
    "# df_papers_in_books.show(truncate = False)\n",
    "print('Schema of the books')\n",
    "df_books.printSchema()\n",
    "print('Books')\n",
    "df_books.show(truncate=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# CONFERENCE TABLE\n",
    "# Preprocessing of the books for cleaning and merging the books\n",
    "\n",
    "schemaConf = StructType(\n",
    "    [StructField('_id', StringType(), True),\n",
    "     StructField('location', StringType(), True),\n",
    "     StructField('venue', StringType(), True),\n",
    "     StructField('publication_type', StringType(), True)])\n",
    "# Reading the json file\n",
    "df_conferences_to_filter = spark.read.format('json').options(**OPTIONS).schema(schemaConf).json(INPUT_FILE)\n",
    "\n",
    "# Filtering and adjusting the dataframe\n",
    "df_conferences_to_filter = df_conferences_to_filter \\\n",
    "    .filter(col('publication_type') == 'Conference') \\\n",
    "    .filter(col('venue') != 'null')\n",
    "# count can be removed (I was interested in evaluating if the group by was meaningful)\n",
    "df_conferences_to_filter = df_conferences_to_filter \\\n",
    "    .groupBy('venue') \\\n",
    "    .agg(collect_list('location').alias('locations_array'), collect_list('_id').alias('_id'), count(col('location')))\n",
    "df_conferences_to_insert = df_conferences_to_filter \\\n",
    "    .withColumn('location', df_conferences_to_filter['locations_array'][0]) \\\n",
    "    .select('venue', 'location', '_id')\n",
    "\n",
    "# Adding the new column which is the id\n",
    "df_conferences = df_conferences_to_insert.withColumn('publication_id', xxhash64('venue'))\n",
    "\n",
    "# Adding the foreign key to the papers\n",
    "exploded_conferences = df_conferences.select(explode('_id'), 'publication_id')\n",
    "#exploded_conferences.show(truncate = False)\n",
    "\n",
    "df_papers_in_conferences = exploded_conferences.join(df_paper, exploded_conferences.col == df_paper.paper_id)\n",
    "df_papers_in_conferences = df_papers_in_conferences.drop('col')\n",
    "\n",
    "df_conferences = df_conferences.drop(df_conferences._id)\n",
    "\n",
    "# Visualizing the data\n",
    "#print('Papers')\n",
    "#df_papers_in_conferences.show(truncate = False)\n",
    "print('Schema of the conferences')\n",
    "df_conferences.printSchema()\n",
    "print('Conferences')\n",
    "df_conferences.show(truncate=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Merging the 3 dataframe which one contains the papers published in a specific media\n",
    "df_papers = df_papers_in_books \\\n",
    "    .union(df_papers_in_journals) \\\n",
    "    .union(df_papers_in_conferences)\n",
    "\n",
    "# Visualizing the data\n",
    "print('Papers schema')\n",
    "df_papers.printSchema()\n",
    "print('Papers data')\n",
    "df_papers.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# For checking the result\n",
    "print('Papers published in books')\n",
    "df_papers \\\n",
    "    .filter(col('publication_type') == 'Book') \\\n",
    "    .select('paper_id', 'title', 'publication_type', 'publication_id') \\\n",
    "    .show()\n",
    "print('Papers published in journals')\n",
    "df_papers \\\n",
    "    .filter(col('publication_type') == 'Journal') \\\n",
    "    .select('paper_id', 'title', 'publication_type', 'publication_id') \\\n",
    "    .show()\n",
    "print('Papers published in conferences')\n",
    "df_papers \\\n",
    "    .filter(col('publication_type') == 'Conference') \\\n",
    "    .select('paper_id', 'title', 'publication_type', 'publication_id') \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COMMANDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Command 1: Add a new row to the Paper DataFrame\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "new_paper_file = 'single_paper.json'\n",
    "\n",
    "new_paper = spark.read.options(**OPTIONS).json(new_paper_file, schemaPaper) \\\n",
    "    .withColumnRenamed(\"_id\", \"paper_id\")\n",
    "\n",
    "journal_schema = StructType(\n",
    "    [StructField('issn', StringType(), True),\n",
    "     StructField('publisher', StringType(), True),\n",
    "     StructField('venue', StringType(), True),\n",
    "     StructField('volume', IntegerType(), True),\n",
    "     StructField('issue', IntegerType(), True)])\n",
    "\n",
    "journal = spark.read.options(**OPTIONS).json(new_paper_file, journal_schema) \\\n",
    "    .withColumn(\"publication_id\", xxhash64('venue', 'volume', 'issue', 'issn'))\n",
    "\n",
    "foreign_key = journal.head(1)[0].asDict()['publication_id']\n",
    "\n",
    "if df_journals.filter(col('publication_id') == foreign_key).count() == 0:\n",
    "    df_jounrals = df_journals.union(journal)\n",
    "\n",
    "paper_id = new_paper.head(1)[0].asDict()['paper_id']\n",
    "\n",
    "# Inserting foreign_key at position 1 (0 for programmers)\n",
    "new_paper = new_paper.select(lit(foreign_key).alias('publication_id'), '*')\n",
    "\n",
    "if df_papers.filter(col('paper_id') == paper_id).collect() == []:\n",
    "    df_papers = df_papers.union(new_paper)\n",
    "\n",
    "df_papers.filter(col('paper_id') == paper_id).show(vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Update one single row of a dataframe or multiple rows\n",
    "#The command modifies the DOI and URL of a paper with identifier equal to '53e997e4b7602d9701fdb48a'.\n",
    "#For doing this operation we firstly filter the dataframe keeping only the row to be modified. We add a column for each value we want to insert under the name new_field_name. Then we drop the old columns containing the previous values and we rename the new columns with the name of the old ones.\n",
    "#Finally, we make the union between the entire dataframe, without the row we want to modify, and the new entry.\n",
    "from pyspark.sql.functions import lit, array\n",
    "\n",
    "#df_papers\\\n",
    "#    .filter(col('title').like('%chatbot%'))\\\\\\n\",\n",
    "#    .select('title', 'paper_id')\\\\\\n\",\n",
    "#    .show(truncate = False)\\n\",\n",
    "updated_df_papers = df_papers\\\n",
    "    .filter(col('paper_id') == '53e997e4b7602d9701fdb48a')\n",
    "updated_df_papers = updated_df_papers\\\n",
    "    .withColumn('new_doi', lit('10.1007/11944577_37'))\\\n",
    "    .withColumn('new_url', array([lit('https://link.springer.com/chapter/10.1007/11944577_37')]))\n",
    "updated_df_papers = updated_df_papers\\\n",
    "    .drop(col('doi')).drop(col('url'))\\\n",
    "    .withColumnRenamed('new_doi', 'doi')\\\n",
    "    .withColumnRenamed('new_url', 'url')\n",
    "\n",
    "# To check the number of the paper is the same\n",
    "updated_df_papers = df_papers.filter(col('paper_id') != '53e997e4b7602d9701fdb48a').union(updated_df_papers)\n",
    "print('The size of the entire initial database is ' + str(df_papers.count()) + ', the size of the current database is ' + str(updated_df_papers.count()))\n",
    "updated_df_papers.filter(col('paper_id') == '53e997e4b7602d9701fdb48a').select('paper_id', 'title', 'doi', 'url').show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Command 3: remove an entire column\n",
    "df_papers_without_lang = df_papers \\\n",
    "    .drop('lang')\n",
    "\n",
    "df_papers_without_lang.printSchema()\n",
    "df_papers_without_lang.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Command 4: delete a group of rows\n",
    "\n",
    "from pyspark.sql.functions import year\n",
    "\n",
    "df_papers = df_papers \\\n",
    "    .filter(year('date') > '1950')\n",
    "\n",
    "df_papers.select('title', 'publication_type', 'date') \\\n",
    "    .orderBy('date') \\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Command 5: create a new column with the length of the paper (number of total pages)\n",
    "\n",
    "df_papers_total_pages = df_papers \\\n",
    "    .filter((col('page_start') >= 0) & (col('page_end') >= 0) & (col('page_start') <= col('page_end'))) \\\n",
    "    .withColumn('total_pages', col('page_end') - col('page_start'))\n",
    "\n",
    "df_papers_total_pages \\\n",
    "    .select(col('title'), col('page_start'), col('page_end'), col('total_pages')) \\\n",
    "    .show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QUERIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#Query 1: WHERE, JOIN\n",
    "\n",
    "venue, volume, issue = ('BMC Bioinformatics', '14', '1')\n",
    "\n",
    "df_papers_q1 = df_journals \\\n",
    "    .filter((col('venue') == venue) &\n",
    "            (col('volume') == volume) &\n",
    "            (col('issue') == issue)) \\\n",
    "    .join(df_papers,\n",
    "          (df_journals['publication_id'] == df_papers['publication_id']) &\n",
    "          (df_papers['publication_type'] == 'Journal'))\n",
    "\n",
    "df_papers_q1 \\\n",
    "    .select(['paper_id', 'title']) \\\n",
    "    .show(truncate=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Query 2: WHERE, LIMIT, LIKE\n",
    "# Find the papers written in the last twenty years in english whose keywords have the word \\verb|artificial| inside the keywords. We require that these papers have the DOI set to a not null value.\n",
    "# The results are ordered ascendingly by the date and only 20 elements are printed.\n",
    "\n",
    "from pyspark.sql.functions import current_timestamp, unix_timestamp\n",
    "\n",
    "df = df_papers.withColumn('current time', current_timestamp())\n",
    "df \\\n",
    "    .filter((((unix_timestamp('current time') - unix_timestamp('date')) / 3600 / 24 / 365) > 50) &\n",
    "            (col('doi').isNotNull())) \\\n",
    "    .select('title',\n",
    "            'date',\n",
    "            explode('keywords').alias('keyword')) \\\n",
    "    .filter(col('keyword').like('%artificial%')) \\\n",
    "    .distinct() \\\n",
    "    .sort(col('date').asc()) \\\n",
    "    .limit(20) \\\n",
    "    .show(truncate=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Query 3: WHERE, IN, Nested Query\n",
    "# Show the papers collected in a book that have `multiagent system` as keyword.\n",
    "\n",
    "nested_query = df_papers \\\n",
    "    .select('title',\n",
    "            'publication_type',\n",
    "            'paper_id',\n",
    "            'publication_id',\n",
    "            'date',\n",
    "            explode('keywords').alias('keyword')) \\\n",
    "    .filter(col('keyword').isin('multiagent system')) \\\n",
    "    .drop('keyword')\n",
    "df_conferences \\\n",
    "    .join(nested_query, 'publication_id') \\\n",
    "    .filter(col('publication_type') == 'Conference') \\\n",
    "    .orderBy('date', ascending=False) \\\n",
    "    .select(col('title').alias('paper title'),\n",
    "            col('venue').alias('conference venue')) \\\n",
    "    .show(5, truncate=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Query 4: GROUP BY, JOIN, AS\n",
    "\n",
    "from pyspark.sql.functions import collect_set, size\n",
    "\n",
    "df_aff \\\n",
    "    .join(df_papers, df_papers.paper_id == df_aff.paper_id, 'inner') \\\n",
    "    .drop(df_papers.paper_id) \\\n",
    "    .select('paper_id',\n",
    "            'organization',\n",
    "            'publication_type') \\\n",
    "    .filter((col('organization').isNotNull()) &\n",
    "            (col('organization') != \"\") &\n",
    "            (col('publication_type') == \"Conference\")) \\\n",
    "    .groupBy('organization') \\\n",
    "    .agg(collect_set('paper_id').alias('papers')) \\\n",
    "    .filter(size(col('papers')) > 10) \\\n",
    "    .show(truncate=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Query 5: WHERE, GROUP BY\n",
    "# Retrieve some statistics about papers\n",
    "\n",
    "from pyspark.sql.functions import sum, min, max, avg, format_number, variance\n",
    "\n",
    "df_papers_total_pages \\\n",
    "    .filter(year(col('date')) >= 2015) \\\n",
    "    .groupBy(year(col('date')).alias('year')) \\\n",
    "    .agg(count('paper_id').alias('total_papers'),\n",
    "         sum('total_pages').alias('total_pages'),\n",
    "         min('total_pages').alias('min_pages'),\n",
    "         max('total_pages').alias('max_pages'),\n",
    "         format_number(avg('total_pages'), 2).alias('avg_pages'),\n",
    "         format_number(variance('total_pages'), 2).alias('var_pages')) \\\n",
    "    .sort(col('year').desc()) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Query 6: GROUP BY, HAVING, AS\n",
    "\n",
    "df_papers \\\n",
    "    .select('paper_id',\n",
    "            'title',\n",
    "            explode(col('references')).alias('reference')) \\\n",
    "    .groupBy('reference') \\\n",
    "    .agg(count('paper_id').alias('references_count')) \\\n",
    "    .filter(col('references_count') > 30) \\\n",
    "    .join(df_papers, col('reference') == df_papers.paper_id) \\\n",
    "    .sort(col('references_count').desc()) \\\n",
    "    .select(['title', 'references_count']) \\\n",
    "    .show(truncate=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Query 7: WHERE, GROUP BY, HAVING, AS\n",
    "# The query returns the association between fields of study and keywords which are more present within the database and how many times they appear together\n",
    "from pyspark.sql.functions import year, col, lit\n",
    "\n",
    "df = df_papers\n",
    "df = df.withColumn('count', lit(1))\n",
    "df = df \\\n",
    "    .filter(\n",
    "    (col('doi').isNotNull()) &\n",
    "    (year(col('date')) >= 2000) &\n",
    "    (size(col('fos')) > 0) &\n",
    "    (size(col('keywords')) > 0)) \\\n",
    "    .select('fos', 'count', explode('keywords').alias('keyword')) \\\n",
    "    .select('keyword', explode('fos').alias('fos'), 'count') \\\n",
    "    .groupby('fos', 'keyword') \\\n",
    "    .sum('count') \\\n",
    "    .withColumnRenamed('sum(count)', 'couple count') \\\n",
    "    .filter(col('couple count') > 100) \\\n",
    "    .sort(col('couple count').desc()) \\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Query 8: WHERE, Nested Query (i.e., 2-step Queries), GROUP BY\n",
    "\n",
    "author_name = 'Hao Wang'\n",
    "\n",
    "sub_nested_query = df_aut \\\n",
    "    .filter(col('name') == author_name) \\\n",
    "    .select('author_id') \\\n",
    "    .rdd.flatMap(lambda x: x) \\\n",
    "    .collect()\n",
    "\n",
    "nested_query = df_aff \\\n",
    "    .filter(col('author_id').isin(sub_nested_query)) \\\n",
    "    .filter(col('organization') != 'null')\n",
    "\n",
    "df_papers \\\n",
    "    .join(nested_query, 'paper_id') \\\n",
    "    .select('paper_id',\n",
    "            'organization',\n",
    "            explode('fos').alias('field_of_study')) \\\n",
    "    .groupBy('field_of_study') \\\n",
    "    .agg(collect_set('organization').alias('organization')) \\\n",
    "    .orderBy('field_of_study') \\\n",
    "    .show(14, truncate=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Query 9: WHERE, GROUP BY, HAVING, 1 JOIN\n",
    "\n",
    "from pyspark.sql.functions import collect_set, concat, size\n",
    "\n",
    "df_journals \\\n",
    "    .withColumnRenamed('venue', 'venueJournals') \\\n",
    "    .filter((col('volume')) > 10) \\\n",
    "    .join(df_books, df_books.publisher == df_journals.publisher, 'inner') \\\n",
    "    .drop(df_journals.publisher) \\\n",
    "    .withColumnRenamed('venue', 'venueBooks') \\\n",
    "    .select('venueBooks',\n",
    "            'venueJournals',\n",
    "            'publisher') \\\n",
    "    .dropDuplicates(['venueBooks',\n",
    "                     'venueJournals',\n",
    "                     'publisher']) \\\n",
    "    .groupBy('publisher') \\\n",
    "    .agg(collect_set('venueBooks').alias('books'),\n",
    "         collect_set('venueJournals').alias('journals')) \\\n",
    "    .withColumn('total_publications_per_publisher', concat('books', 'journals')) \\\n",
    "    .filter(size('total_publications_per_publisher') > '500') \\\n",
    "    .select('publisher',\n",
    "            'total_publications_per_publisher') \\\n",
    "    .show(truncate=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Query 10: WHERE, GROUP BY, HAVING, 2 JOINs\n",
    "# Retrieve authors who worked for at least 3 different organizations and have published at least 3 papers with at least 5 fos and 5 references each\n",
    "\n",
    "from pyspark.sql.functions import approx_count_distinct\n",
    "\n",
    "df_papers \\\n",
    "    .filter((size(col('fos')) >= 5) &\n",
    "            (size(col('references')) >= 5)) \\\n",
    "    .join(df_aff, df_papers.paper_id == df_aff.paper_id, 'inner') \\\n",
    "    .drop(df_aff.paper_id) \\\n",
    "    .join(df_aut, df_aff.author_id == df_aut.author_id, 'inner') \\\n",
    "    .drop(df_aff.author_id) \\\n",
    "    .groupBy('author_id') \\\n",
    "    .agg(count('paper_id').alias('papers_count'),\n",
    "         approx_count_distinct('organization').alias('organizations_count'),\n",
    "         collect_set('name').alias('name')) \\\n",
    "    .filter((size('name') == 1) &\n",
    "            (col('papers_count') >= 3) &\n",
    "            (col('organizations_count') >= 3)) \\\n",
    "    .orderBy(col('papers_count').desc(),\n",
    "             col('organizations_count').desc()) \\\n",
    "    .select(explode('name').alias('name'),\n",
    "            'papers_count',\n",
    "            'organizations_count') \\\n",
    "    .show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
