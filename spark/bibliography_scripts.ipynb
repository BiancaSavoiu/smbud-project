{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U4ugBnQJt8dk",
    "outputId": "85f42e41-b3d2-4c1b-de49-56addeabdf0a"
   },
   "outputs": [],
   "source": [
    "# Downloading pyspark\n",
    "#!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PREPARE THE ENVIRONMENT, UPLOAD DATA, PREPROCESS DATA AND CREATE THE TABLES: Author, Paper, Affiliation, Book, Journal and Conference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/11 19:33:50 WARN Utils: Your hostname, MacBook-Air-di-Enrico.local resolves to a loopback address: 127.0.0.1; using 172.20.10.13 instead (on interface en0)\n",
      "22/12/11 19:33:50 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/11 19:33:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# With sparkSession we create a connection to our database\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import count, col, xxhash64, collect_list, explode\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType, TimestampType\n",
    "\n",
    "# Create an entry point to the PySpark Application\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"Bibliography\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "INPUT_FILE = \"bibliography.json\"\n",
    "OPTIONS = {'multiline': 'true', 'allowNumericLeadingZero': 'true', 'timestampFormat': \"yyyy-MM-dd'T'HH:mm:ss[.ZZZ'Z']\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- author_id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- bio: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+--------------------+--------------------+\n",
      "|           author_id|              name|               email|                 bio|\n",
      "+--------------------+------------------+--------------------+--------------------+\n",
      "|53f3186fdabfae9a8...|   A. M. A. Hariri|a..m..a..hariride...|My name is A. M. ...|\n",
      "|53f3186fdabfae9a8...|    Matthew Prowse|matthew.prowsefb@...|My name is Matthe...|\n",
      "|53f31870dabfae9a8...|       Sui-ping Qi|sui-ping.qi19@gma...|My name is Sui-pi...|\n",
      "|53f31871dabfae9a8...|     Renato Fabbri|renato.fabbrib7@g...|My name is Renato...|\n",
      "|53f31873dabfae9a8...|   Joachim Schimpf|joachim.schimpf8a...|My name is Joachi...|\n",
      "|53f31874dabfae9a8...|    E. Di Bernardo|e..di.bernardo10@...|My name is E. Di ...|\n",
      "|53f31875dabfae9a8...|    Steven F. Roth|steven.f..roth46@...|My name is Steven...|\n",
      "|53f31878dabfae9a8...|      Nima Zahadat|nima.zahadat3d@gm...|My name is Nima Z...|\n",
      "|53f3187ddabfae9a8...|         Ke Fa Cen|ke.fa.cen23@gmail...|My name is Ke Fa ...|\n",
      "|53f31881dabfae9a8...|    Ricky Houghton|ricky.houghton97@...|My name is Ricky ...|\n",
      "|53f31881dabfae9a8...|Eduardo H. Ramirez|eduardo.h..ramire...|My name is Eduard...|\n",
      "|53f31883dabfae9a8...| Cassidy J. Curtis|cassidy.j..curtis...|My name is Cassid...|\n",
      "|53f31885dabfae9a8...|  Brian D. Koblenz|brian.d..koblenz4...|My name is Brian ...|\n",
      "|53f31887dabfae9a8...|        Guohua Wan|guohua.wanca@gmai...|My name is Guohua...|\n",
      "|53f3188adabfae9a8...|     Irineu Theiss|irineu.theissfe@g...|My name is Irineu...|\n",
      "|53f31892dabfae9a8...|     Derek Yip-Hoi|derek.yip-hoidd@g...|My name is Derek ...|\n",
      "|53f31892dabfae9a8...|     Soo-Hyung Kim|soo-hyung.kimf5@g...|My name is Soo-Hy...|\n",
      "|53f31893dabfae9a8...|      Harold Lorin|harold.lorin7c@gm...|My name is Harold...|\n",
      "|53f3189ddabfae9a8...|    J. Nagy-György|j..nagy-györgyf4@...|My name is J. Nag...|\n",
      "|53f3189ddabfae9a8...|    Roy Varshavsky|roy.varshavsky1d@...|My name is Roy Va...|\n",
      "+--------------------+------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#AUTHOR TABLE\n",
    "schemaAut = StructType(\n",
    "    [StructField('authors', ArrayType(StructType([\n",
    "        StructField('_id', StringType(), nullable=False),\n",
    "        StructField('name', StringType(), True),\n",
    "        StructField('email', StringType(), True),\n",
    "        StructField('bio', StringType(), True),\n",
    "    ])), True)\n",
    "     ])\n",
    "\n",
    "df_aut = spark.read.format('json').options(**OPTIONS).schema(schemaAut).json(INPUT_FILE)\n",
    "df_aut = df_aut.select(explode(df_aut.authors))\n",
    "df_aut = df_aut.withColumnRenamed(\"col\", \"authors\")\n",
    "df_aut = df_aut.filter(col(\"authors._id\") != \"null\").select(\"authors._id\", \"authors.name\", \"authors.email\",\n",
    "                                                            \"authors.bio\")\n",
    "df_aut = df_aut.withColumnRenamed(\"_id\", \"author_id\")\n",
    "df_aut = df_aut.dropDuplicates([\"author_id\"])\n",
    "df_aut.printSchema()\n",
    "df_aut.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- paper_id: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- keywords: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- fos: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- references: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- page_start: integer (nullable = true)\n",
      " |-- page_end: integer (nullable = true)\n",
      " |-- lang: string (nullable = true)\n",
      " |-- abstract: string (nullable = true)\n",
      " |-- publication_type: string (nullable = true)\n",
      " |-- date: timestamp (nullable = true)\n",
      " |-- doi: string (nullable = true)\n",
      " |-- url: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+----------+--------+----+--------------------+----------------+-------------------+--------------------+--------------------+\n",
      "|            paper_id|               title|            keywords|                 fos|          references|page_start|page_end|lang|            abstract|publication_type|               date|                 doi|                 url|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+----------+--------+----+--------------------+----------------+-------------------+--------------------+--------------------+\n",
      "|53e99784b7602d970...|Using XML to Inte...|[internet, hyperm...|[xml base, world ...|[53e9adbdb7602d97...|       167|     172|  en|The eXtensible Ma...|            Book|1974-09-13 06:34:29|10.1109/CMPSAC.20...|[http://dx.doi.or...|\n",
      "|53e99784b7602d970...|               FCLOS|[molap, subsumpti...|[information syst...|[53e99ee0b7602d97...|       192|     220|  en|Mobile online ana...|         Journal|1950-04-14 05:33:22|10.1016/j.datak.2...|[http://dx.doi.or...|\n",
      "|53e99785b7602d970...|              Bhoomi|[icts, e governan...|[revenue, transpa...|[53e9b2ffb7602d97...|        20|      31|  en|The emergence of ...|         Journal|1984-04-06 03:49:29|10.1016/j.tele.20...|[http://dx.doi.or...|\n",
      "|53e9978ab7602d970...|                Laps|[health care, hom...|[health care, pop...|[573695936e3b1202...|       962|     976|  en|The health care s...|         Journal|1951-10-28 04:52:04|10.1016/j.ejor.20...|[http://dx.doi.or...|\n",
      "|53e9978db7602d970...|             Mindful|[meta learning, h...|[data science, ro...|[53e9b0c2b7602d97...|      3253|    3274|  en|Common inductive ...|         Journal|1955-04-13 19:06:45|10.1016/j.ins.200...|[http://dx.doi.or...|\n",
      "|53e9978db7602d970...|             MESHMdl|[tuple space, mob...|[middleware, mobi...|[53e99a49b7602d97...|       467|     487|  en|Mobile ad hoc net...|         Journal|2009-09-23 09:35:36|10.1016/j.pmcj.20...|[http://dx.doi.or...|\n",
      "|53e9978db7602d970...|               iCity|[irregular cellul...|[software tool, a...|[53e9ac54b7602d97...|       761|     773|  en|The objective of ...|         Journal|2002-05-14 19:30:38|10.1016/j.envsoft...|[http://dx.doi.or...|\n",
      "|53e9978db7602d970...|              Wisdom|[decision support...|[cognitive map, r...|[53e99b7eb7602d97...|       156|     171|  en|Many decision sup...|         Journal|1987-12-30 04:51:23|10.1016/j.ejor.20...|[http://dx.doi.or...|\n",
      "|53e99792b7602d970...|        SwissAnalyst|         [key words]|[computer science...|[5c795a6e4895d9cb...|       393|     406|  en|This paper introd...|      Conference|1991-03-30 19:52:33|10.1007/1-4020-81...|                null|\n",
      "|53e99792b7602d970...|Short-Term Traffi...|[considerable acc...|[spline mathemati...|[53e9b95bb7602d97...|       669|     675|  en|A promising traff...|      Conference|2020-03-15 04:13:40|10.1109/FSKD.2008...|[http://dx.doi.or...|\n",
      "|53e99792b7602d970...|An approach to fe...|[feature location...|[data mining, cau...|[53e9b6eeb7602d97...|        57|      68|  en|This paper descri...|         Journal|2012-05-05 11:48:40|10.1016/j.jss.200...|[http://dx.doi.or...|\n",
      "|53e99792b7602d970...|Nested Graph-Stru...|[nested graph str...|[adjacency matrix...|[53e9b049b7602d97...|         1|      12|  en|This paper descri...|            Book|1965-07-21 08:40:34|  10.1007/BFb0056317|[http://dx.doi.or...|\n",
      "|53e99792b7602d970...|Demo: Yalut -- us...|[cellular data tr...|[world wide web, ...|[53e9b04eb7602d97...|       360|     361|  en|Yalut is a novel ...|      Conference|2009-02-24 08:00:34|10.1145/2594368.2...|[http://dx.doi.or...|\n",
      "|53e99792b7602d970...|A Uniform Paralle...|[knowledge discov...|[data mining, dat...|[53e99cbbb7602d97...|       306|     312|  en|Grid is a new sol...|         Journal|2015-02-28 22:51:57|10.1007/978-3-540...|[http://dx.doi.or...|\n",
      "|53e99792b7602d970...|WAVELET-BASED IMA...|[mathematical mod...|[computer vision,...|[53e9b550b7602d97...|      1737|    1740|  en|Because digital i...|      Conference|2005-12-16 23:43:26|10.1109/ICIP.2010...|[http://dx.doi.or...|\n",
      "|53e99792b7602d970...|A fuzzy multi-obj...|[location, fire s...|[objective progra...|[53e9bd50b7602d97...|       903|     915|  en|Location of fire ...|         Journal|1971-12-17 16:55:38|10.1016/j.ejor.20...|[http://dx.doi.or...|\n",
      "|53e99792b7602d970...|Adaptive presenta...|[evolving informa...|[metadata, progra...|[53e9a2c8b7602d97...|       193|     196|  en|The article descr...|            Book|2011-05-08 17:41:02|10.1109/ICALT.200...|[http://dx.doi.or...|\n",
      "|53e99792b7602d970...|Testing the stabi...|[asymptotic justi...|[functional princ...|[53e9ace1b7602d97...|       352|     367|  en|The functional au...|         Journal|1984-05-13 13:56:58|10.1016/j.jmva.20...|[http://dx.doi.or...|\n",
      "|53e99792b7602d970...|Statistical Prope...|[power cables, po...|[topology, trippi...|[53e9a7ddb7602d97...|      2517|    2526|  en|We present the sy...|      Conference|2018-08-18 19:58:00|10.1109/HICSS.201...|[http://dx.doi.or...|\n",
      "|53e99792b7602d970...|Generating novel ...|[adaptive behavio...|[interactive evol...|[53e9bc80b7602d97...|         8|      14|  en|Simulated evoluti...|         Journal|2021-05-05 05:11:38|10.1145/1056754.1...|[http://dx.doi.or...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+----------+--------+----+--------------------+----------------+-------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# PAPER TABLE WITHOUT PUBLICATION_ID\n",
    "schemaPaper = StructType(\n",
    "    [StructField('_id', StringType(), True),\n",
    "     StructField('title', StringType(), True),\n",
    "     StructField('keywords', ArrayType(StringType()), True),\n",
    "     StructField('fos', ArrayType(StringType()), True),\n",
    "     StructField('references', ArrayType(StringType()), True),\n",
    "     StructField('page_start', IntegerType(), True),\n",
    "     StructField('page_end', IntegerType(), True),\n",
    "     StructField('lang', StringType(), True),\n",
    "     StructField('abstract', StringType(), True),\n",
    "     StructField('publication_type', StringType(), True),\n",
    "     StructField('date', TimestampType(), True),\n",
    "     StructField('doi', StringType(), True),\n",
    "     StructField('url', ArrayType(StringType()), True)\n",
    "     ])\n",
    "\n",
    "df_paper = spark.read.format('json').options(**OPTIONS).schema(schemaPaper).json(INPUT_FILE)\n",
    "df_paper = df_paper.withColumnRenamed(\"_id\", \"paper_id\")\n",
    "df_paper.printSchema()\n",
    "df_paper.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- paper_id: string (nullable = true)\n",
      " |-- author_id: string (nullable = true)\n",
      " |-- organization: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|            paper_id|           author_id|        organization|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|53e998c7b7602d970...|53f3186fdabfae9a8...|Department of Sta...|\n",
      "|53e99827b7602d970...|53f3186fdabfae9a8...|Laboratory for Fo...|\n",
      "|53e99924b7602d970...|53f31870dabfae9a8...|Henan Academy of ...|\n",
      "|53e998dbb7602d970...|53f31871dabfae9a8...|Instituto de Físi...|\n",
      "|53e998f6b7602d970...|53f31873dabfae9a8...|                null|\n",
      "|53e998bfb7602d970...|53f31874dabfae9a8...|                null|\n",
      "|53e9984bb7602d970...|53f31875dabfae9a8...|                null|\n",
      "|53e998e8b7602d970...|53f31878dabfae9a8...|George Mason Univ...|\n",
      "|53e99905b7602d970...|53f3187ddabfae9a8...|State Key Laborat...|\n",
      "|53e998e9b7602d970...|53f31881dabfae9a8...|                null|\n",
      "|53e9984fb7602d970...|53f31881dabfae9a8...|Tecnologico de Mo...|\n",
      "|53e9980eb7602d970...|53f31883dabfae9a8...|University of Was...|\n",
      "|53e997e9b7602d970...|53f31885dabfae9a8...|Digital Equipment...|\n",
      "|53e998b0b7602d970...|53f31887dabfae9a8...|Antai College of ...|\n",
      "|53e998cdb7602d970...|53f31887dabfae9a8...|Department of Ind...|\n",
      "|53e998f0b7602d970...|53f3188adabfae9a8...|E-Gov, Juridical ...|\n",
      "|53e998fcb7602d970...|53f31892dabfae9a8...|2250 G G Brown, A...|\n",
      "|53e99998b7602d970...|53f31892dabfae9a8...|Chonnam National ...|\n",
      "|53e99800b7602d970...|53f31893dabfae9a8...|IBM Systems Resea...|\n",
      "|53e99866b7602d970...|53f3189ddabfae9a8...|Department of Mat...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# AFFILIATION TABLE\n",
    "schemaAffiliation = StructType(\n",
    "    [StructField('_id', StringType(), True),\n",
    "     StructField('authors', ArrayType(StructType([\n",
    "         StructField('_id', StringType(), True),\n",
    "         StructField('org', StringType(), True)\n",
    "     ])), True),\n",
    "     ])\n",
    "\n",
    "df_aff = spark.read.format('json').options(**OPTIONS).schema(schemaAffiliation).json(INPUT_FILE)\n",
    "df_aff = df_aff.withColumnRenamed(\"_id\", \"paper_id\")\n",
    "df_aff = df_aff.select(\"paper_id\", explode(df_aff.authors))\n",
    "df_aff = df_aff.withColumnRenamed(\"col\", \"authors\")\n",
    "df_aff = df_aff.filter(col(\"authors._id\") != \"null\").filter(col(\"paper_id\") != \"null\").select(\"paper_id\", \"authors._id\",\n",
    "                                                                                              \"authors.org\")\n",
    "df_aff = df_aff.withColumnRenamed(\"_id\", \"author_id\")\n",
    "df_aff = df_aff.dropDuplicates([\"author_id\", \"paper_id\"])\n",
    "df_aff = df_aff.withColumnRenamed(\"org\", \"organization\")\n",
    "df_aff.printSchema()\n",
    "df_aff.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema of the journals\n",
      "root\n",
      " |-- venue: string (nullable = true)\n",
      " |-- volume: integer (nullable = true)\n",
      " |-- issue: integer (nullable = true)\n",
      " |-- publisher: string (nullable = true)\n",
      " |-- issn: string (nullable = true)\n",
      " |-- publication_id: long (nullable = false)\n",
      "\n",
      "Journals\n",
      "+-----------------------+------+-----+------------------------------------------------------------------+---------+--------------------+\n",
      "|venue                  |volume|issue|publisher                                                         |issn     |publication_id      |\n",
      "+-----------------------+------+-----+------------------------------------------------------------------+---------+--------------------+\n",
      "|4OR                    |2     |4    |Inderscience Publishers                                           |1614-2411|-8668689828003778653|\n",
      "|4OR                    |3     |1    |Accent Social and Welfare Society                                 |1614-2411|-6055461944651662439|\n",
      "|4OR                    |4     |1    |Innovative Information Science & Technology Research Group (ISYOU)|1614-2411|4482587362010925183 |\n",
      "|4OR                    |4     |3    |Elsevier                                                          |1614-2411|-5369333412851545957|\n",
      "|4OR                    |6     |2    |John Wiley & Sons Inc.                                            |1614-2411|4907811008184705540 |\n",
      "|4OR                    |6     |4    |Elsevier                                                          |1614-2411|-7478992719207099405|\n",
      "|4OR                    |7     |2    |Taylor and Francis Ltd.                                           |1614-2411|-5578571051264951046|\n",
      "|4OR                    |8     |2    |Intellect Ltd.                                                    |1614-2411|3303577774132183121 |\n",
      "|4OR                    |8     |4    |Springer London                                                   |1614-2411|-3839102222582550984|\n",
      "|4OR                    |11    |2    |IOP Publishing Ltd.                                               |1614-2411|-8848711838540298032|\n",
      "|A retargetable debugger|27    |7    |IEEE Computer Society                                             |0362-1340|5898809803706162287 |\n",
      "|AAAI                   |21    |12   |Hindawi Publishing Corporation                                    |1041-4347|226561117377407236  |\n",
      "|AAIM                   |106   |4    |AGH University of Science and Technology                          |0020-0190|-1537115915808707536|\n",
      "|ACL                    |95    |3    |Wiley-Blackwell Publishing Ltd                                    |0885-6125|6922825622688887655 |\n",
      "|ACM Comput. Surv.      |10    |4    |Intellect Ltd.                                                    |0360-0300|-4352573166015971962|\n",
      "|ACM Comput. Surv.      |15    |3    |Elsevier                                                          |0360-0300|7493946580737751003 |\n",
      "|ACM Comput. Surv.      |15    |4    |Universidad Internacional de la Rioja                             |0360-0300|7340232518232887060 |\n",
      "|ACM Comput. Surv.      |16    |3    |Technischen Universitat Braunschweig                              |0360-0300|-7713800651548746073|\n",
      "|ACM Comput. Surv.      |18    |4    |Springer London                                                   |0360-0300|-7000780314495031015|\n",
      "|ACM Comput. Surv.      |27    |1    |Wiley-Blackwell Publishing Ltd                                    |0360-0300|-7255461002200856409|\n",
      "+-----------------------+------+-----+------------------------------------------------------------------+---------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# JOURNAL TABLE\n",
    "# Preprocessing of the journals for cleaning and merging the journals\n",
    "\n",
    "journal_schema_preprocessing = StructType(\n",
    "    [StructField('_id', StringType(), True),\n",
    "     StructField('issn', StringType(), True),\n",
    "     StructField('publisher', StringType(), True),\n",
    "     StructField('venue', StringType(), True),\n",
    "     StructField('volume', IntegerType(), True),\n",
    "     StructField('issue', IntegerType(), True),\n",
    "     StructField('publication_type', StringType(), True)])\n",
    "\n",
    "# Reading the json file\n",
    "df_journals_to_filter = spark.read.format('json').options(**OPTIONS).schema(journal_schema_preprocessing).json(\n",
    "    INPUT_FILE)\n",
    "\n",
    "# Filtering and adjusting the dataframe\n",
    "df_journals_to_filter = df_journals_to_filter.filter(col('publication_type') == 'Journal').filter(\n",
    "    col('issn') != 'null').filter(col('venue') != 'null').filter(col('issue') >= 0).filter(col('volume') >= 0)\n",
    "df_journals_to_filter = df_journals_to_filter.groupBy('venue', 'volume', 'issue', 'issn').agg(\n",
    "    collect_list('publisher').alias('publishersArray'), collect_list('_id').alias('_id'),\n",
    "    count(col('publisher')))  # count can be removed (I was interested in evaluating if the group by was meaningful)\n",
    "df_journals_to_insert = df_journals_to_filter.withColumn('publisher',\n",
    "                                                         df_journals_to_filter['publishersArray'][0]).select('venue',\n",
    "                                                                                                             'volume',\n",
    "                                                                                                             'issue',\n",
    "                                                                                                             'publisher',\n",
    "                                                                                                             'issn',\n",
    "                                                                                                             '_id')\n",
    "\n",
    "# Adding the new column which contains the publication_identifier\n",
    "df_journals = df_journals_to_insert.withColumn(\"publication_id\", xxhash64('venue', 'volume', 'issue', 'issn'))\n",
    "\n",
    "# Adding the foreign key to the papers\n",
    "exploded_journals = df_journals.select(explode('_id'), 'publication_id')\n",
    "#exploded_journals.show(truncate = False)\n",
    "\n",
    "df_papers_in_journals = exploded_journals.join(df_paper, exploded_journals.col == df_paper.paper_id, \"inner\")\n",
    "df_papers_in_journals = df_papers_in_journals.drop('col')\n",
    "\n",
    "df_journals = df_journals.drop(df_journals._id)\n",
    "\n",
    "# Visualizing the data\n",
    "#print('Papers')\n",
    "#df_papers_in_journals.show(truncate = False)\n",
    "print('Schema of the journals')\n",
    "df_journals.printSchema()\n",
    "print('Journals')\n",
    "df_journals.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema of the books\n",
      "root\n",
      " |-- venue: string (nullable = true)\n",
      " |-- isbn: string (nullable = true)\n",
      " |-- publisher: string (nullable = true)\n",
      " |-- publication_id: long (nullable = false)\n",
      "\n",
      "Books\n",
      "+----------------------------------------+-------------+----------------------------------------+--------------------+\n",
      "|                                   venue|         isbn|                               publisher|      publication_id|\n",
      "+----------------------------------------+-------------+----------------------------------------+--------------------+\n",
      "|  ACM SIGSOFT Software Engineering Notes|-159593-125-2|AGH University of Science and Technology|-4146681027596907540|\n",
      "|                     Theor. Comput. Sci.|  0-0304-3975|                                Elsevier|-8641604748836216424|\n",
      "|                         ACL (Companion)| 0-111-456789|                          Intellect Ltd.|-1765624198251536382|\n",
      "|                                    VLDB|0-12-088469-0|The American Society of Mechanical En...|-6546099223240010863|\n",
      "|Papers from the second workshop Vol. ...|0-12-597345-4|                            SpringerOpen|-5170613698490267273|\n",
      "|                                    VLDB|0-12-722442-4|              Open Library of Humanities| 5803877754064281982|\n",
      "|                     Artif. Intell. Rev.|0-13-176751-8| World Scientific Publishing Co. Pte Ltd|-5191022792199354246|\n",
      "|SFCS '85 Proceedings of the 26th Annu...|0-13-652017-0|                                Elsevier| 4898287511903231544|\n",
      "|                                     CHI|0-201-30987-4|AGH University of Science and Technology| 4745093320936461129|\n",
      "|              Computer Human Interaction|0-201-30987-4|                        Brown University|-3213114594400519435|\n",
      "|                                SIGGRAPH|0-201-48560-5|       Springer Science + Business Media| 7678090193378385384|\n",
      "|                                     CHI|0-201-50932-6|                                Elsevier|   22285894483278744|\n",
      "|                         Sigplan Notices|0-201-53372-3|                                Elsevier|  127134876966261428|\n",
      "|                                   ISSAC|0-201-54892-5|                 Taylor and Francis Ltd.|-1355898789984227595|\n",
      "|   History of programming languages---II|0-201-89502-1|                         Springer Verlag|-2667848576388063536|\n",
      "|                         Neural Networks|0-262-03188-4|                                Elsevier| 4141971837342876240|\n",
      "|                Computational Statistics|0-262-10076-2|       Springer Science + Business Media| -619826641325096993|\n",
      "|Symposium on Programming Language Imp...|0-262-19297-7|                                Elsevier| 6701520537364205852|\n",
      "|                                    AAAI|0-262-51057-X|               Tsinghua University Press| 1804946402782986900|\n",
      "|                          Artif. Intell.|0-262-51091-X|   Editorial and Publishing Board of JIG|  529998888475133215|\n",
      "+----------------------------------------+-------------+----------------------------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# BOOK TABLE\n",
    "# Preprocessing of the books for cleaning and merging the books\n",
    "book_schema_preprocessing = StructType(\n",
    "    [StructField('_id', StringType(), True),\n",
    "     StructField('isbn', StringType(), True),\n",
    "     StructField('publisher', StringType(), True),\n",
    "     StructField('venue', StringType(), True),\n",
    "     StructField('publication_type', StringType(), True)])\n",
    "\n",
    "# Reading the json file\n",
    "df_books_to_filter = spark.read.format('json').options(**OPTIONS).schema(book_schema_preprocessing).json(INPUT_FILE)\n",
    "\n",
    "# Filtering and adjusting the dataframe\n",
    "df_books_to_filter = df_books_to_filter.filter(col('publication_type') == 'Book').filter(col('isbn') != 'null').filter(\n",
    "    col('venue') != 'null')\n",
    "df_books_to_filter = df_books_to_filter.groupBy('isbn', 'venue').agg(collect_list('publisher').alias('publishersArray'),\n",
    "                                                                     collect_list('_id').alias('_id'), count(\n",
    "        col('publisher')))  # count can be removed (I was interested in evaluating if the group by was meaningful)\n",
    "dfbooks_to_insert = df_books_to_filter.withColumn('publisher', df_books_to_filter['publishersArray'][0]).select('venue',\n",
    "                                                                                                                'isbn',\n",
    "                                                                                                                'publisher',\n",
    "                                                                                                                '_id')\n",
    "\n",
    "# Adding the new column which is the id\n",
    "df_books = dfbooks_to_insert.withColumn('publication_id', xxhash64('isbn', 'venue'))\n",
    "\n",
    "# Adding the foreign key to the papers\n",
    "exploded_books = df_books.select(explode('_id'), 'publication_id')\n",
    "# exploded_books.show(truncate = False)\n",
    "\n",
    "df_papers_in_books = exploded_books.join(df_paper, exploded_books.col == df_paper.paper_id)\n",
    "df_papers_in_books = df_papers_in_books.drop('col')\n",
    "\n",
    "df_books = df_books.drop(df_books._id)\n",
    "\n",
    "# Visualizing the data\n",
    "# print('Papers')\n",
    "# df_papers_in_books.show(truncate = False)\n",
    "print('Schema of the books')\n",
    "df_books.printSchema()\n",
    "print('Books')\n",
    "df_books.show(truncate=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema of the conferences\n",
      "root\n",
      " |-- venue: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- publication_id: long (nullable = false)\n",
      "\n",
      "Conferences\n",
      "+--------------------------------------------------+--------------------------+--------------------+\n",
      "|                                             venue|                  location|      publication_id|\n",
      "+--------------------------------------------------+--------------------------+--------------------+\n",
      "|                                           \"EDUCON|            Moscow, Russia|  950373860555954453|\n",
      "|2012 50TH ANNUAL ALLERTON CONFERENCE ON COMMUNI...|           Dublin, Ireland|-4245717996156385657|\n",
      "|                                        2985415099|       Mexico City, Mexico| -762166203857654039|\n",
      "|                                        2985532720|              Milan, Italy|-7124098142925130015|\n",
      "|                                        2987493177|              Osaka, Japan| 7551243357793819285|\n",
      "|                                            3D-GIS|                London, UK| 1334491883820369589|\n",
      "|                                              3DIM|Johannesburg, South Africa|-1039299792543803944|\n",
      "|                                           3DIMPVT|         Helsinki, Finland|-4577231382119670667|\n",
      "|                                              3DOR|                London, UK|-8797854120325021263|\n",
      "|                                             3DPVT|    Amsterdam, Netherlands| 1375276120944605061|\n",
      "|                                              3DUI|        Seoul, South Korea| 1791021788110955539|\n",
      "|                                               3DV|        Jakarta, Indonesia| 3883608681790763346|\n",
      "|                                            3PGCIC|            Beijing, China|-3347673940037205862|\n",
      "|                   50 Years of Integer Programming|                London, UK|-5154516725105661246|\n",
      "|                                             AAMAS|              Milan, Italy| 5503995461761334696|\n",
      "|                                         AAMAS (2)|             Paris, France|-7242857281816016648|\n",
      "|                                              ACAI|           Toronto, Canada| 8524679771896464283|\n",
      "|ACC'09 Proceedings of the 2009 conference on Am...|            Beijing, China|-3525577454028830485|\n",
      "|                                              ACCV|                London, UK| 8287527867910422941|\n",
      "|                                          ACCV (1)|          Tel Aviv, Israel|-2757184551560203638|\n",
      "+--------------------------------------------------+--------------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# CONFERENCE TABLE\n",
    "# Preprocessing of the books for cleaning and merging the books\n",
    "\n",
    "schemaConf = StructType(\n",
    "    [StructField('_id', StringType(), True),\n",
    "     StructField('location', StringType(), True),\n",
    "     StructField('venue', StringType(), True),\n",
    "     StructField('publication_type', StringType(), True)])\n",
    "# Reading the json file\n",
    "df_conferences_to_filter = spark.read.format('json').options(**OPTIONS).schema(schemaConf).json(INPUT_FILE)\n",
    "\n",
    "# Filtering and adjusting the dataframe\n",
    "df_conferences_to_filter = df_conferences_to_filter \\\n",
    "    .filter(col('publication_type') == 'Conference') \\\n",
    "    .filter(col('venue') != 'null')\n",
    "# count can be removed (I was interested in evaluating if the group by was meaningful)\n",
    "df_conferences_to_filter = df_conferences_to_filter \\\n",
    "    .groupBy('venue') \\\n",
    "    .agg(collect_list('location').alias('locations_array'), collect_list('_id').alias('_id'), count(col('location')))\n",
    "df_conferences_to_insert = df_conferences_to_filter \\\n",
    "    .withColumn('location', df_conferences_to_filter['locations_array'][0]) \\\n",
    "    .select('venue', 'location', '_id')\n",
    "\n",
    "# Adding the new column which is the id\n",
    "df_conferences = df_conferences_to_insert.withColumn('publication_id', xxhash64('venue'))\n",
    "\n",
    "# Adding the foreign key to the papers\n",
    "exploded_conferences = df_conferences.select(explode('_id'), 'publication_id')\n",
    "#exploded_conferences.show(truncate = False)\n",
    "\n",
    "df_papers_in_conferences = exploded_conferences.join(df_paper, exploded_conferences.col == df_paper.paper_id)\n",
    "df_papers_in_conferences = df_papers_in_conferences.drop('col')\n",
    "\n",
    "df_conferences = df_conferences.drop(df_conferences._id)\n",
    "\n",
    "# Visualizing the data\n",
    "#print('Papers')\n",
    "#df_papers_in_conferences.show(truncate = False)\n",
    "print('Schema of the conferences')\n",
    "df_conferences.printSchema()\n",
    "print('Conferences')\n",
    "df_conferences.show(truncate=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Papers schema\n",
      "root\n",
      " |-- publication_id: long (nullable = false)\n",
      " |-- paper_id: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- keywords: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- fos: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- references: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- page_start: integer (nullable = true)\n",
      " |-- page_end: integer (nullable = true)\n",
      " |-- lang: string (nullable = true)\n",
      " |-- abstract: string (nullable = true)\n",
      " |-- publication_type: string (nullable = true)\n",
      " |-- date: timestamp (nullable = true)\n",
      " |-- doi: string (nullable = true)\n",
      " |-- url: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n",
      "Papers data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 19:>                 (0 + 1) / 1][Stage 21:>                 (0 + 0) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------------+-------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------+--------+----+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------+-------------------+-----------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|publication_id      |paper_id                |title                                                                    |keywords                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |fos                                                                                                                                                                                                                                                                                                                            |references                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |page_start|page_end|lang|abstract                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |publication_type|date               |doi                          |url                                                                                                                                                                                                                                      |\n",
      "+--------------------+------------------------+-------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------+--------+----+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------+-------------------+-----------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|-4336127124407324726|53e997d1b7602d9701fc5024|A fault diagnosis methodology for the UltraSPARC/sup TM/-I microprocessor|[stuck at defects, stuck at fault, computational effort, excellent correlation, predicted errors, dictionary format, memory size, precomputed fault dictionary, fault diagnosis methodology, diagnostic resolution, precomputed fault dictionaries, sup tm, layout, novel procedure, failure mechanisms, ultrasparc i microprocessor, bridging defects, actual defect, computer testing, dictionary information, sun, testing, failure analysis, design methodology, prediction error, cmos technology, computational modeling, dictionaries]|[computer testing, logic testing, bridging networking, microprocessor, design methods, cmos, engineering, embedded system, ultrasparc]                                                                                                                                                                                         |[53e9bca6b7602d97049270e3, 53e99a91b7602d9702306177, 53e9babab7602d97046ebaef, 557d891cf6678c77ea21b183, 53e99af2b7602d970237ba75, 53e9a54eb7602d9702e715fd, 53e9ad68b7602d970374edbc, 53e9b5dab7602d970412710b]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |494       |494     |en  |In this paper we study the use of precomputed fault dictionaries to diagnose stuck-at and bridging defects in the UltraSPARC/sup TM/-I processor. In constructing the dictionary we analyze the effect of the dictionary format on parameters such as memory size, computational effort, and diagnostic resolution. The dictionary is built based on modeled stuck-at faults. However to effectively diagnose both stuck-at and bridging faults, we employ a novel procedure that combines dictionary information with potential bridge defects extracted from layout. Experiments with failing devices show excellent correlation of predicted errors with actual defects.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |Book            |1950-09-08 12:23:35|10.1109/EDTC.1997.582406     |[http://dx.doi.org/10.1109/EDTC.1997.582406]                                                                                                                                                                                             |\n",
      "|4674035537543784623 |53e997e4b7602d9701fda80a|Problem Decomposition and the Learning of Skills                         |[problem decomposition, human performance, machine learning, divide and conquer]                                                                                                                                                                                                                                                                                                                                                                                                                                                             |[inductive logic programming, a domain, constructive induction, computer science, artificial intelligence, divide and conquer algorithms, solver, hierarchy, combinatorial explosion]                                                                                                                                          |[53e9a690b7602d9702fc076a, 53e9a099b7602d97029835d1, 53e9b6d1b7602d970425d7d6, 53e9a893b7602d97031df74f, 53e9ab89b7602d970353578d, 558ad1cbe4b0b32fcb390ecd, 53e9aaa9b7602d9703427a0a, 53e9bba1b7602d97047e9fe1, 53e9983db7602d9702068e05, 53e9aeb1b7602d97038d6792, 573696736e3b12023e57f7ec]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |17        |31      |en  |One dimension of divide and conquer in problem solving concerns the domain and its subdomains. Humans learn the general structure of a domain while solving particular learning problems in it. Another dimension concerns the solver's goals and subgoals. Finding good decompositions is a major AI tactic both for defusing the combinatorial explosion and for ensuring a transparent end-product. In machine learning, pre-occupation with free-standing performance has led to comparative neglect of this resource, illustrated under the following headings. 1. Automatic manufacture of new attributes from primitives (constructive induction). 2. Machine learning within goal-subgoal hierarchies (structured induction). 3. Reconstruction of skills from human performance data (behavioural cloning).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |Book            |1986-12-29 07:27:28|10.1007/3-540-59286-5_46     |[http://dx.doi.org/10.1007/3-540-59286-5_46, http://www.webofknowledge.com/]                                                                                                                                                             |\n",
      "|1144530160964015551 |53e997e8b7602d9701fe0ddc|X-tract: Structure Extraction from Botanical Textual Descriptions        |[information retrieval, botanical textual descriptions, botanical document, information structure, available information, structure extraction, botanical digital library, retrieving information, proposed structure, digital repository, botanical textual description, databases, botany, prototypes, data mining, html, digital libraries, text analysis, grammar, digital library, taxonomy]                                                                                                                                            |[information structure, information retrieval, structure extraction, computer science, grammar, plain text, natural language processing, artificial intelligence, application domain, digital library, vocabulary]                                                                                                             |[53e9b0d8b7602d9703b52665, 53e9be44b7602d9704b0413d, 53e9ab9eb7602d970354c65a, 53e9a52bb7602d9702e4b728, 53e9ac3db7602d970360843b]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |2         |2       |en  |Most available information today, both from printed books and digital repositories, is in the form of free-format texts. The task of retrieving information from these ever-growing repositories has become a challenge for information retrieval (IR) researchers. In some fields, such as Botany and Taxonomy, textual descriptions observe a set of rules and use a relatively limited vocabulary. This makes botanical textual descriptions an interesting area to explore IR techniques for finding structure and facilitating semantic analysis.This paper presents X-tract, a solution to the problem of text analysis and structure extraction in a specific application domain, namely floristic morphologic descriptions. The solution demonstrates the potential of using a grammar in the determination of information structure in a botanical digital library. We have developed a prototype based on this approach in which given an HTML or plain text, X-tract analyzes it and presents results to the user so he or she can verify the proposed structure before updating the database. This transformation is useful also in the process of storing morphologic descriptions in a database with a pre-established format. The solution is implemented in the context of the Floristic Digital Library (FDL), a large digital library project comprising a wide variety of botanical documents, formats and services.                                                                                                                                                                                                                                                                           |Book            |1954-04-24 05:44:09|10.1109/SPIRE.1999.796571    |[http://computer.org/proceedings/spire/0268/02680002abs.htm, http://dx.doi.org/10.1109/SPIRE.1999.796571]                                                                                                                                |\n",
      "|-6465152039831803131|53e997e8b7602d9701fe213d|Cognitive agent programming                                              |[dynamic logic, agent programming languages, denotational semantics, cognitive agent programming, plan revision, operational semantics, declarative goals]                                                                                                                                                                                                                                                                                                                                                                                   |[functional logic programming, procedural programming, programming language, computer science, inductive programming, first generation programming language, declarative programming, programming domain, programming language theory, well founded semantics]                                                                 |[53e9b38fb7602d9703e728af, 53e9a5a1b7602d9702ec76c0, 53e9a611b7602d9702f403c1, 53e9a8f9b7602d9703249628]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |1385      |1385    |en  |This is a summary of the thesis entitled \"Cognitive Agent Programming: Semantics and Logics\".                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |Book            |1992-05-16 17:49:48|10.1145/1082473.1082793      |[http://dx.doi.org/10.1145/1082473.1082793, http://doi.acm.org/10.1145/1082473.1082793]                                                                                                                                                  |\n",
      "|5112790730981969721 |53e997e9b7602d9701fe3ba8|Constraint based vectorization                                           |[uniform framework, unconstrained loop, constraint tree, parallel decomposition, memory access, loop transformation, alternative execution method, parallelization, reduction, vectorization, induction variable]                                                                                                                                                                                                                                                                                                                            |[computer science, parallel computing, induction variable, vectorization mathematics]                                                                                                                                                                                                                                          |[53e9b326b7602d9703df1a9b, 53e9a0d2b7602d97029bc5aa, 53e9b693b7602d97042038d0, 53e9b716b7602d97042ac87a, 53e99a52b7602d97022baad8, 53e9a3dbb7602d9702cf11f4, 53e9ab13b7602d9703498238]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |195       |204     |en  |The constraint tree provides a uniform framework for representing many loop transformations. It allows us to estimate the performance of several alternative execution methods before committing to any of the transformations.We introduce the constraint tree, show how it is built, and demonstrate its use for vectorization and parallel decomposition. We show how unconstrained loops can be moved to reduce the costs of memory accesses.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |Book            |1978-02-04 19:19:02|10.1145/318789.318811        |[http://dx.doi.org/10.1145/318789.318811, http://doi.acm.org/10.1145/318789.318811]                                                                                                                                                      |\n",
      "|1369111316993402587 |53e997ecb7602d9701fe6ad0|Automatic input rectification                                            |[google picasa, dangerous input, swfdec, image processing, atypical input, malicious input, manual code analysis, subjective perceptual quality, internet, original input, dillo, amazon mechanical turk users, typical input, benign input, imagemagick, subjective qualitative perception, benchmark application, input rectifier, automatic input rectification, vlc, security of data, soap, engines, simple object access protocol, security]                                                                                           |[novel technique, static program analysis, data mining, rectifier, rectification, simulation, computer science, image processing, exploit, soap, the internet]                                                                                                                                                                 |[53e9a29db7602d9702ba68eb, 53e99a52b7602d97022b691d, 53e9a1c9b7602d9702ac27ee, 557cd4e8f667eeed56193517, 53e9a691b7602d9702fc6bf2, 558a95a0e4b0b32fcb37acb2, 53e99938b7602d9702179580, 53e99b56b7602d97023f8290, 53e9a0a6b7602d970298e7aa, 53e9b862b7602d970442d1e8, 5ea818689fced0a24b65ab62, 53e9a6e6b7602d970301b415, 53e9a114b7602d9702a04ad6, 53e99b86b7602d970242ecbe, 53e9b029b7602d9703a83631, 53e9b3c1b7602d9703ea9bb3, 53e9b4a5b7602d9703fb3e92, 53e99cdfb7602d97025919af, 53e99b7fb7602d9702426b8e, 53e9afa6b7602d97039f6675, 53e9baf6b7602d970472b18c, 53e9ae28b7602d970383a1b2, 558b29fce4b0b32fcb3b1741, 53e99fddb7602d97028bc25c, 53e99d88b7602d9702646de6, 53e9a7ddb7602d970311c843, 53e9b86db7602d970443976d, 53e9a618b7602d9702f49187, 53e9bb9ab7602d97047df71c, 53e9a10eb7602d97029fd5e4, 53e9b002b7602d9703a5b6c2, 53e9abb9b7602d9703567b95, 53e99a4eb7602d97022b3f6e]|80        |90      |en  |We present a novel technique, automatic input rectification, and a prototype implementation, SOAP. SOAP learns a set of constraints characterizing typical inputs that an application is highly likely to process correctly. When given an atypical input that does not satisfy these constraints, SOAP automatically rectifies the input (i.e., changes the input so that it satisfies the learned constraints). The goal is to automatically convert potentially dangerous inputs into typical inputs that the program is highly likely to process correctly. Our experimental results show that, for a set of benchmark applications (Google Picasa, ImageMagick, VLC, Swfdec, and Dillo), this approach effectively converts malicious inputs (which successfully exploit vulnerabilities in the application) into benign inputs that the application processes correctly. Moreover, a manual code analysis shows that, if an input does satisfy the learned constraints, it is incapable of exploiting these vulnerabilities. We also present the results of a user study designed to evaluate the subjective perceptual quality of outputs from benign but atypical inputs that have been automatically rectified by SOAP to conform to the learned constraints. Specifically, we obtained benign inputs that violate learned constraints, used our input rectifier to obtain rectified inputs, then paid Amazon Mechanical Turk users to provide their subjective qualitative perception of the difference between the outputs from the original and rectified inputs. The results indicate that rectification can often preserve much, and in many cases all, of the desirable data in the original input.|Book            |1971-08-25 11:47:59|10.1109/ICSE.2012.6227204    |[http://dx.doi.org/10.1109/ICSE.2012.6227204, http://ieeexplore.ieee.org/xpl/abstractAuthors.jsp?tp=&arnumber=6227204]                                                                                                                   |\n",
      "|1430384888819899512 |53e997ecb7602d9701fe88aa|Acceptability-oriented computing                                         |[monitoring, acceptability properties, repair, rectification, languages, design, security, reliability]                                                                                                                                                                                                                                                                                                                                                                                                                                      |[programming language, software engineering, computer science, software system]                                                                                                                                                                                                                                                |[53e9badfb7602d9704710310, 53e9a55cb7602d9702e83e77, 53e997f1b7602d9701ff150a, 53e9995ab7602d970219d805, 53e997f1b7602d9701ff150f, 53e99bdcb7602d9702487b93, 53e9ad3bb7602d9703721e6d, 557d0b206feeaa8086da59f5, 53e9b587b7602d97040c828b, 53e9a7feb7602d9703141ae3, 557d2339f667eeed56198259, 53e9ba32b7602d970463fc8b, 53e9a824b7602d970316ca2d, 53e9a87eb7602d97031ca2ea, 53e99b0ab7602d970239a426, 53e9b4c4b7602d9703fe184d]                                                                                                                                                                                                                                                                                                                                                                                                                                                          |57        |75      |en  |We discuss a new approach to the construction of software systems. Instead of attempting to build a system that is as free of errors as possible, the designer instead identifies key properties that the execution must satisfy to be acceptable to its users. Together, these properties define the acceptability envelope of the system: the region that it must stay within to remain acceptable. The developer then augments the system with a layered set of components, each of which enforces one of the acceptability properties. The potential advantages of this approach include more flexible, resilient systems that recover from errors and behave acceptably across a wide range of operating environments, an appropriately prioritized investment of engineering resources, and the ability to productively incorporate unreliable components into the final software system.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |Book            |1964-08-09 22:48:00|10.1145/966051.966060        |[http://dx.doi.org/10.1145/966051.966060, http://doi.acm.org/10.1145/949344.949402, https://static.aminer.org/pdf/20170130/pdfs/index.txt]                                                                                               |\n",
      "|710748220667054414  |53e997ecb7602d9701fe8cf5|Anomalous Neighborhood Selection                                         |[column wise difference, proposed regularization term, direction method, column wise heterogeneous element, group wise regularization, anomaly localization simulation, convex optimization problem, anomalous neighborhood selection, anomaly localization, proposed algorithm, gaussian processes, data analysis, feature extraction, convex programming]                                                                                                                                                                                  |[mathematical optimization, matrix mathematics, matrix algebra, real world data, algorithm, feature extraction, regularization mathematics, gaussian process, convex optimization, mathematics]                                                                                                                                |[53e9a0a6b7602d970298fcb7, 53e9b8cdb7602d97044ae570, 53e99e1ab7602d97026dae48, 53e9b93eb7602d9704529b9b, 53e9a29eb7602d9702ba7615, 53e9a689b7602d9702fbd442, 53e9bb37b7602d9704777dcc, 53e9b45eb7602d9703f5b9c3, 53e9a718b7602d970304e382, 558acf35e4b0b32fcb38fe96, 53e9bb15b7602d970474d8ba, 53e9a720b7602d9703057b4e, 53e9b6d0b7602d970425aa2b]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |474       |480     |en  |We propose a method to extract row/column-wise heterogeneous elements between two precision matrices for an anomaly localization. We formulate the task as a convex optimization problem using a regularization term that penalizes row/column-wise differences between two matrices. The fundamental difficulties of the problem are that the proposed regularization term (1) is a sum of group-wise regularizations with overlapping supports between the groups, (2) penalizes matrices in a symmetric manner. Our proposed algorithm with an alternating direction method of multipliers can deal with these two difficulties efficiently resulting in a very simple formulation with each updating step computed analytically. We also show the validity of the proposed method through an anomaly localization simulation using a real world data.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |Book            |1996-03-16 14:37:00|10.1109/ICDMW.2012.10        |[http://dx.doi.org/10.1109/ICDMW.2012.10, http://doi.ieeecomputersociety.org/10.1109/ICDMW.2012.10, http://ieeexplore.ieee.org/xpl/abstractAuthors.jsp?tp=&arnumber=6406477]                                                             |\n",
      "|8148044123726554362 |53e997f1b7602d9701feeaf7|A Digital Watermark.                                                     |[authentication, authorisation, security, decoding, image processing, lsb, digital watermark]                                                                                                                                                                                                                                                                                                                                                                                                                                                |[computer vision, digital watermarking, authentication, bit plane, computer science, image processing, watermark, jpeg, artificial intelligence, digital image processing, least significant bit]                                                                                                                              |[53e99822b7602d9702042bd8]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |86        |90      |en  |This paper discusses the feasibility of coding an \"undetectable \" digital water mark on a standard 512*512 intensity image with an 8 bit gray scale. The watermark is capable of carrying such information as authenticatio n or authorisation codes, or a legend essential for image interpretation. This capability is envisaged to find application in image tagging, copyright enforcement, counterfeit protection, and controlled access. Two methods of implementation are discussed. The first is based on bit plane manipulation of the LSB, which offers easy and rapid decoding. The second method utilises linear addition of the water mark to the image data, and is more difficult to decode, offering inherent security. This linearity property also allows some image processing, such as averaging, to take place on the image, without corrupting the water mark beyond recovery. Either method is potentially compatible with JPEG and MPEG processing.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |Book            |2001-12-04 20:38:24|10.1109/ICIP.1994.413536     |[http://dx.doi.org/10.1109/ICIP.1994.413536, http://ieeexplore.ieee.org/xpl/abstractAuthors.jsp?tp=&arnumber=413536]                                                                                                                     |\n",
      "|-8250923667053511275|53e997f4b7602d9701ff6d84|Independent Tree Spanners                                                |[independent tree spanners, linear time, spanning tree, fault tolerant]                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |[combinatorics, minimum degree spanning tree, disjoint sets, vertex geometry, k ary tree, root vertex, spanning tree, time complexity, mathematics, minimum spanning tree]                                                                                                                                                     |[53e9b2f5b7602d9703db4164, 53e9984fb7602d970208660e, 53e99bcdb7602d9702478869, 53e99b21b7602d97023bada1, 53e99924b7602d9702160f82, 53e99a91b7602d9702306916, 53e99b3cb7602d97023de207, 53e9ba38b7602d970464665d]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |203       |214     |en  |\\n For any fixed parameter t ≥ 1, a tree t-spanner of a graph G is a spanning tree T of G such that the distance between every pair of vertices in T is at most t times their distance in G. In this paper, we incorporate a concept of fault-tolerance by examining independent tree t-spanners. Given a root vertex r, this is a pair of tree t-spanners, such that the two paths from any vertex to r are edge (resp., internally vertex) disjoint. It is shown that a pair of independent tree 2-spanners can be found in linear\\n time, whereas the problem for arbitrary t ≥ 4 is NP\\cal NP-complete.\\n \\n \\n As a less restrictive concept, we treat tree t-root-spanners, where the distance constraint is relaxed. Here, we show that the problem of finding an independent pair of such subgraphs is NP\\cal NP-complete for all t. As a special case, we then consider direct tree t-root-spanners. These are tree t-root-spanners where paths from any vertex to the root have to be detour-free. In the edge independent case, a pair of these can be found in linear time for all t, whereas the vertex independent case remains NP\\cal NP-complete.\\n \\n \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |Book            |2014-02-26 04:06:46|10.1007/10692760_17          |[http://dx.doi.org/10.1007/10692760_17]                                                                                                                                                                                                  |\n",
      "|8160813231728112221 |53e997f5b7602d9701ff880b|Inverting onto functions                                                 |[nondeterministic polynomial time, polynomial time computable refinement, polynomial time computable, total multivalued nondeterministic function, nondeterministic turing machine, important complexity statement, satisfying assignment, polynomial time computable inverse, polynomial time, polynomial time computable function, inverting onto functions, entire inverse, robustness, satisfiability, turing machines, computational complexity, polynomials, complexity, sat, computer science, telecommunications]                    |[computer science, turing reduction, pspace, up, time complexity, time hierarchy theorem, computable function, discrete mathematics, combinatorics, algebra, nondeterministic algorithm, turing machine, alternating turing machine, non deterministic turing machine, computable number, mathematics, np, computable analysis]|[53e9b4afb7602d9703fc1403, 53e9b910b7602d97044f4b9f, 53e9b381b7602d9703e6476a, 53e9aa9bb7602d9703412a65, 53e9a440b7602d9702d5c5a6, 53e9b4fab7602d9704029467, 53e9bc9eb7602d970491d8ca, 53e9b5bcb7602d9704105cc9, 53e9a309b7602d9702c155d5, 53e9a8eab7602d970323c278, 5ea818689fced0a24b65ab62, 557cd9646feeaa8086da2863, 53e9a660b7602d9702f92fc2, 53e9bd4bb7602d97049dc92d, 5c8c43194895d9cbc6d6e0c8, 53e99d0cb7602d97025c03ee, 5ccafd246558b90bfa945e4d, 53e9b0abb7602d9703b19818, 53e99df7b7602d97026bafae, 53e9ad05b7602d97036e49d6, 53e99df7b7602d97026bafae, 53e9be09b7602d9704abe268, 53e9bbd4b7602d97048240f2, 56d8fed1dabfae2eeec9f586, 53e9a635b7602d9702f631c6]                                                                                                                                                                                                                |213       |222     |en  |We look at the hypothesis that all honest onto polynomial-time computable functions have a polynomial-time computable inverse. We show this hypothesis equivalent to several other complexity conjectures including: • In polynomial time, one can find accepting paths of nondeterministic polynomial-time Turing machines that accept Σ*. • Every total multivalued nondeterministic function has a polynomial-time computable refinement. • In polynomial time, one can compute satisfying assignments for any polynomial-time computable set of satisfiable formulae. • In polynomial time, one can convert the accepting computations of any nondeterministic Turing machine that accepts SAT to satisfying assignments.We compare these hypotheses with several other important complexity statements. We also examine the complexity of these statements where we only require a single bit instead of the entire inverse.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |Book            |2015-12-31 07:56:33|10.1016/S0890-5401(03)00119-6|[http://dx.doi.org/10.1016/S0890-5401(03)00119-6, http://dx.doi.org/10.1109/CCC.1996.507683, http://ieeexplore.ieee.org/xpl/abstractAuthors.jsp?tp=&arnumber=507683, https://www.sciencedirect.com/science/article/pii/S0890540103001196]|\n",
      "|6712300784079919943 |53e9980eb7602d9702025478|EVES: An Overview                                                        |[never, formal methods, logic of programs, verdi, automated deduction, eves, formal method]                                                                                                                                                                                                                                                                                                                                                                                                                                                  |[programming language, automated theorem proving, formal methods]                                                                                                                                                                                                                                                              |[53e9a2bab7602d9702bc22cc, 53e9a848b7602d970319254b, 53e99cbbb7602d970256c2fa, 53e99846b7602d97020769ae]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |389       |405     |en  |In this paper we describe a new formal methods tool called EVES. EVES consists of a set theoretic language, called Verdi, and an automated deduction system, called NEVER. We present an overview of Verdi, NEVER, and the underlying mathematics; and develop a small program, to demonstrate the basic functionality of EVES.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |Book            |1972-08-07 17:35:34|10.1007/3-540-54834-3_24     |[http://dx.doi.org/10.1007/3-540-54834-3_24]                                                                                                                                                                                             |\n",
      "|-1585425199704921662|53e99813b7602d9702029222|Distributed probability network                                          |[error correction, belief network representation, sub pattern joint probability distribution, sub pattern space binary associative, associative architecture, error correction problem, pattern completion, probability network, belief network, associative memory, proposed architecture, fault tolerant, neural nets, distributed architecture, probability, neural network, artificial neural networks, domain knowledge, degradation, probability distribution, computer architecture, fault tolerance, information retrieval]          |[binary pattern, associative property, content addressable memory, computer science, bidirectional associative memory, distributed memory, theoretical computer science, bayesian network, memory map, distributed shared memory]                                                                                              |[53e99a67b7602d97022d5927, 557d4146f667eeed56199e3c, 557d42c36feeaa8086da8e3e]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |3398      |3401    |en  |This paper introduces a distributed sub-pattern space binary associative memory for error correction and pattern completion. The associative architecture presented in this work is analogous to belief network representations in that, it represents domain knowledge using a set of sub-pattern joint probability distributions. It is superior to belief networks in that, it can be used as an associative memory. The proposed architecture is distributed in nature and offers a fault tolerant solution to binary pattern completion and error correction problems.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |Book            |2016-08-28 14:12:19|10.1109/ICASSP.2000.860130   |[http://dx.doi.org/10.1109/ICASSP.2000.860130, http://doi.ieeecomputersociety.org/10.1109/ICASSP.2000.860130, db/conf/icassp/icassp2000.html#Ozturk00, https://doi.org/10.1109/ICASSP.2000.860130]                                       |\n",
      "|6134863903676584956 |53e99818b7602d970202fd4b|Regulations by Valences                                                  |[power method, normal form]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |[context sensitive grammar, regulated rewriting, context free language, context free grammar, algebra, computer science, phrase structure grammar, algorithm, monoid, chomsky normal form, matrix grammar]                                                                                                                     |[53e9a946b7602d9703299707, 53e9a8b0b7602d97031ff474, 53e9a937b7602d9703288948, 53e9a885b7602d97031d47de, 53e999b4b7602d97021f6a6a, 53e9a8e3b7602d970323154d, 56d849d2dabfae2eeecbea2e, 53e99a26b7602d970227c90e]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |239       |248     |en  | . Valences are a very simple and yet powerful method of regulatedrewriting. In this paper we give an overview on different aspectsof this subject. We discuss closure properties of valence languages. It isshown that matrix grammars can be simulated by valence grammars overfinite monoids. A Chomsky normal form theorem is proved for multiplicativevalence grammars, thereby solving the open question of the existenceof normal forms for unordered vector grammars. This also gives an alternative...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |Book            |1961-02-26 08:04:12|10.1007/BFb0029967           |[http://dx.doi.org/10.1007/BFb0029967]                                                                                                                                                                                                   |\n",
      "|2530147181524079819 |53e99818b7602d9702031fca|Reflections on symmetry                                                  |[object oriented, symmetry, user interface design, statechart, visual design, affordance, user interface, object orientation]                                                                                                                                                                                                                                                                                                                                                                                                                |[communication design, computer science, beauty, implementation, human computer interaction, user interface design, user interface, affordance, human action cycle, interface metaphor]                                                                                                                                        |[53e99832b7602d9702056f23, 53e997ecb7602d9701fec225, 53e99a1fb7602d9702273fd3, 53e9b2a3b7602d9703d4bf7e, 53e99a43b7602d97022a4005, 573697e76e3b12023e6c463d]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |28        |33      |en  |Symmetry is routinely used in visual design, but in fact is not just a visual concept. This paper explores how deeper symmetries in user interface implementations can be 'reflected' in the design of the user interface, and make them easier to use. This deeper application of symmetry for user interface design is related to affordance, and therefore makes that concept constructively applicable. Recommendations for programming better user interfaces are suggested. \"Symmetry, as wide or as narrow as you may define its meaning, is one idea by which man through the ages has tried to comprehend and create order, beauty, and perfection.\" Hermann Weyl [16]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |Book            |2009-10-03 17:07:24|10.1145/1556262.1556265      |[http://dx.doi.org/10.1145/1556262.1556265, http://doi.acm.org/10.1145/1556262.1556265]                                                                                                                                                  |\n",
      "|2735482298684145043 |53e99827b7602d9702045ccd|Timestamping after commit                                                |[committed transaction entry, main problem, transaction consistent picture, recovery detail, past state, committed transaction identifiers, relational databases, voting, application software, concurrent computing, transaction processing, recovery, distributed databases, distributed transactions, concurrency control, computer science, logging, data analysis]                                                                                                                                                                      |[transaction processing, compensating transaction, three phase commit protocol, two phase commit protocol, computer science, commit, x open xa, nested transaction, distributed transaction, database]                                                                                                                         |[53e9b587b7602d97040c828b, 53e9bb08b7602d9704743ba1, 53e9af2db7602d9703965e9b, 599c7c24601a182cd278d0f2, 53e9b699b7602d970420c9d7, 53e9bb72b7602d97047b7ff1, 53e9ab3db7602d97034cae4a]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |160       |167     |en  |Many applications need transaction-consistent pictures of past states of the database. These applications use the commit time of the transaction to timestamp the data. When a transaction is distributed, the cohorts must vote on a commit time and the coordinator must choose a commit time based on the votes of the cohorts. This implies that timestamps are applied after commit.Until the timestamps are on all the records, one must keep a table of all the committed transaction identifiers and their commit times. The main problem solved is that of determining when all timestamps corresponding to a given transaction have been placed in the records so that a committed transaction entry can be erased from the table. This information must be stable. Logging and recovery details are included.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |Book            |1951-11-27 17:50:37|10.1109/PDIS.1994.331720     |[http://dx.doi.org/10.1109/PDIS.1994.331720, http://ieeexplore.ieee.org/xpl/abstractAuthors.jsp?tp=&arnumber=331720]                                                                                                                     |\n",
      "|5744511372260825900 |53e99827b7602d9702046f11|The Animachine renderer                                                  |[useful effect, various result, three dimensional animation, animachine renderer, multi layer animated graphics, computer based help, animachine project, movement aspect, poor relation, two dimensional animation, whole production process, production process, computer animation, production, image quality, animation, head, computer graphics, three dimensional, tv]                                                                                                                                                                 |[3d computer graphics, computer graphics lighting, interactive skeleton driven simulation, computer science, non photorealistic rendering, computer facial animation, animation, skeletal animation, computer animation, multimedia]                                                                                           |[53e9ab2bb7602d97034b6cd1, 53e9b0b2b7602d9703b1f740, 56d92e8ddabfae2eeeeb9f8b, 53e9af1fb7602d9703957459, 53e9a92ab7602d970327b6e8, 53e9b1b6b7602d9703c472bf, 53e9ab42b7602d97034d3b62, 557f5ab4d19faf961d17095d, 53e9a3e7b7602d9702cfc19f]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |90        |90      |en  |Two-dimensional animation has generally been the poor relation of three-dimensional animation, as far as computer-assistance is concerned. The Animachine project is an attempt to view the whole production process of 2D (cel) animation as ripe for computer-based help, while still allowing artists to retain as much of their traditional way of working as possible. In this paper, we summarize our progress on a renderer for such multi-layer animated graphics. There is little emphasis on the movement aspects of our work. We discuss the various results which can be achieved without recourse to full 3D techniques, we show that the approach is versatile and can be used to achieve a wide range of useful effects.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |Book            |2007-10-16 04:48:40|10.1109/CA.1995.393543       |[http://doi.ieeecomputersociety.org/10.1109/CA.1995.393543, http://dx.doi.org/10.1109/CA.1995.393543]                                                                                                                                    |\n",
      "|2483812910487536561 |53e99827b7602d9702049897|The CD1 system.                                                          |[advanced constraint, card system, sql translation, error prone aspect, cardinality, graduate level class, er schema, case tool, schema import, advanced cardinality constraint, relational transla tion, triggers, keywords conceptual design, sql script generation, conceptual design phase, conceptual design, data model]                                                                                                                                                                                                               |[sql, conceptual design, data modeling, programming language, computer science, cardinality, computer aided software engineering, schema psychology, completeness statistics]                                                                                                                                                  |[53e99967b7602d97021abc47, 53e9adc2b7602d97037c6122, 53e998b0b7602d97020ee328, 53e9a46ab7602d9702d859cd]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |1         |9       |en  |\\n We describe a CASE tool (the CARD system) that allows users to represent and translate ER schemas, along with more advanced\\n cardinality constraints (such as participation, co-occurrence and projection [1]). The CARD system supports previous research\\n that proposes representing constraints at the conceptual design phase [1], and builds upon work presenting a framework for\\n establishing completeness of cardinality and the associated SQL translation [2]. From a teaching perspective, instructors\\n can choose to focus student efforts on data modeling and design, and leave the time-consuming and error-prone aspect of SQL\\n script generation to the CARD system. Graduate-level classes can take advantage of support for more advanced constraints.\\n \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |Book            |2013-03-31 03:16:37|10.1007/978-3-642-16373-9_31 |[http://dx.doi.org/10.1007/978-3-642-16373-9_31, http://www.ncbi.nlm.nih.gov/pubmed/1712128?report=xml&format=text]                                                                                                                      |\n",
      "|8942054755879642437 |53e9982cb7602d970204fd22|Public-Key Registration                                                  |[registration request, network terminal, intended registration, cryptographic facility, terminal identification, public key registration, cryptographic variable, public key, secret key pair, key distribution center, terminal initializer]                                                                                                                                                                                                                                                                                                |[fixed access, cryptography, computer security, computer science, initialization, key distribution center, public key cryptography]                                                                                                                                                                                            |[53e99a74b7602d97022e646b, 53e9a584b7602d9702eaa486, 53e9baecb7602d970471e0d1]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |451       |458     |en  |A procedure is described for securely initializing cryptographic variables in a large number of network terminals. Each terminal\\n has a cryptographic facility which performs all necessary cryptographic functions. A key distribution center is established,\\n and a public and secret key pair is generated for the key distribution center. Each terminal in the network is provided with\\n a terminal identification known to the key distribution center. The terminal identification and the public key of the key\\n distribution center are stored in the cryptographic facility of each terminal. A terminal initializer is designated for each\\n terminal, and the terminal initializer is notified of two expiration times for the purpose of registering the terminal’s cryptovariable\\n with the key distribution center. The cryptovariable is generated by the terminal using its cryptographic facility. Prior\\n to the first expiration time, a registration request is prepared and transmitted to the key distribution center. The registration\\n request includes the terminal identification and the cryptovariable. When the key distribution center receives this request,\\n the cryptovariable is temporarily registered and that fact is acknowledged to the requesting terminal. After the expiration\\n of the second time, the registration is complete. Provisions are also made for invalidating a terminal identification if more\\n than one registration is attempted for a given terminal identification or an intended registration was not made in time.\\n                                                                                                                     |Book            |1993-11-20 16:32:56|10.1007/3-540-47721-7_33     |[http://dx.doi.org/10.1007/3-540-47721-7_33]                                                                                                                                                                                             |\n",
      "|8730891231843712925 |53e9983db7602d9702065fc5|Learning Automata Teams                                                  |[blue fringe edsm algorithm, known result, polynomial characteristic, dfa learning, minimum dfa, certain condition, states algorithm, merges state, automata team, automata teams, different automata output]                                                                                                                                                                                                                                                                                                                                |[quantum finite automata, automata theory, learning automata, mobile automaton, growcut algorithm, algorithm, theoretical computer science, dfa minimization, trie, mathematics, ω automaton]                                                                                                                                  |[53e9afa6b7602d97039f35ac, 53e9a1d5b7602d9702ad133a, 53e9b762b7602d9704308d48, 53e9a8ffb7602d970324e524, 53e99da4b7602d9702660cc5, 5c78d2124895d9cbc6eef1d8, 53e9af81b7602d97039c7d21]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |52        |65      |en  |\\n We prove in this work that, under certain conditions, an algorithm that arbitrarily merges states in the prefix tree acceptor\\n of the sample in a consistent way, converges to the minimum DFA for the target language in the limit. This fact is used to learn automata teams, which use the different automata output\\n by this algorithm to classify the test. Experimental results show that the use of automata teams improve the best known results\\n for this type of algorithms. We also prove that the well known Blue-Fringe EDSM algorithm, which represents the state of art\\n in merging states algorithms, suffices a polynomial characteristic set to converge.\\n \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |Book            |1960-12-16 13:42:14|10.1007/978-3-642-15488-1_6  |[http://dx.doi.org/10.1007/978-3-642-15488-1_6, http://www.webofknowledge.com/]                                                                                                                                                          |\n",
      "+--------------------+------------------------+-------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------+--------+----+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------+-------------------+-----------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Merging the 3 dataframe which one contains the papers published in a specific media\n",
    "df_papers = df_papers_in_books \\\n",
    "    .union(df_papers_in_journals) \\\n",
    "    .union(df_papers_in_conferences)\n",
    "\n",
    "# Visualizing the data\n",
    "print('Papers schema')\n",
    "df_papers.printSchema()\n",
    "print('Papers data')\n",
    "df_papers.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Papers published in books\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----------------+--------------------+\n",
      "|            paper_id|               title|publication_type|      publication_id|\n",
      "+--------------------+--------------------+----------------+--------------------+\n",
      "|53e997d1b7602d970...|A fault diagnosis...|            Book|-4336127124407324726|\n",
      "|53e997e4b7602d970...|Problem Decomposi...|            Book| 4674035537543784623|\n",
      "|53e997e8b7602d970...|X-tract: Structur...|            Book| 1144530160964015551|\n",
      "|53e997e8b7602d970...|Cognitive agent p...|            Book|-6465152039831803131|\n",
      "|53e997e9b7602d970...|Constraint based ...|            Book| 5112790730981969721|\n",
      "|53e997ecb7602d970...|Automatic input r...|            Book| 1369111316993402587|\n",
      "|53e997ecb7602d970...|Acceptability-ori...|            Book| 1430384888819899512|\n",
      "|53e997ecb7602d970...|Anomalous Neighbo...|            Book|  710748220667054414|\n",
      "|53e997f1b7602d970...|A Digital Watermark.|            Book| 8148044123726554362|\n",
      "|53e997f4b7602d970...|Independent Tree ...|            Book|-8250923667053511275|\n",
      "|53e997f5b7602d970...|Inverting onto fu...|            Book| 8160813231728112221|\n",
      "|53e9980eb7602d970...|   EVES: An Overview|            Book| 6712300784079919943|\n",
      "|53e99813b7602d970...|Distributed proba...|            Book|-1585425199704921662|\n",
      "|53e99818b7602d970...|Regulations by Va...|            Book| 6134863903676584956|\n",
      "|53e99818b7602d970...|Reflections on sy...|            Book| 2530147181524079819|\n",
      "|53e99827b7602d970...|Timestamping afte...|            Book| 2735482298684145043|\n",
      "|53e99827b7602d970...|The Animachine re...|            Book| 5744511372260825900|\n",
      "|53e99827b7602d970...|     The CD1 system.|            Book| 2483812910487536561|\n",
      "|53e9982cb7602d970...|Public-Key Regist...|            Book| 8942054755879642437|\n",
      "|53e9983db7602d970...|Learning Automata...|            Book| 8730891231843712925|\n",
      "+--------------------+--------------------+----------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Papers published in conferences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----------------+--------------------+\n",
      "|            paper_id|               title|publication_type|      publication_id|\n",
      "+--------------------+--------------------+----------------+--------------------+\n",
      "|53e99854b7602d970...|  The EOLES project.|      Conference|  950373860555954453|\n",
      "|53e9989bb7602d970...|Life is engineeri...|      Conference|  950373860555954453|\n",
      "|53e998bfb7602d970...|Gaining and maint...|      Conference|  950373860555954453|\n",
      "|53e998c0b7602d970...|From manuals towa...|      Conference|  950373860555954453|\n",
      "|53e998e9b7602d970...|Learning with com...|      Conference|  950373860555954453|\n",
      "|53e99976b7602d970...|Cloud E-learning ...|      Conference|  950373860555954453|\n",
      "|53e9997eb7602d970...|Motivating progra...|      Conference|  950373860555954453|\n",
      "|53e99991b7602d970...|Monitoring studen...|      Conference|  950373860555954453|\n",
      "|53e99998b7602d970...|OLAREX project: O...|      Conference|  950373860555954453|\n",
      "|53e99859b7602d970...|Studying dynamic ...|      Conference|-4245717996156385657|\n",
      "|53e99860b7602d970...|Equivalence of br...|      Conference|-4245717996156385657|\n",
      "|53e998bfb7602d970...|Fish Schools: PDE...|      Conference| -762166203857654039|\n",
      "|53e9997eb7602d970...|Automatic Generat...|      Conference|-7124098142925130015|\n",
      "|53e998fdb7602d970...|Fault-Tolerant Sc...|      Conference| 7551243357793819285|\n",
      "|53e9990db7602d970...|Automatic Generat...|      Conference| 1334491883820369589|\n",
      "|53e99822b7602d970...|Stroboscopic Ster...|      Conference|-1039299792543803944|\n",
      "|53e998bfb7602d970...|Image-Based Techn...|      Conference|-1039299792543803944|\n",
      "|53e9991cb7602d970...|Scanning and Proc...|      Conference|-1039299792543803944|\n",
      "|53e99940b7602d970...|Range Image Regis...|      Conference|-1039299792543803944|\n",
      "|53e99937b7602d970...|Cage-Based Motion...|      Conference|-4577231382119670667|\n",
      "+--------------------+--------------------+----------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Papers published in journals\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+-----------------------------------------------------------------------------------------------------------+----------------+--------------------+\n",
      "|paper_id                |title                                                                                                      |publication_type|publication_id      |\n",
      "+------------------------+-----------------------------------------------------------------------------------------------------------+----------------+--------------------+\n",
      "|53e99915b7602d97021502bb|A note on robust 0-1 optimization with uncertain cost coefficients                                         |Journal         |-8668689828003778653|\n",
      "|53e99984b7602d97021c6a1c|A new approach for unsupervised classification                                                             |Journal         |-6055461944651662439|\n",
      "|53e998b0b7602d97020ebee5|Two-machine flow shop scheduling problems with minimal and maximal delays                                  |Journal         |4482587362010925183 |\n",
      "|53e9994cb7602d970218a5b5|Stochastic semidefinite programming: a new paradigm for stochastic optimization                            |Journal         |-5369333412851545957|\n",
      "|53e9990db7602d970214c179|Models and algorithms for the reconfiguration of distributed wireless switching systems                    |Journal         |4907811008184705540 |\n",
      "|53e99952b7602d9702190cba|Maximizing the minimum completion time on parallel machines                                                |Journal         |-7478992719207099405|\n",
      "|53e99800b7602d970200de91|Integer extended ABS algorithms and possible control of intermediate results for linear Diophantine systems|Journal         |-5578571051264951046|\n",
      "|53e99858b7602d970209388f|Attraction probabilities in variable neighborhood search                                                   |Journal         |3303577774132183121 |\n",
      "|53e9998bb7602d97021d0975|Strategy vs risk in margining portfolios of options                                                        |Journal         |-3839102222582550984|\n",
      "|53e99915b7602d970214eaa6|A necessary 4-cycle condition for dice representability of reciprocal relations.                           |Journal         |-8848711838540298032|\n",
      "|53e997f1b7602d9701ff00c4|A retargetable debugger                                                                                    |Journal         |5898809803706162287 |\n",
      "|53e997ddb7602d9701fd2cc7|Clustering with Local and Global Regularization                                                            |Journal         |226561117377407236  |\n",
      "|53e99858b7602d9702090f11|Weighted broadcast in linear radio networks                                                                |Journal         |-1537115915808707536|\n",
      "|53e997f4b7602d9701ff6953|Interactive topic modeling                                                                                 |Journal         |6922825622688887655 |\n",
      "|53e99991b7602d97021d34e9|Graphics Programming Using the Core System                                                                 |Journal         |-4352573166015971962|\n",
      "|53e99832b7602d9702055ead|Program Transformation Systems                                                                             |Journal         |7493946580737751003 |\n",
      "|53e9984bb7602d970207bddc|Voice response systems                                                                                     |Journal         |7340232518232887060 |\n",
      "|53e9982cb7602d970204e106|Parallel graph algorithms                                                                                  |Journal         |-7713800651548746073|\n",
      "|53e99813b7602d970202d7d6|Dataflow machine architecture                                                                              |Journal         |-7000780314495031015|\n",
      "|53e997d1b7602d9701fc9123|Can there be a science of information?                                                                     |Journal         |-7255461002200856409|\n",
      "+------------------------+-----------------------------------------------------------------------------------------------------------+----------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For checking the result\n",
    "print('Papers published in books')\n",
    "df_papers \\\n",
    "    .filter(col('publication_type') == 'Book') \\\n",
    "    .select('paper_id', 'title', 'publication_type', 'publication_id') \\\n",
    "    .show()\n",
    "print('Papers published in conferences')\n",
    "df_papers \\\n",
    "    .filter(col('publication_type') == 'Conference') \\\n",
    "    .select('paper_id', 'title', 'publication_type', 'publication_id') \\\n",
    "    .show()\n",
    "\n",
    "\n",
    "print('Papers published in journals')\n",
    "df_papers \\\n",
    "    .filter(col('publication_type') == 'Journal') \\\n",
    "    .select('paper_id', 'title', 'publication_type', 'publication_id') \\\n",
    "    .show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COMMANDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 86:>                 (0 + 1) / 1][Stage 88:>                 (0 + 0) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0--------------------------------\n",
      " publication_id   | 6935858940620736824  \n",
      " paper_id         | 53e9999eb7602d970... \n",
      " title            | Performance evalu... \n",
      " keywords         | [system level per... \n",
      " fos              | [reconfigurabilit... \n",
      " references       | [53e9b47cb7602d97... \n",
      " page_start       | 1656                 \n",
      " page_end         | 1667                 \n",
      " lang             | en                   \n",
      " doi              | 10.1155/ASP.2005.... \n",
      " url              | [http://dx.doi.or... \n",
      " abstract         | A reconfigurable ... \n",
      " publication_type | Journal              \n",
      " date             | 1998-07-15 06:56:20  \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Command 1: Add a new row to the Paper DataFrame\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "new_paper_file = 'single_paper.json'\n",
    "\n",
    "new_paper = spark.read.options(**OPTIONS).json(new_paper_file, schemaPaper) \\\n",
    "    .withColumnRenamed(\"_id\", \"paper_id\")\n",
    "\n",
    "journal_schema = StructType(\n",
    "    [StructField('issn', StringType(), True),\n",
    "     StructField('publisher', StringType(), True),\n",
    "     StructField('venue', StringType(), True),\n",
    "     StructField('volume', IntegerType(), True),\n",
    "     StructField('issue', IntegerType(), True)])\n",
    "\n",
    "journal = spark.read.options(**OPTIONS).json(new_paper_file, journal_schema) \\\n",
    "    .withColumn(\"publication_id\", xxhash64('venue', 'volume', 'issue', 'issn'))\n",
    "\n",
    "foreign_key = journal.head(1)[0].asDict()['publication_id']\n",
    "\n",
    "if df_journals.filter(col('publication_id') == foreign_key).count() == 0:\n",
    "    df_jounrals = df_journals.union(journal)\n",
    "\n",
    "paper_id = new_paper.head(1)[0].asDict()['paper_id']\n",
    "\n",
    "# Inserting foreign_key at position 1 (0 for programmers)\n",
    "new_paper = new_paper.select(lit(foreign_key).alias('publication_id'), '*')\n",
    "\n",
    "if df_papers.filter(col('paper_id') == paper_id).collect() == []:\n",
    "    df_papers = df_papers.union(new_paper)\n",
    "\n",
    "df_papers.filter(col('paper_id') == paper_id).show(vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the entire initial database is 37626, the size of the current database is 37626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 284:>                (0 + 1) / 1][Stage 286:>                (0 + 0) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+-------------------------------------------------------+-------------------+-------------------------------------------------------+\n",
      "|                paper_id|                                                  title|                doi|                                                    url|\n",
      "+------------------------+-------------------------------------------------------+-------------------+-------------------------------------------------------+\n",
      "|53e997e4b7602d9701fdb48a|CitizenTalk: application of chatbot infotainment to ...|10.1007/11944577_37|[https://link.springer.com/chapter/10.1007/11944577_37]|\n",
      "+------------------------+-------------------------------------------------------+-------------------+-------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Update one single row of a dataframe (or multiple rows)\n",
    "# The command modifies the DOI and URL of a paper with identifier equal to '53e997e4b7602d9701fdb48a'.\n",
    "# For doing this operation we firstly filter the dataframe keeping only the row to be modified. We add a column for each value we want to insert under the name new_fieldName. Then we drop the old columns containing the previous values and we rename the new columns with the name of the old ones.\n",
    "#Finally, we make the union between the entire dataframe, without the row we want to modify, and the new entry.\n",
    "from pyspark.sql.functions import lit, array\n",
    "\n",
    "updated_df_papers = df_papers\\\n",
    "    .filter(col('paper_id') == '53e997e4b7602d9701fdb48a')\n",
    "updated_df_papers = updated_df_papers\\\n",
    "    .withColumn('new_doi', lit('10.1007/11944577_37'))\\\n",
    "    .withColumn('new_url', array([lit('https://link.springer.com/chapter/10.1007/11944577_37')]))\n",
    "updated_df_papers = updated_df_papers\\\n",
    "    .drop(col('doi')).drop(col('url'))\\\n",
    "    .withColumnRenamed('new_doi', 'doi')\\\n",
    "    .withColumnRenamed('new_url', 'url')\n",
    "\n",
    "# To check the number of the paper is the same as before\n",
    "updated_df_papers = df_papers.filter(col('paper_id') != '53e997e4b7602d9701fdb48a').union(updated_df_papers)\n",
    "print('The size of the entire initial database is ' + str(df_papers.count()) + ', the size of the current database is ' + str(updated_df_papers.count()))\n",
    "updated_df_papers.filter(col('paper_id') == '53e997e4b7602d9701fdb48a').select('paper_id', 'title', 'doi', 'url').show(truncate = 55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Command 3: remove an entire column\n",
    "df_papers_without_lang = df_papers \\\n",
    "    .drop('lang')\n",
    "\n",
    "df_papers_without_lang.printSchema()\n",
    "df_papers_without_lang.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Command 4: delete a group of rows\n",
    "\n",
    "from pyspark.sql.functions import year\n",
    "\n",
    "df_papers = df_papers \\\n",
    "    .filter(year('date') > '1950')\n",
    "\n",
    "df_papers.select('title', 'publication_type', 'date') \\\n",
    "    .orderBy('date') \\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Command 5: create a new column with the length of the paper (number of total pages)\n",
    "\n",
    "df_papers_total_pages = df_papers \\\n",
    "    .filter((col('page_start') >= 0) & (col('page_end') >= 0) & (col('page_start') <= col('page_end'))) \\\n",
    "    .withColumn('total_pages', col('page_end') - col('page_start'))\n",
    "\n",
    "df_papers_total_pages \\\n",
    "    .select(col('title'), col('page_start'), col('page_end'), col('total_pages')) \\\n",
    "    .show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QUERIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#Query 1: WHERE, JOIN\n",
    "\n",
    "venue, volume, issue = ('BMC Bioinformatics', '14', '1')\n",
    "\n",
    "df_papers_q1 = df_journals \\\n",
    "    .filter((col('venue') == venue) &\n",
    "            (col('volume') == volume) &\n",
    "            (col('issue') == issue)) \\\n",
    "    .join(df_papers,\n",
    "          (df_journals['publication_id'] == df_papers['publication_id']) &\n",
    "          (df_papers['publication_type'] == 'Journal'))\n",
    "\n",
    "df_papers_q1 \\\n",
    "    .select(['paper_id', 'title']) \\\n",
    "    .show(truncate=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------+-------------------+----------------------------------------+\n",
      "|                                             title|               date|                                 keyword|\n",
      "+--------------------------------------------------+-------------------+----------------------------------------+\n",
      "|An Ensemble Approach of Simple Regression Model...|1950-03-26 17:54:02|                 artificial intelligence|\n",
      "|Structure Learning of Bayesian Networks Based o...|1950-03-28 09:32:11|        learning artificial intelligence|\n",
      "|                         Robot social intelligence|1950-03-28 09:32:11|                 artificial intelligence|\n",
      "|A Hybrid Formal Theory of Plan Recognition and ...|1950-05-04 20:32:22|                  artificial intelligent|\n",
      "|Financial risk evaluation of chinese commercial...|1950-05-04 20:32:22|              artificial neural networks|\n",
      "|Backpropagation Algorithm for Logic Oriented Ne...|1950-07-09 17:07:10|               artificial neural network|\n",
      "|Backpropagation Algorithm for Logic Oriented Ne...|1950-07-09 17:07:10|              artificial neural networks|\n",
      "|Semantic Discovery of Grid Services Using Funct...|1950-08-15 10:07:02|      ontologies artificial intelligence|\n",
      "|Quadruped robot obstacle negotiation via reinfo...|1950-08-15 10:07:02|        learning artificial intelligence|\n",
      "|Self-Supervised Neural System for Reactive Navi...|1950-08-22 16:20:11|        learning artificial intelligence|\n",
      "|Self-Supervised Neural System for Reactive Navi...|1950-08-22 16:20:11|                artificial neural system|\n",
      "|                    Predictive Subspace Clustering|1950-08-30 13:14:15|        learning artificial intelligence|\n",
      "|CI in general game playing: to date achievement...|1950-09-07 23:16:44|classic artificial intelligence approach|\n",
      "|CI in general game playing: to date achievement...|1950-09-07 23:16:44|                  artificial intelligent|\n",
      "|Real-Time Warning System for Driver Drowsiness ...|1950-09-08 12:23:35|                 artificial intelligence|\n",
      "+--------------------------------------------------+-------------------+----------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query 2: WHERE, LIMIT, LIKE\n",
    "# Find the papers written in the last twenty years in english whose keywords have the word \\verb|artificial| inside the keywords. We require that these papers have the DOI set to a not null value.\n",
    "# The results are ordered ascending by the date and only 20 elements are printed.\n",
    "\n",
    "from pyspark.sql.functions import current_timestamp, unix_timestamp\n",
    "\n",
    "df = df_papers.withColumn('current time', current_timestamp())\n",
    "df \\\n",
    "    .filter((((unix_timestamp('current time') - unix_timestamp('date')) / 3600 / 24 / 365) > 50) &\n",
    "            (col('doi').isNotNull())) \\\n",
    "    .select('title',\n",
    "            'date',\n",
    "            explode('keywords').alias('keyword')) \\\n",
    "    .filter(col('keyword').like('%artificial%')) \\\n",
    "    .distinct() \\\n",
    "    .sort(col('date').asc()) \\\n",
    "    .limit(15) \\\n",
    "    .show(truncate=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Query 3: WHERE, IN, Nested Query\n",
    "# Show the papers collected in a book that have `multiagent system` as keyword.\n",
    "\n",
    "nested_query = df_papers \\\n",
    "    .select('title',\n",
    "            'publication_type',\n",
    "            'paper_id',\n",
    "            'publication_id',\n",
    "            'date',\n",
    "            explode('keywords').alias('keyword')) \\\n",
    "    .filter(col('keyword').isin('multiagent system')) \\\n",
    "    .drop('keyword')\n",
    "df_conferences \\\n",
    "    .join(nested_query, 'publication_id') \\\n",
    "    .filter(col('publication_type') == 'Conference') \\\n",
    "    .orderBy('date', ascending=False) \\\n",
    "    .select(col('title').alias('paper title'),\n",
    "            col('venue').alias('conference venue')) \\\n",
    "    .show(5, truncate=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Query 4: GROUP BY, JOIN, AS\n",
    "\n",
    "from pyspark.sql.functions import collect_set, size\n",
    "\n",
    "df_aff \\\n",
    "    .join(df_papers, df_papers.paper_id == df_aff.paper_id, 'inner') \\\n",
    "    .drop(df_papers.paper_id) \\\n",
    "    .select('paper_id',\n",
    "            'organization',\n",
    "            'publication_type') \\\n",
    "    .filter((col('organization').isNotNull()) &\n",
    "            (col('organization') != \"\") &\n",
    "            (col('publication_type') == \"Conference\")) \\\n",
    "    .groupBy('organization') \\\n",
    "    .agg(collect_set('paper_id').alias('papers')) \\\n",
    "    .filter(size(col('papers')) > 10) \\\n",
    "    .show(truncate=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Query 5: WHERE, GROUP BY\n",
    "# Retrieve some statistics about papers\n",
    "\n",
    "from pyspark.sql.functions import sum, min, max, avg, format_number, variance\n",
    "\n",
    "df_papers_total_pages \\\n",
    "    .filter(year(col('date')) >= 2015) \\\n",
    "    .groupBy(year(col('date')).alias('year')) \\\n",
    "    .agg(count('paper_id').alias('total_papers'),\n",
    "         sum('total_pages').alias('total_pages'),\n",
    "         min('total_pages').alias('min_pages'),\n",
    "         max('total_pages').alias('max_pages'),\n",
    "         format_number(avg('total_pages'), 2).alias('avg_pages'),\n",
    "         format_number(variance('total_pages'), 2).alias('var_pages')) \\\n",
    "    .sort(col('year').desc()) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Query 6: GROUP BY, HAVING, AS\n",
    "\n",
    "df_papers \\\n",
    "    .select('paper_id',\n",
    "            'title',\n",
    "            explode(col('references')).alias('reference')) \\\n",
    "    .groupBy('reference') \\\n",
    "    .agg(count('paper_id').alias('references_count')) \\\n",
    "    .filter(col('references_count') > 30) \\\n",
    "    .join(df_papers, col('reference') == df_papers.paper_id) \\\n",
    "    .sort(col('references_count').desc()) \\\n",
    "    .select(['title', 'references_count']) \\\n",
    "    .show(truncate=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 432:======================================>                  (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+--------------------------------+------------+\n",
      "|fos                            |keyword                         |couple count|\n",
      "+-------------------------------+--------------------------------+------------+\n",
      "|computer science               |internet                        |247         |\n",
      "|computer science               |data mining                     |242         |\n",
      "|computer science               |computer science                |226         |\n",
      "|computer science               |real time                       |189         |\n",
      "|computer science               |protocols                       |178         |\n",
      "|artificial intelligence        |feature extraction              |159         |\n",
      "|feature extraction             |feature extraction              |159         |\n",
      "|computer science               |quality of service              |156         |\n",
      "|computer science               |satisfiability                  |155         |\n",
      "|computer science               |feature extraction              |154         |\n",
      "|computer science               |real time systems               |146         |\n",
      "|the internet                   |internet                        |137         |\n",
      "|computer science               |computational modeling          |136         |\n",
      "|computer science               |learning artificial intelligence|132         |\n",
      "|computational complexity theory|computational complexity        |131         |\n",
      "+-------------------------------+--------------------------------+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "# Query 7: WHERE, GROUP BY, HAVING, AS\n",
    "# The query returns the association between fields of study and keywords which are more present within the database and how many times they appear together\n",
    "from pyspark.sql.functions import year, col, lit, size\n",
    "\n",
    "df = df_papers\n",
    "df = df.withColumn('count', lit(1))\n",
    "df = df \\\n",
    "    .filter(\n",
    "    (col('doi').isNotNull()) &\n",
    "    (year(col('date')) >= 2000) &\n",
    "    (size(col('fos')) > 0) &\n",
    "    (size(col('keywords')) > 0)) \\\n",
    "    .select('fos', 'count', explode('keywords').alias('keyword')) \\\n",
    "    .select('keyword', explode('fos').alias('fos'), 'count') \\\n",
    "    .groupby('fos', 'keyword') \\\n",
    "    .sum('count') \\\n",
    "    .withColumnRenamed('sum(count)', 'couple count') \\\n",
    "    .filter(col('couple count') > 100) \\\n",
    "    .sort(col('couple count').desc()) \\\n",
    "    .limit(15) \\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Query 8: WHERE, Nested Query (i.e., 2-step Queries), GROUP BY\n",
    "\n",
    "author_name = 'Hao Wang'\n",
    "\n",
    "sub_nested_query = df_aut \\\n",
    "    .filter(col('name') == author_name) \\\n",
    "    .select('author_id') \\\n",
    "    .rdd.flatMap(lambda x: x) \\\n",
    "    .collect()\n",
    "\n",
    "nested_query = df_aff \\\n",
    "    .filter(col('author_id').isin(sub_nested_query)) \\\n",
    "    .filter(col('organization') != 'null')\n",
    "\n",
    "df_papers \\\n",
    "    .join(nested_query, 'paper_id') \\\n",
    "    .select('paper_id',\n",
    "            'organization',\n",
    "            explode('fos').alias('field_of_study')) \\\n",
    "    .groupBy('field_of_study') \\\n",
    "    .agg(collect_set('organization').alias('organization')) \\\n",
    "    .orderBy('field_of_study') \\\n",
    "    .show(14, truncate=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Query 9: WHERE, GROUP BY, HAVING, 1 JOIN\n",
    "\n",
    "from pyspark.sql.functions import collect_set, concat, size\n",
    "\n",
    "df_journals \\\n",
    "    .withColumnRenamed('venue', 'venueJournals') \\\n",
    "    .filter((col('volume')) > 10) \\\n",
    "    .join(df_books, df_books.publisher == df_journals.publisher, 'inner') \\\n",
    "    .drop(df_journals.publisher) \\\n",
    "    .withColumnRenamed('venue', 'venueBooks') \\\n",
    "    .select('venueBooks',\n",
    "            'venueJournals',\n",
    "            'publisher') \\\n",
    "    .dropDuplicates(['venueBooks',\n",
    "                     'venueJournals',\n",
    "                     'publisher']) \\\n",
    "    .groupBy('publisher') \\\n",
    "    .agg(collect_set('venueBooks').alias('books'),\n",
    "         collect_set('venueJournals').alias('journals')) \\\n",
    "    .withColumn('total_publications_per_publisher', concat('books', 'journals')) \\\n",
    "    .filter(size('total_publications_per_publisher') > '500') \\\n",
    "    .select('publisher',\n",
    "            'total_publications_per_publisher') \\\n",
    "    .show(truncate=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Query 10: WHERE, GROUP BY, HAVING, 2 JOINs\n",
    "# Retrieve authors who worked for at least 3 different organizations and have published at least 3 papers with at least 5 fos and 5 references each\n",
    "\n",
    "from pyspark.sql.functions import approx_count_distinct\n",
    "\n",
    "df_papers \\\n",
    "    .filter((size(col('fos')) >= 5) &\n",
    "            (size(col('references')) >= 5)) \\\n",
    "    .join(df_aff, df_papers.paper_id == df_aff.paper_id, 'inner') \\\n",
    "    .drop(df_aff.paper_id) \\\n",
    "    .join(df_aut, df_aff.author_id == df_aut.author_id, 'inner') \\\n",
    "    .drop(df_aff.author_id) \\\n",
    "    .groupBy('author_id') \\\n",
    "    .agg(count('paper_id').alias('papers_count'),\n",
    "         approx_count_distinct('organization').alias('organizations_count'),\n",
    "         collect_set('name').alias('name')) \\\n",
    "    .filter((size('name') == 1) &\n",
    "            (col('papers_count') >= 3) &\n",
    "            (col('organizations_count') >= 3)) \\\n",
    "    .orderBy(col('papers_count').desc(),\n",
    "             col('organizations_count').desc()) \\\n",
    "    .select(explode('name').alias('name'),\n",
    "            'papers_count',\n",
    "            'organizations_count') \\\n",
    "    .show(5)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
