{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U4ugBnQJt8dk",
    "outputId": "16695f04-6931-4395-dc6f-f1cc9a9af174"
   },
   "outputs": [],
   "source": [
    "# Dowloading pyspark\n",
    "#!pip install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PREPARE THE ENVIRONMENT, UPLOAD DATA, PREPROCESS DATA AND CREATE THE TABLES: Author, Paper, Affiliation, Book, Journal and Conference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/08 09:07:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# With sparkSession we create a connection to our database\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType, TimestampType\n",
    "from pyspark.sql.functions import count, col, xxhash64, collect_list, explode, monotonically_increasing_id\n",
    "\n",
    "# Create an entry point to the PySpark Application\n",
    "spark = SparkSession.builder \\\n",
    "      .master(\"local\") \\\n",
    "      .appName(\"Bibliography\") \\\n",
    "      .getOrCreate()\n",
    "\n",
    "INPUT_FILE = \"bibliography.json\"\n",
    "OPTIONS = {'multiline': 'true', 'allowNumericLeadingZero': 'true','timestampFormat': \"yyyy-MM-dd'T'HH:mm:ss[.ZZZ'Z']\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- authorID: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- bio: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+--------------------+--------------------+\n",
      "|            authorID|              name|               email|                 bio|\n",
      "+--------------------+------------------+--------------------+--------------------+\n",
      "|53f3186fdabfae9a8...|   A. M. A. Hariri|a..m..a..hariride...|My name is A. M. ...|\n",
      "|53f3186fdabfae9a8...|    Matthew Prowse|matthew.prowsefb@...|My name is Matthe...|\n",
      "|53f31870dabfae9a8...|       Sui-ping Qi|sui-ping.qi19@gma...|My name is Sui-pi...|\n",
      "|53f31871dabfae9a8...|     Renato Fabbri|renato.fabbrib7@g...|My name is Renato...|\n",
      "|53f31873dabfae9a8...|   Joachim Schimpf|joachim.schimpf8a...|My name is Joachi...|\n",
      "|53f31874dabfae9a8...|    E. Di Bernardo|e..di.bernardo10@...|My name is E. Di ...|\n",
      "|53f31875dabfae9a8...|    Steven F. Roth|steven.f..roth46@...|My name is Steven...|\n",
      "|53f31878dabfae9a8...|      Nima Zahadat|nima.zahadat3d@gm...|My name is Nima Z...|\n",
      "|53f3187ddabfae9a8...|         Ke Fa Cen|ke.fa.cen23@gmail...|My name is Ke Fa ...|\n",
      "|53f31881dabfae9a8...|    Ricky Houghton|ricky.houghton97@...|My name is Ricky ...|\n",
      "|53f31881dabfae9a8...|Eduardo H. Ramirez|eduardo.h..ramire...|My name is Eduard...|\n",
      "|53f31883dabfae9a8...| Cassidy J. Curtis|cassidy.j..curtis...|My name is Cassid...|\n",
      "|53f31885dabfae9a8...|  Brian D. Koblenz|brian.d..koblenz4...|My name is Brian ...|\n",
      "|53f31887dabfae9a8...|        Guohua Wan|guohua.wanca@gmai...|My name is Guohua...|\n",
      "|53f3188adabfae9a8...|     Irineu Theiss|irineu.theissfe@g...|My name is Irineu...|\n",
      "|53f31892dabfae9a8...|     Derek Yip-Hoi|derek.yip-hoidd@g...|My name is Derek ...|\n",
      "|53f31892dabfae9a8...|     Soo-Hyung Kim|soo-hyung.kimf5@g...|My name is Soo-Hy...|\n",
      "|53f31893dabfae9a8...|      Harold Lorin|harold.lorin7c@gm...|My name is Harold...|\n",
      "|53f3189ddabfae9a8...|    J. Nagy-György|j..nagy-györgyf4@...|My name is J. Nag...|\n",
      "|53f3189ddabfae9a8...|    Roy Varshavsky|roy.varshavsky1d@...|My name is Roy Va...|\n",
      "+--------------------+------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#AUTHOR TABLE\n",
    "schemaAut = StructType(\n",
    "            [StructField('authors', ArrayType(StructType([\n",
    "                StructField('_id', StringType(), nullable = False),\n",
    "                StructField('name', StringType(), True),\n",
    "                StructField('email', StringType(), True),\n",
    "                StructField('bio', StringType(), True),\n",
    "                ])), True)\n",
    "            ])\n",
    "\n",
    "dfAut = spark.read.format('json').options(**OPTIONS).schema(schemaAut).json(INPUT_FILE)\n",
    "dfAut = dfAut.select(explode(dfAut.authors))\n",
    "dfAut = dfAut.withColumnRenamed(\"col\", \"authors\")\n",
    "dfAut = dfAut.filter(col(\"authors._id\") != \"null\").select(\"authors._id\",\"authors.name\",\"authors.email\", \"authors.bio\")\n",
    "dfAut = dfAut.withColumnRenamed(\"_id\", \"authorID\")\n",
    "dfAut = dfAut.dropDuplicates([\"authorID\"])\n",
    "dfAut.printSchema()\n",
    "dfAut.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- paperID: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- keywords: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- fos: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- references: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- page_start: integer (nullable = true)\n",
      " |-- page_end: integer (nullable = true)\n",
      " |-- lang: string (nullable = true)\n",
      " |-- doi: string (nullable = true)\n",
      " |-- url: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- abstract: string (nullable = true)\n",
      " |-- publication_type: string (nullable = true)\n",
      " |-- date: timestamp (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+----------+--------+----+--------------------+--------------------+--------------------+----------------+-------------------+\n",
      "|             paperID|               title|            keywords|                 fos|          references|page_start|page_end|lang|                 doi|                 url|            abstract|publication_type|               date|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+----------+--------+----+--------------------+--------------------+--------------------+----------------+-------------------+\n",
      "|53e99784b7602d970...|Using XML to Inte...|[internet, hyperm...|[xml base, world ...|[53e9adbdb7602d97...|       167|     172|  en|10.1109/CMPSAC.20...|[http://dx.doi.or...|The eXtensible Ma...|            Book|1974-09-13 06:34:29|\n",
      "|53e99784b7602d970...|               FCLOS|[molap, subsumpti...|[information syst...|[53e99ee0b7602d97...|       192|     220|  en|10.1016/j.datak.2...|[http://dx.doi.or...|Mobile online ana...|         Journal|1950-04-14 05:33:22|\n",
      "|53e99785b7602d970...|              Bhoomi|[icts, e governan...|[revenue, transpa...|[53e9b2ffb7602d97...|        20|      31|  en|10.1016/j.tele.20...|[http://dx.doi.or...|The emergence of ...|         Journal|1984-04-06 03:49:29|\n",
      "|53e9978ab7602d970...|                Laps|[health care, hom...|[health care, pop...|[573695936e3b1202...|       962|     976|  en|10.1016/j.ejor.20...|[http://dx.doi.or...|The health care s...|         Journal|1951-10-28 04:52:04|\n",
      "|53e9978db7602d970...|             Mindful|[meta learning, h...|[data science, ro...|[53e9b0c2b7602d97...|      3253|    3274|  en|10.1016/j.ins.200...|[http://dx.doi.or...|Common inductive ...|         Journal|1955-04-13 19:06:45|\n",
      "|53e9978db7602d970...|             MESHMdl|[tuple space, mob...|[middleware, mobi...|[53e99a49b7602d97...|       467|     487|  en|10.1016/j.pmcj.20...|[http://dx.doi.or...|Mobile ad hoc net...|         Journal|2009-09-23 09:35:36|\n",
      "|53e9978db7602d970...|               iCity|[irregular cellul...|[software tool, a...|[53e9ac54b7602d97...|       761|     773|  en|10.1016/j.envsoft...|[http://dx.doi.or...|The objective of ...|         Journal|2002-05-14 19:30:38|\n",
      "|53e9978db7602d970...|              Wisdom|[decision support...|[cognitive map, r...|[53e99b7eb7602d97...|       156|     171|  en|10.1016/j.ejor.20...|[http://dx.doi.or...|Many decision sup...|         Journal|1987-12-30 04:51:23|\n",
      "|53e99792b7602d970...|        SwissAnalyst|         [key words]|[computer science...|[5c795a6e4895d9cb...|       393|     406|  en|10.1007/1-4020-81...|                null|This paper introd...|      Conference|1991-03-30 19:52:33|\n",
      "|53e99792b7602d970...|Short-Term Traffi...|[considerable acc...|[spline mathemati...|[53e9b95bb7602d97...|       669|     675|  en|10.1109/FSKD.2008...|[http://dx.doi.or...|A promising traff...|      Conference|2020-03-15 04:13:40|\n",
      "|53e99792b7602d970...|An approach to fe...|[feature location...|[data mining, cau...|[53e9b6eeb7602d97...|        57|      68|  en|10.1016/j.jss.200...|[http://dx.doi.or...|This paper descri...|         Journal|2012-05-05 11:48:40|\n",
      "|53e99792b7602d970...|Nested Graph-Stru...|[nested graph str...|[adjacency matrix...|[53e9b049b7602d97...|         1|      12|  en|  10.1007/BFb0056317|[http://dx.doi.or...|This paper descri...|            Book|1965-07-21 08:40:34|\n",
      "|53e99792b7602d970...|Demo: Yalut -- us...|[cellular data tr...|[world wide web, ...|[53e9b04eb7602d97...|       360|     361|  en|10.1145/2594368.2...|[http://dx.doi.or...|Yalut is a novel ...|      Conference|2009-02-24 08:00:34|\n",
      "|53e99792b7602d970...|A Uniform Paralle...|[knowledge discov...|[data mining, dat...|[53e99cbbb7602d97...|       306|     312|  en|10.1007/978-3-540...|[http://dx.doi.or...|Grid is a new sol...|         Journal|2015-02-28 22:51:57|\n",
      "|53e99792b7602d970...|WAVELET-BASED IMA...|[mathematical mod...|[computer vision,...|[53e9b550b7602d97...|      1737|    1740|  en|10.1109/ICIP.2010...|[http://dx.doi.or...|Because digital i...|      Conference|2005-12-16 23:43:26|\n",
      "|53e99792b7602d970...|A fuzzy multi-obj...|[location, fire s...|[objective progra...|[53e9bd50b7602d97...|       903|     915|  en|10.1016/j.ejor.20...|[http://dx.doi.or...|Location of fire ...|         Journal|1971-12-17 16:55:38|\n",
      "|53e99792b7602d970...|Adaptive presenta...|[evolving informa...|[metadata, progra...|[53e9a2c8b7602d97...|       193|     196|  en|10.1109/ICALT.200...|[http://dx.doi.or...|The article descr...|            Book|2011-05-08 17:41:02|\n",
      "|53e99792b7602d970...|Testing the stabi...|[asymptotic justi...|[functional princ...|[53e9ace1b7602d97...|       352|     367|  en|10.1016/j.jmva.20...|[http://dx.doi.or...|The functional au...|         Journal|1984-05-13 13:56:58|\n",
      "|53e99792b7602d970...|Statistical Prope...|[power cables, po...|[topology, trippi...|[53e9a7ddb7602d97...|      2517|    2526|  en|10.1109/HICSS.201...|[http://dx.doi.or...|We present the sy...|      Conference|2018-08-18 19:58:00|\n",
      "|53e99792b7602d970...|Generating novel ...|[adaptive behavio...|[interactive evol...|[53e9bc80b7602d97...|         8|      14|  en|10.1145/1056754.1...|[http://dx.doi.or...|Simulated evoluti...|         Journal|2021-05-05 05:11:38|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+----------+--------+----+--------------------+--------------------+--------------------+----------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# PAPER TABLE WITHOUT PUBLICATION_ID\n",
    "schemaPaper = StructType(\n",
    "            [StructField('_id', StringType(), True),\n",
    "             StructField('title', StringType(),True),\n",
    "             StructField('keywords', ArrayType(StringType()), True),\n",
    "             StructField('fos', ArrayType(StringType()), True),\n",
    "             StructField('references', ArrayType(StringType()), True),\n",
    "             StructField('page_start', IntegerType(), True),\n",
    "             StructField('page_end', IntegerType(), True),\n",
    "             StructField('lang', StringType(),True),\n",
    "             StructField('doi', StringType(),True),\n",
    "             StructField('url', ArrayType(StringType()),True),\n",
    "             StructField('abstract', StringType(),True),\n",
    "             StructField('publication_type', StringType(),True),\n",
    "             StructField('date', TimestampType(), True)\n",
    "            ])\n",
    "\n",
    "dfPaper = spark.read.format('json').options(**OPTIONS).schema(schemaPaper).json(INPUT_FILE)\n",
    "dfPaper = dfPaper.withColumnRenamed(\"_id\", \"paperID\")\n",
    "dfPaper.printSchema()\n",
    "dfPaper.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- paperID: string (nullable = true)\n",
      " |-- authorID: string (nullable = true)\n",
      " |-- organization: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|             paperID|            authorID|        organization|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|53e998c7b7602d970...|53f3186fdabfae9a8...|Department of Sta...|\n",
      "|53e99827b7602d970...|53f3186fdabfae9a8...|Laboratory for Fo...|\n",
      "|53e99924b7602d970...|53f31870dabfae9a8...|Henan Academy of ...|\n",
      "|53e998dbb7602d970...|53f31871dabfae9a8...|Instituto de Físi...|\n",
      "|53e998f6b7602d970...|53f31873dabfae9a8...|                null|\n",
      "|53e998bfb7602d970...|53f31874dabfae9a8...|                null|\n",
      "|53e9984bb7602d970...|53f31875dabfae9a8...|                null|\n",
      "|53e998e8b7602d970...|53f31878dabfae9a8...|George Mason Univ...|\n",
      "|53e99905b7602d970...|53f3187ddabfae9a8...|State Key Laborat...|\n",
      "|53e998e9b7602d970...|53f31881dabfae9a8...|                null|\n",
      "|53e9984fb7602d970...|53f31881dabfae9a8...|Tecnologico de Mo...|\n",
      "|53e9980eb7602d970...|53f31883dabfae9a8...|University of Was...|\n",
      "|53e997e9b7602d970...|53f31885dabfae9a8...|Digital Equipment...|\n",
      "|53e998b0b7602d970...|53f31887dabfae9a8...|Antai College of ...|\n",
      "|53e998cdb7602d970...|53f31887dabfae9a8...|Department of Ind...|\n",
      "|53e998f0b7602d970...|53f3188adabfae9a8...|E-Gov, Juridical ...|\n",
      "|53e998fcb7602d970...|53f31892dabfae9a8...|2250 G G Brown, A...|\n",
      "|53e99998b7602d970...|53f31892dabfae9a8...|Chonnam National ...|\n",
      "|53e99800b7602d970...|53f31893dabfae9a8...|IBM Systems Resea...|\n",
      "|53e99866b7602d970...|53f3189ddabfae9a8...|Department of Mat...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# AFFILIATION TABLE\n",
    "schemaAffiliation = StructType(\n",
    "            [StructField('_id', StringType(), True),\n",
    "             StructField('authors', ArrayType(StructType([\n",
    "                    StructField('_id', StringType(), True),\n",
    "                    StructField('org', StringType(), True)\n",
    "             ])), True),\n",
    "            ])\n",
    "\n",
    "dfAff = spark.read.format('json').options(**OPTIONS).schema(schemaAffiliation).json(INPUT_FILE)\n",
    "dfAff = dfAff.withColumnRenamed(\"_id\", \"paperID\")\n",
    "dfAff = dfAff.select(\"paperID\", explode(dfAff.authors))\n",
    "dfAff = dfAff.withColumnRenamed(\"col\", \"authors\")\n",
    "dfAff = dfAff.filter(col(\"authors._id\") != \"null\").filter(col(\"paperID\") != \"null\").select(\"paperID\", \"authors._id\",\"authors.org\")\n",
    "dfAff = dfAff.withColumnRenamed(\"_id\", \"authorID\")\n",
    "dfAff = dfAff.dropDuplicates([\"authorID\", \"paperID\"])\n",
    "dfAff = dfAff.withColumnRenamed(\"org\", \"organization\")\n",
    "dfAff.printSchema()\n",
    "dfAff.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# BOOK TABLE\n",
    "# Preprocessing of the books for cleaning and merging the books\n",
    "book_schema_preprocessing = StructType(\n",
    "    [StructField('_id', StringType(), True),\n",
    "     StructField('isbn', StringType(), True),\n",
    "     StructField('publisher', StringType(), True),\n",
    "     StructField('venue', StringType(), True),\n",
    "     StructField('publication_type', StringType(),True)])\n",
    "\n",
    "# Reading the json file\n",
    "dfbooks_to_filter = spark.read.format('json').options(**OPTIONS).schema(book_schema_preprocessing).json(INPUT_FILE)\n",
    "\n",
    "# Filtering and adjusting the dataframe\n",
    "dfbooks_to_filter = dfbooks_to_filter.filter(col('publication_type') == 'Book').filter(col('isbn') != 'null').filter(col('venue') != 'null')\n",
    "dfbooks_to_filter = dfbooks_to_filter.groupBy('isbn', 'venue').agg(collect_list('publisher').alias('publishersArray'), collect_list('_id').alias('_id'), count(col('publisher'))) # count can be removed (I was interested in evaluating if the group by was meaningful)\n",
    "dfbooks_to_insert = dfbooks_to_filter.withColumn('publisher', dfbooks_to_filter['publishersArray'][0]).select('venue', 'isbn', 'publisher', '_id')\n",
    "\n",
    "# Adding the new column which is the id\n",
    "df_books = dfbooks_to_insert.withColumn('publication id', xxhash64('isbn', 'venue'))\n",
    "\n",
    "# Adding the foreign key to the papers\n",
    "exploded_books = df_books.select(explode('_id'), 'publication id')\n",
    "# exploded_books.show(truncate = False)\n",
    "\n",
    "df_papers_in_books = exploded_books.join(dfPaper, exploded_books.col == dfPaper.paperID)\n",
    "df_papers_in_books = df_papers_in_books.drop('col')\n",
    "\n",
    "df_books = df_books.drop(df_books._id)\n",
    "\n",
    "# Visualizing the data\n",
    "# print('Papers')\n",
    "# df_papers_in_books.show(truncate = False)\n",
    "print('Schema of the books')\n",
    "df_books.printSchema()\n",
    "print('Books')\n",
    "df_books.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# CONFERENCE TABLE\n",
    "# Preprocessing of the books for cleaning and merging the books\n",
    "\n",
    "schemaConf = StructType(\n",
    "    [StructField('_id', StringType(), True),\n",
    "     StructField('location', StringType(), True),\n",
    "     StructField('venue', StringType(), True),\n",
    "     StructField('publication_type', StringType(),True)])\n",
    "# Reading the json file\n",
    "df_conferences_to_filter = spark.read.format('json').options(**OPTIONS).schema(schemaConf).json(INPUT_FILE)\n",
    "\n",
    "# Filtering and adjusting the dataframe\n",
    "df_conferences_to_filter = df_conferences_to_filter.filter(col('publication_type') == 'Conference').filter(col('venue') != 'null')\n",
    "df_conferences_to_filter = df_conferences_to_filter.groupBy('venue').agg(collect_list('location').alias('locations_array'), collect_list('_id').alias('_id'), count(col('location'))) # count can be removed (I was interested in evaluating if the group by was meaningful)\n",
    "df_conferences_to_insert = df_conferences_to_filter.withColumn('location', df_conferences_to_filter['locations_array'][0]).select('venue', 'location', '_id')\n",
    "\n",
    "# Adding the new column which is the id\n",
    "df_conferences = df_conferences_to_insert.withColumn('publication id', xxhash64('venue'))\n",
    "\n",
    "# Adding the foreign key to the papers\n",
    "exploded_conferences = df_conferences.select(explode('_id'), 'publication id')\n",
    "#exploded_conferences.show(truncate = False)\n",
    "\n",
    "df_papers_in_conferences = exploded_conferences.join(dfPaper, exploded_conferences.col == dfPaper.paperID)\n",
    "df_papers_in_conferences = df_papers_in_conferences.drop('col')\n",
    "\n",
    "df_conferences = df_conferences.drop(df_conferences._id)\n",
    "\n",
    "# Visualizing the data\n",
    "#print('Papers')\n",
    "#df_papers_in_conferences.show(truncate = False)\n",
    "print('Schema of the conferences')\n",
    "df_conferences.printSchema()\n",
    "print('Conferences')\n",
    "df_conferences.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Merging the 3 dataframe which one contains the papers published in a specific media\n",
    "df_papers = df_papers_in_books.union(df_papers_in_journals).union(df_papers_in_conferences)\n",
    "\n",
    "# Visualizing the data\n",
    "print('Papers schema')\n",
    "df_papers.printSchema()\n",
    "print('Papers data')\n",
    "df_papers.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# For checking the result\n",
    "print('Papers published in books')\n",
    "df_papers.filter(col('publication_type') == 'Book').select('paperID', 'title', 'publication_type', 'publication id').show()\n",
    "print('Papers published in journals')\n",
    "df_papers.filter(col('publication_type') == 'Journal').select('paperID', 'title', 'publication_type', 'publication id').show()\n",
    "print('Papers published in conferences')\n",
    "df_papers.filter(col('publication_type') == 'Conference').select('paperID', 'title', 'publication_type', 'publication id').show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "COMMANDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Command 4: delete a group of rows\n",
    "\n",
    "# Use the function year to extract the year from the timestamp\n",
    "from pyspark.sql.functions import year\n",
    "\n",
    "# Drop rows with conditions – where clause\n",
    "# From 37626 to 37175 -> delete all the rows that represent papers published before 1950, because obsolete\n",
    "df_papers = df_papers.where(year('date') > '1950')\n",
    "df_papers.select('title', 'publication_type', 'date').orderBy('date').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Command 5: create a new column with the length of the paper (number of total pages)\n",
    "\n",
    "df_papers_total_pages = df_papers \\\n",
    "    .filter((col('page_start') >= 0) & (col('page_end') >= 0) & (col('page_start') <= col('page_end'))) \\\n",
    "    .withColumn('total_pages', col('page_end') - col('page_start'))\n",
    "\n",
    "df_papers_total_pages \\\n",
    "    .select(col('title'), col('page_start'), col('page_end'), col('total_pages')) \\\n",
    "    .show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "QUERIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query 1: WHERE, JOIN\n",
    "# from pyspark.sql.functions import collect_list, size\n",
    "# \n",
    "# venue, volume, issue = ('IEEE Internet Computing', '5', '6') \n",
    "# \n",
    "# df_papers_q1 = df_journals\\\n",
    "#                 .filter((col('venue') == venue) &\n",
    "#                         (col('volume') == volume) &\n",
    "#                         (col('issue') == issue))\\\n",
    "#                 .join(df_papers.select(['paperID', 'publication id', 'publication_type', 'title']),\n",
    "#                       (df_journals['publication id'] == df_papers['publication id']) &\n",
    "#                         (df_papers['publication_type'] == 'Journal'),\n",
    "#                       'inner')\\\n",
    "#                 .groupBy(['venue', 'volume', 'issue'])\\\n",
    "#                 .agg(collect_list('title').alias('papers'))\\\n",
    "#                 .withColumn('papers', size(col('papers')))\n",
    "# \n",
    "# df_papers_q1.select(['papers']).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- paperID: string (nullable = true)\n",
      " |-- hash: long (nullable = false)\n",
      " |-- publication id: long (nullable = false)\n",
      " |-- publication_type: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- p2: string (nullable = true)\n",
      " |-- pub_join: long (nullable = false)\n",
      " |-- p1: string (nullable = true)\n",
      " |-- hash_join: long (nullable = false)\n",
      " |-- pubID?: boolean (nullable = false)\n",
      " |-- hashID?: boolean (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# JOURNAL TABLE\n",
    "# Preprocessing of the journals for cleaning and merging the journals\n",
    "\n",
    "journal_schema_preprocessing = StructType(\n",
    "    [StructField('_id', StringType(), True),\n",
    "     StructField('issn', StringType(), True),\n",
    "     StructField('publisher', StringType(), True),\n",
    "     StructField('venue', StringType(), True),\n",
    "     StructField('volume', IntegerType(), True),\n",
    "     StructField('issue', IntegerType(), True),\n",
    "     StructField('publication_type', StringType(),True)])\n",
    "\n",
    "# Reading the json file\n",
    "df_journals_to_filter = spark.read.format('json').options(**OPTIONS).schema(journal_schema_preprocessing).json(INPUT_FILE)\n",
    "\n",
    "# Filtering and adjusting the dataframe\n",
    "df_journals_to_filter = df_journals_to_filter\\\n",
    "                            .filter(col('publication_type') == 'Journal')\\\n",
    "                            .filter(col('issn') != 'null')\\\n",
    "                            .filter(col('venue') != 'null')\\\n",
    "                            .filter(col('issue') >= 0)\\\n",
    "                            .filter(col('volume') >= 0)\n",
    "\n",
    "df_journals_to_filter = df_journals_to_filter\\\n",
    "                            .groupBy('venue', 'volume', 'issue', 'issn')\\\n",
    "                            .agg(collect_list('publisher').alias('publishersArray'),\n",
    "                                 collect_list('_id').alias('_id'),\n",
    "                                 count(col('publisher'))) # count can be removed (I was interested in evaluating if the group by was meaningful)\n",
    "df_journals_to_insert = df_journals_to_filter\\\n",
    "                            .withColumn('publisher', df_journals_to_filter['publishersArray'][0])\\\n",
    "                            .select('venue', 'volume', 'issue', 'publisher', 'issn', '_id')\n",
    "\n",
    "# Adding the new column which contains the publication identifier\n",
    "df_journals = df_journals_to_insert.withColumn(\"publication id\", xxhash64('venue', 'volume', 'issue', 'issn'))\n",
    "df_journals2 = df_journals_to_insert.withColumn(\"publication id\", monotonically_increasing_id())\n",
    "\n",
    "df_journals4 = df_journals_to_insert.withColumns({'hash': xxhash64('venue', 'volume', 'issue', 'issn'),\n",
    "                                                  'publication id': monotonically_increasing_id()})\n",
    "\n",
    "# df_journals.printSchema()\n",
    "# Adding the foreign key to the papers\n",
    "exploded_journals = df_journals.select(explode('_id'), 'publication id')\n",
    "exploded_journals2 = df_journals2.select(explode('_id').alias('pID'), 'publication id')\n",
    "exploded_journals4 = df_journals4.select(explode('_id').alias('pID'), 'publication id', 'hash')\n",
    "#exploded_journals.show(truncate = False)\n",
    "\n",
    "df_papers_in_journals = exploded_journals.join(dfPaper, exploded_journals.col == dfPaper.paperID, \"inner\")\n",
    "df_papers_in_journals = df_papers_in_journals.drop('col')\n",
    "\n",
    "df_papers_in_journals2 = exploded_journals2\\\n",
    "                            .join(dfPaper, exploded_journals2.pID == dfPaper.paperID, \"inner\")\n",
    "df_papers_in_journals2 = df_papers_in_journals2.drop('pID')\n",
    "\n",
    "df_papers_in_journals4 = exploded_journals4\\\n",
    "                            .join(dfPaper, exploded_journals4.pID == dfPaper.paperID, \"inner\")\n",
    "df_papers_in_journals4 = df_papers_in_journals4.drop('pID')\n",
    "\n",
    "df_journals3 = df_journals.withColumn('hash', col('publication id')).drop('publication id').drop('_id')\\\n",
    "                            .join(df_journals2.drop('publisher'),\n",
    "                                  ['venue', 'volume', 'issue', 'issn'],\n",
    "                                 'inner')\n",
    "\n",
    "# df_journals3.printSchema()\n",
    "exploded_journals3 = df_journals3.select(explode('_id').alias('pID'), 'publication id', 'hash')\n",
    "\n",
    "df_papers_in_journals3 = exploded_journals3\\\n",
    "                            .join(dfPaper, exploded_journals3.pID == dfPaper.paperID, \"inner\")\n",
    "\n",
    "df_papers_in_journals3.drop('pID')\n",
    "all_papers = df_papers_in_journals3\\\n",
    "                .select(['paperID', 'hash', 'publication id', 'publication_type', 'title'])\\\n",
    "                .join(df_papers_in_journals2.select(col('paperID').alias('p2'), col('publication id').alias('pub_join')),\n",
    "                      df_papers_in_journals3.paperID == col('p2'))\\\n",
    "                .join(df_papers_in_journals.select(col('paperID').alias('p1'), col('publication id').alias('hash_join')),\n",
    "                      df_papers_in_journals3.paperID == col('p1'))\n",
    "\n",
    "stats = all_papers.withColumn('pubID?', col('publication id') == col('pub_join'))\\\n",
    "                  .withColumn('hashID?', col('hash') == col('pub_join'))\n",
    "\n",
    "stats.printSchema()\n",
    "\n",
    "\n",
    "df_journals = df_journals.drop(df_journals._id)\n",
    "df_journals2 = df_journals2.drop('_id')\n",
    "df_journals4 = df_journals4.drop('_id')\n",
    "# Visualizing the data\n",
    "# print('Papers')\n",
    "# df_papers_in_journals.show(truncate = False)\n",
    "# print('Schema of the journals')\n",
    "# df_papers_in_journals.printSchema()\n",
    "# df_journals.printSchema()\n",
    "# print('Journals')\n",
    "# df_journals.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n",
      "|count((pubID? = true))|\n",
      "+----------------------+\n",
      "|                 13708|\n",
      "+----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|count((hashID? = true))|\n",
      "+-----------------------+\n",
      "|                  13708|\n",
      "+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmp = stats.agg(count(col('pubID?') == True))\n",
    "tmp.show()\n",
    "tmp = stats.agg(count(col('hashID?') == True))\n",
    "tmp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11270\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11270\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11270\n",
      "4 mon_inc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11270\n",
      "4 hash\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11270\n",
      "Papers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 898:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(df_journals.count())\n",
    "print(df_journals.drop_duplicates(['publication id']).count())\n",
    "\n",
    "print('2')\n",
    "print(df_journals2.count())\n",
    "print(df_journals2.drop_duplicates(['publication id']).count())\n",
    "\n",
    "print('3')\n",
    "print(df_journals3.count())\n",
    "print(df_journals3.drop_duplicates(['publication id', 'hash']).count())\n",
    "\n",
    "print('4 mon_inc')\n",
    "print(df_journals4.count())\n",
    "print(df_journals4.drop_duplicates(['publication id']).count())\n",
    "\n",
    "print('4 hash')\n",
    "print(df_journals4.count())\n",
    "print(df_journals4.drop_duplicates(['hash']).count())\n",
    "\n",
    "print('Papers')\n",
    "print(df_papers_in_journals.count())\n",
    "print(df_papers_in_journals2.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1003:>                                                       (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "keys = df_papers_in_journals4.select('publication id', 'hash').dropDuplicates()\n",
    "print(keys.count())\n",
    "jkeys = df_journals4.select('publication id', 'hash').dropDuplicates()\n",
    "print(jkeys.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers = df_papers_in_journals4\n",
    "journals = df_journals4\n",
    "\n",
    "journals.printSchema()\n",
    "df_journals.printSchema()\n",
    "papers.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+-----+------+\n",
      "|                name|volume|issue|papers|\n",
      "+--------------------+------+-----+------+\n",
      "|J. Intelligent Ma...|    25|    2|     2|\n",
      "+--------------------+------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import collect_list, size\n",
    "\n",
    "\n",
    "venue, volume, issue = ('IEEE Internet Computing', '5', '6') \n",
    "\n",
    "papers = df_papers_in_journals4\n",
    "journals = df_journals4\n",
    "df_papers_q1 = journals\\\n",
    "                .join(papers,\n",
    "                      ['hash', 'publication id'],\n",
    "                      'inner')\\\n",
    "                .groupBy('venue', 'volume', 'issue')\\\n",
    "                .agg(collect_list('title').alias('papers'))\\\n",
    "                .withColumn('papers', size(col('papers')))\\\n",
    "                .sort(col('papers').desc())\n",
    "\n",
    "df_papers_q1.select(col('venue').alias('name'), 'volume', 'issue', 'papers').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Query 4: GROUP BY, JOIN, AS\n",
    "from pyspark.sql.functions import collect_set, concat, size\n",
    "\n",
    "df_journals_venue_rename = df_journals.withColumnRenamed('venue', 'venueJournals')\n",
    "df_books_venue_rename = df_books.withColumnRenamed('venue', 'venueBooks')\n",
    "df = df_books_venue_rename\\\n",
    "    .join(df_journals_venue_rename,\n",
    "          df_books_venue_rename.publisher == df_journals_venue_rename.publisher,\n",
    "          \"inner\")\\\n",
    "    .drop(df_journals.publisher)\\\n",
    "    .select('venueBooks', 'venueJournals', 'publisher')\\\n",
    "    .dropDuplicates(['venueBooks', 'venueJournals', 'publisher'])\\\n",
    "    .groupBy('publisher')\\\n",
    "    .agg(collect_set('venueBooks').alias('books'),\n",
    "         collect_set('venueJournals').alias('journals'))\\\n",
    "    .withColumn(\"total_publications_per_publisher\",\n",
    "                concat(col(\"books\"), col(\"journals\")))\\\n",
    "    .filter(size(col(\"total_publications_per_publisher\")) > '500')\\\n",
    "    .select('publisher', \"total_publications_per_publisher\")\\\n",
    "    .show(truncate = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Query 5: WHERE, GROUP BY\n",
    "# Retrieve some statistics about papers\n",
    "from pyspark.sql.functions import sum, min, max, avg, format_number, variance\n",
    "\n",
    "df_papers_total_pages.filter(year(col('date')) >= 2015) \\\n",
    "    .groupBy(year(col('date')).alias('year')) \\\n",
    "    .agg(count('paperID').alias('total_papers'),\n",
    "         sum('total_pages').alias('total_pages'),\n",
    "         min('total_pages').alias('min_pages'),\n",
    "         max('total_pages').alias('max_pages'),\n",
    "         format_number(avg('total_pages'), 2).alias('avg_pages'),\n",
    "         format_number(variance('total_pages'), 2).alias('var_pages')) \\\n",
    "    .sort(col('year').desc()) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query 6: GROUP BY, HAVING, AS\n",
    "df_papers_q6 = df_papers\\\n",
    "                    .select(['paperID', 'title', explode(col('references')).alias('reference')])\\\n",
    "                    .groupBy('reference')\\\n",
    "                    .agg(count('*').alias('references_count'))\\\n",
    "                    .filter(col('references_count') > 30)\\\n",
    "                    .join(df_papers.select(['paperID', 'title', 'doi', 'url']),\n",
    "                          col('reference') == df_papers.paperID)\\\n",
    "                    .sort(col('references_count').desc())\\\n",
    "\n",
    "df_papers_q6.select(['title', 'references_count']).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Query 9: WHERE, GROUP BY, HAVING, 1 JOIN\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Query 10: WHERE, GROUP BY, HAVING, 2 JOINs\n",
    "# Retrieve authors who worked for at least 3 different organizations and have published at least 3 papers with at least 5 fos and 5 references each\n",
    "\n",
    "from pyspark.sql.functions import approx_count_distinct\n",
    "\n",
    "df_papers \\\n",
    "    .filter((size(col('fos')) >= 5) & (size(col('references')) >= 5)) \\\n",
    "    .join(dfAff, df_papers.paperID == dfAff.paperID, \"inner\") \\\n",
    "    .drop(dfAff.paperID) \\\n",
    "    .join(dfAut, dfAff.authorID == dfAut.authorID, \"inner\") \\\n",
    "    .drop(dfAff.authorID) \\\n",
    "    .groupBy(\"authorID\") \\\n",
    "    .agg(count(\"paperID\").alias(\"papers_count\"),                                                                                       approx_count_distinct(\"organization\").alias(\"organizations_count\"),\n",
    "         collect_set(\"name\").alias(\"name\")) \\\n",
    "    .filter((size(\"name\") == 1) & (col(\"papers_count\") >= 3)  & (col(\"organizations_count\") >= 3)) \\\n",
    "    .orderBy(col(\"papers_count\").desc(), col(\"organizations_count\").desc()) \\\n",
    "    .select(explode(col(\"name\")).alias(\"name\"), col(\"papers_count\"), col(\"organizations_count\")) \\\n",
    "    .show(5)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
